{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install","metadata":{}},{"cell_type":"code","source":"!pip3 install huggingface-hub","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade openai\n!pip install --upgrade pydantic","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U peft\n%pip install -U accelerate\n%pip install -U trl","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !mkdir MetaMath-Mistral-7B\n# !huggingface-cli download meta-math/MetaMath-Mistral-7B --local-dir MetaMath-Mistral-7B --local-dir-use-symlinks False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install sentencepiece","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# VERSION = \"1.11\"\n# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py > /dev/null\n# !python pytorch-xla-env-setup.py --version $VERSION  > /dev/null","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import library","metadata":{}},{"cell_type":"code","source":"# import tensorflow as tf\n# import tensorflow_hub as hub\n# print(\"Tensorflow version \" + tf.__version__)\n# AUTO = tf.data.experimental.AUTOTUNE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Detect TPU, return appropriate distribution strategy\n# try:\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n#     print('Running on TPU ', tpu.master())\n# except ValueError:\n#     tpu = None\n\n# if tpu:\n#     tf.config.experimental_connect_to_cluster(tpu)\n#     tf.tpu.experimental.initialize_tpu_system(tpu)\n#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# else:\n#     strategy = tf.distribute.get_strategy() \n\n# print(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nimport os,torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:06:53.918593Z","iopub.execute_input":"2023-11-28T16:06:53.919910Z","iopub.status.idle":"2023-11-28T16:07:02.109248Z","shell.execute_reply.started":"2023-11-28T16:06:53.919860Z","shell.execute_reply":"2023-11-28T16:07:02.108205Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# # PyTorch XLA-specific imports\n# import torch_xla.core.xla_model as xm\n# import torch_xla.distributed.parallel_loader as pl\n# import torch_xla.distributed.xla_multiprocessing as xmp\n# import torch_xla.debug.metrics as met","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:02.111184Z","iopub.execute_input":"2023-11-28T16:07:02.111783Z","iopub.status.idle":"2023-11-28T16:07:02.116572Z","shell.execute_reply.started":"2023-11-28T16:07:02.111755Z","shell.execute_reply":"2023-11-28T16:07:02.115415Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nsecret_wandb = user_secrets.get_secret(\"wandb\")","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:02.118034Z","iopub.execute_input":"2023-11-28T16:07:02.118393Z","iopub.status.idle":"2023-11-28T16:07:02.546552Z","shell.execute_reply.started":"2023-11-28T16:07:02.118366Z","shell.execute_reply":"2023-11-28T16:07:02.545493Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login --token $secret_hf","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:02.549510Z","iopub.execute_input":"2023-11-28T16:07:02.549919Z","iopub.status.idle":"2023-11-28T16:07:04.173706Z","shell.execute_reply.started":"2023-11-28T16:07:02.549879Z","shell.execute_reply":"2023-11-28T16:07:04.172287Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# wandb.login(key = secret_wandb)\n# run = wandb.init(\n# #     project='Fine tuning MetaMath mistral 7B - ZAIC',\n#     project='Fine tuning openchat 7B - ZAIC',\n#     job_type=\"training\", \n#     anonymous=\"allow\"\n# )","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:04.175557Z","iopub.execute_input":"2023-11-28T16:07:04.175965Z","iopub.status.idle":"2023-11-28T16:07:04.181640Z","shell.execute_reply.started":"2023-11-28T16:07:04.175926Z","shell.execute_reply":"2023-11-28T16:07:04.180430Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# base_model = \"meta-math/MetaMath-Mistral-7B\"\n# base_model = \"gpt2-xl\"\nbase_model = \"gpt2\"\n# base_model = \"openchat/openchat_3.5\"\nnew_model = \"BK-BigAI-Math\"\nmodel_hotamath_path = \"/kaggle/working/BK-BigAI-Math\"","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:04.183293Z","iopub.execute_input":"2023-11-28T16:07:04.183935Z","iopub.status.idle":"2023-11-28T16:07:04.197139Z","shell.execute_reply.started":"2023-11-28T16:07:04.183896Z","shell.execute_reply":"2023-11-28T16:07:04.196045Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Download dataset","metadata":{}},{"cell_type":"code","source":"!mkdir dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\nhf_hub_download(repo_id=\"hotamago/ZAIC-2023\", filename=\"Elementary Maths Solving/test.zip\", revision=\"main\", repo_type=\"dataset\", local_dir=\"dataset\", local_dir_use_symlinks=False)\nhf_hub_download(repo_id=\"hotamago/ZAIC-2023\", filename=\"Elementary Maths Solving/train.zip\", revision=\"main\", repo_type=\"dataset\", local_dir=\"dataset\", local_dir_use_symlinks=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo apt-get install unzip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir datasetRaw\n!unzip -q -o \"dataset/Elementary Maths Solving/test.zip\" -d \"datasetRaw\"\n!unzip -q -o \"dataset/Elementary Maths Solving/train.zip\" -d \"datasetRaw\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport re\nimport time","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:04.198716Z","iopub.execute_input":"2023-11-28T16:07:04.199142Z","iopub.status.idle":"2023-11-28T16:07:04.221955Z","shell.execute_reply.started":"2023-11-28T16:07:04.199104Z","shell.execute_reply":"2023-11-28T16:07:04.220779Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_data = None\ntest_data = None\nwith open(os.path.join(\"datasetRaw\", \"train\", \"/kaggle/working/datasetRaw/math_train.json\"), \"r\") as f:\n    train_data = json.loads(f.read())['data']\nwith open(os.path.join(\"datasetRaw\", \"test\", \"/kaggle/working/datasetRaw/math_test.json\"), \"r\") as f:\n    test_data = json.loads(f.read())['data']","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:04.223647Z","iopub.execute_input":"2023-11-28T16:07:04.225224Z","iopub.status.idle":"2023-11-28T16:07:04.249295Z","shell.execute_reply.started":"2023-11-28T16:07:04.225188Z","shell.execute_reply":"2023-11-28T16:07:04.248255Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_data[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:04.250641Z","iopub.execute_input":"2023-11-28T16:07:04.250952Z","iopub.status.idle":"2023-11-28T16:07:04.259188Z","shell.execute_reply.started":"2023-11-28T16:07:04.250925Z","shell.execute_reply":"2023-11-28T16:07:04.258233Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'id': '1',\n 'question': 'Một người bán hàng bỏ ra 80,000 đồng tiền vốn và bị lỗ 6%. Để tính số tiền lỗ ta phải tính',\n 'choices': ['A. 80,000 : 6',\n  'B. 80,000 x 6',\n  'C. 80,000 : (6 x 100)',\n  'D. (80,000 x 6) : 100'],\n 'explanation': 'Theo đề bài, số tiền lỗ bằng 6% của 80 000 đồng . Để tìm số tiền lỗ ta có thể lấy 80 000 chia cho 100 rồi nhân với 6 (tức là 80 000 : 100 × 6) hoặc lấy 80000 nhân với 6 rồi chia cho 100 (tức là 80 000 × 6 : 100).',\n 'answer': 'D. (80,000 x 6) : 100'}"},"metadata":{}}]},{"cell_type":"code","source":"test_data[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:04.262469Z","iopub.execute_input":"2023-11-28T16:07:04.262830Z","iopub.status.idle":"2023-11-28T16:07:04.273562Z","shell.execute_reply.started":"2023-11-28T16:07:04.262802Z","shell.execute_reply":"2023-11-28T16:07:04.272383Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'id': '01-0203',\n 'question': 'Một cửa hàng đã bán 30% số hàng hiện có và thu được 15 000 000 đồng. Hỏi nếu bán hết hàng thì cửa hàng thu được bao nhiêu tiền?',\n 'choices': ['A. 4 500 000 đồng',\n  'B. 45 000 000 đồng',\n  'C. 50 000 000 đồng',\n  'D. 450 000 000 đồng']}"},"metadata":{}}]},{"cell_type":"code","source":"MAX_TOKEN_MODEL = 512","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:04.274965Z","iopub.execute_input":"2023-11-28T16:07:04.275296Z","iopub.status.idle":"2023-11-28T16:07:04.285956Z","shell.execute_reply.started":"2023-11-28T16:07:04.275270Z","shell.execute_reply":"2023-11-28T16:07:04.285122Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"DEFAULT_PAD_TOKEN = \"[PAD]\"\nDEFAULT_EOS_TOKEN = \"<|endoftext|>\" # \"<|end_of_turn|>\" # \"</s>\"\nDEFAULT_BOS_TOKEN = \"<|endoftext|>\" # \"<s>\"\nDEFAULT_UNK_TOKEN = \"<|endoftext|>\" # \"<unk>\"\nDEFAULT_BOI_TOKEN = \"<|human|>\" # \"Human:\" # \"[INST]\"\nDEFAULT_EOI_TOKEN = \"<|assistant|>\" # \"Assistant:\" # \"[/INST]\"\nPROMPT_DICT = {\n    \"prompt_input\": (\n        \"Below is an instruction that describes a task, paired with the choices, one of the choices is the correct answer to the request. \"\n        \"Write a response that appropriately completes the request.\\n\\n\"\n        \"### Instruction:\\n{instruction}\\n\\n### Choices:\\n{choices}\"\n    ),\n    \"prompt_input_run\": (\n        DEFAULT_BOI_TOKEN + \" Below is an instruction that describes a task. paired with the choices, one of the choices is the correct answer to the request. \"\n        \"Write a response that appropriately completes the request.\\n\\n\"\n        \"### Instruction:\\n{instruction}\\n\\n### Choices:\\n{choices}\"\n        \"\\n\" + DEFAULT_EOI_TOKEN + \" \\n\\n\"\n#         \"### Explanation:\\n Let's think step by step.\\n\"\n        \"### Explanation:\\n Hãy suy nghĩ từng bước một.\\n\"\n    ),\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:04.287255Z","iopub.execute_input":"2023-11-28T16:07:04.287577Z","iopub.status.idle":"2023-11-28T16:07:04.301299Z","shell.execute_reply.started":"2023-11-28T16:07:04.287551Z","shell.execute_reply":"2023-11-28T16:07:04.300320Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"timeGlobal = 0\ndef startTime():\n    global timeGlobal\n    timeGlobal = time.time()\ndef getTime():\n    return (time.time() - timeGlobal)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:04.302610Z","iopub.execute_input":"2023-11-28T16:07:04.303021Z","iopub.status.idle":"2023-11-28T16:07:04.316653Z","shell.execute_reply.started":"2023-11-28T16:07:04.302958Z","shell.execute_reply":"2023-11-28T16:07:04.315649Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def ApplyPromptTemplate(instruction, choices, typeP = \"prompt_input\"):\n    return PROMPT_DICT[typeP].format(instruction = instruction, choices = \"\\n\".join(choices))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:04.318051Z","iopub.execute_input":"2023-11-28T16:07:04.318649Z","iopub.status.idle":"2023-11-28T16:07:04.329545Z","shell.execute_reply.started":"2023-11-28T16:07:04.318620Z","shell.execute_reply":"2023-11-28T16:07:04.328672Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Add explantion by GPT 3.5 Tubo","metadata":{}},{"cell_type":"code","source":"from openai import OpenAI\nclient = OpenAI(\n    api_key=user_secrets.get_secret(\"OPENAI_API_KEY\"),\n#     organization='org-j48waUrvSOM1n0J1SLXiAr8n',\n)\n\ndef autoGPTAddExplantion(problem, answer):\n    response = client.chat.completions.create(\n      model=\"gpt-3.5-turbo\",\n      messages=[\n        {\"role\": \"system\", \"content\": \"Explan by Vietnamese step-by-step for given answer to given problem.\\nRule:\\n- No markdown format\\n- Given answer always true for given problem\\n- No title\\n- Short explantion\\n\"},\n        {\"role\": \"user\", \"content\": f\"### Problem:\\n{problem}\\n\\n### Answer:\\n{answer}\"},\n        {\"role\": \"system\", \"content\": \"Giải thích: \"},\n      ],\n#       max_tokens=512,\n      temperature=0,\n      top_p=1.0,\n#       top_k=50,\n    )\n    return response.choices[0].message.content","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"startTime()\ncntSt = 0\ni = 0\ntimeWait = 20\nwhile i < len(train_data):\n    singleData = train_data[i]\n    if \"explanation\" in singleData.keys():\n        i += 1\n        continue\n    print(f\"Runing testcase ({i})\")\n    \n    try:\n        res = autoGPTAddExplantion(singleData['question'], singleData['answer'])\n    except Exception as inst:\n        print(type(inst))\n        print(inst)\n        \n        time.sleep(timeWait)\n        timeWait *= 2\n        if timeWait > 80:\n            break\n        print\n        continue\n    \n    timeWait = 20\n        \n    cntSt += 1\n    train_data[i]['explanation'] = res\n    print(f\"Done testcase ({i})\")\n    #     print(train_data[i])\n    if cntSt%3 == 0:\n        deltaTime = getTime()\n        if (60 - deltaTime) > -1:\n            time.sleep(60 - deltaTime + 2)\n        startTime()\n        cntSt = 0\n    \n    i += 1\nprint(\"Done all testcase\")","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/working/datasetRaw/math_train.json\", \"w\", encoding='utf-8') as f:\n    json.dump({\n        \"__count__\": len(train_data),\n        \"data\": train_data\n    }, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add data by education program","metadata":{}},{"cell_type":"markdown","source":"Todo: Create data generation algorithms for each chapter.\n\nBelow is class base, change \\_call function to a function that create radom question for that chapter","metadata":{}},{"cell_type":"code","source":"class BaseGenChapter:\n    def __init__(*args, **kwargs):\n        pass\n    \n    def _call(self, *args, **kwargs) -> str:\n        raise NotImplementedError(\"_call not Implement yet\")\n    \n    def call(self, *args, **kwargs) -> str:\n        return self._call(*args, **kwargs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocess data","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\nimport random","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:07.756743Z","iopub.execute_input":"2023-11-28T16:07:07.757158Z","iopub.status.idle":"2023-11-28T16:07:07.761692Z","shell.execute_reply.started":"2023-11-28T16:07:07.757126Z","shell.execute_reply":"2023-11-28T16:07:07.760759Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# random.shuffle(train_data)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:09.650009Z","iopub.execute_input":"2023-11-28T16:07:09.651211Z","iopub.status.idle":"2023-11-28T16:07:09.655648Z","shell.execute_reply.started":"2023-11-28T16:07:09.651167Z","shell.execute_reply":"2023-11-28T16:07:09.654760Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"num_train_dataset = len(train_data)\nvalition_radio = 0.1\ntokenized_train_dataset_raw = train_data[:-int(num_train_dataset*valition_radio)]\ntokenized_val_dataset_raw = train_data[-int(num_train_dataset*valition_radio):]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:09.861637Z","iopub.execute_input":"2023-11-28T16:07:09.862798Z","iopub.status.idle":"2023-11-28T16:07:09.868725Z","shell.execute_reply.started":"2023-11-28T16:07:09.862746Z","shell.execute_reply":"2023-11-28T16:07:09.867705Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"datasetStruct = {\"input\":[], \"output\":[]}\ndataset = {\"text\":[]}\nnum_train_dataset = len(tokenized_train_dataset_raw)\nfor i in range(num_train_dataset):\n    ttdro = tokenized_train_dataset_raw[i]\n    \n    if \"explanation\" not in ttdro.keys():\n        continue\n    \n    input_content = \"{0} {1}\".format(\n        DEFAULT_BOI_TOKEN,\n        ApplyPromptTemplate(ttdro['question'], ttdro['choices']),\n    )\n    datasetStruct[\"input\"].append(input_content)\n    \n    if \"explanation\" not in ttdro.keys():\n        output_content = \"\\n{0} \\n\\n{1}\\n\\n{2} {3}\".format(\n            DEFAULT_EOI_TOKEN,\n            \"### Explanation:\\nNo explanation{0}\",\n            \"### Answer:\\n{0}\".format(ttdro['answer']),\n            DEFAULT_EOS_TOKEN,\n        )\n    else:\n        output_content = \"\\n{0} \\n\\n{1}\\n\\n{2} {3}\".format(\n            DEFAULT_EOI_TOKEN,\n            \"### Explanation:\\n{0}\".format(ttdro['explanation']),\n            \"### Answer:\\n{0}\".format(ttdro['answer']),\n            DEFAULT_EOS_TOKEN,\n        )\n    datasetStruct[\"output\"].append(output_content)\n    \n    dataset[\"text\"].append(input_content + output_content)\n#     <s>[INST][/INST] </s>","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:10.037679Z","iopub.execute_input":"2023-11-28T16:07:10.038449Z","iopub.status.idle":"2023-11-28T16:07:10.057087Z","shell.execute_reply.started":"2023-11-28T16:07:10.038414Z","shell.execute_reply":"2023-11-28T16:07:10.055666Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(dataset[\"text\"][32])","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:10.293282Z","iopub.execute_input":"2023-11-28T16:07:10.293690Z","iopub.status.idle":"2023-11-28T16:07:10.300316Z","shell.execute_reply.started":"2023-11-28T16:07:10.293658Z","shell.execute_reply":"2023-11-28T16:07:10.299311Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"<|human|> Below is an instruction that describes a task, paired with the choices, one of the choices is the correct answer to the request. Write a response that appropriately completes the request.\n\n### Instruction:\nNgày thứ nhất, bác Thái thu hoạch được 250 kg nhãn. Ngày thứ hai, số ki- lô-gam nhãn bác Thái thu hoạch được đã giảm đi 2 lần so với ngày thứ nhất. Vậy cả hai ngày bác Thái thu hoạch được số ki-lô-gam nhãn là:\n\n### Choices:\nA. 500 kg\nB. 750 kg\nC. 125kg\nD. 375 kg\n<|assistant|> \n\n### Explanation:\nNgày thứ hai, bác Thái thu hoạch được số ki-lô-ham nhãn là: 250 : 2 = 125 (kg) \n Cả hai ngày bác Thái thu hoạch được số ki-lô-gam nhãn là: $250 + 125 = 375$ (kg)\n Đáp số: 375 kg\n\n### Answer:\nD. 375 kg <|endoftext|>\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(dataset[\"text\"]))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:10.805317Z","iopub.execute_input":"2023-11-28T16:07:10.805720Z","iopub.status.idle":"2023-11-28T16:07:10.811470Z","shell.execute_reply.started":"2023-11-28T16:07:10.805687Z","shell.execute_reply":"2023-11-28T16:07:10.810401Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"1080\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Train model","metadata":{}},{"cell_type":"markdown","source":"### Load model","metadata":{}},{"cell_type":"code","source":"# bnb_config = BitsAndBytesConfig(  \n#     load_in_4bit= True,\n#     bnb_4bit_quant_type= \"nf4\",\n#     bnb_4bit_compute_dtype= torch.bfloat16,\n#     bnb_4bit_use_double_quant= False,\n# )\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n#         model_hotamath_path,\n#     load_in_4bit=True,\n#     load_in_8bit= True,\n#     quantization_config=bnb_config,\n#     torch_dtype=torch.bfloat16,\n#     torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-28T16:07:18.255935Z","iopub.execute_input":"2023-11-28T16:07:18.256842Z","iopub.status.idle":"2023-11-28T16:07:24.220281Z","shell.execute_reply.started":"2023-11-28T16:07:18.256809Z","shell.execute_reply":"2023-11-28T16:07:24.219408Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    base_model,\n    model_max_length=MAX_TOKEN_MODEL,\n    padding_side=\"right\",\n    use_fast=False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:24.222270Z","iopub.execute_input":"2023-11-28T16:07:24.222658Z","iopub.status.idle":"2023-11-28T16:07:24.679962Z","shell.execute_reply.started":"2023-11-28T16:07:24.222622Z","shell.execute_reply":"2023-11-28T16:07:24.679051Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"special_tokens_dict = {\n    'additional_special_tokens': [DEFAULT_BOI_TOKEN, DEFAULT_EOI_TOKEN],\n    'pad_token': DEFAULT_PAD_TOKEN,\n    'bos_token': DEFAULT_BOS_TOKEN,\n    'eos_token': DEFAULT_EOS_TOKEN,\n    'unk_token': DEFAULT_UNK_TOKEN,\n}\nnum_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:24.681300Z","iopub.execute_input":"2023-11-28T16:07:24.681726Z","iopub.status.idle":"2023-11-28T16:07:24.703363Z","shell.execute_reply.started":"2023-11-28T16:07:24.681691Z","shell.execute_reply":"2023-11-28T16:07:24.702221Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"Embedding(50260, 768)"},"metadata":{}}]},{"cell_type":"code","source":"print(tokenizer.encode(\"{0} Hello, how are you? \\n{1} I'm fine, thank you!{2}\".format(\n    DEFAULT_BOI_TOKEN,\n    DEFAULT_EOI_TOKEN,\n    DEFAULT_EOS_TOKEN,\n)))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:24.706644Z","iopub.execute_input":"2023-11-28T16:07:24.707582Z","iopub.status.idle":"2023-11-28T16:07:24.715173Z","shell.execute_reply.started":"2023-11-28T16:07:24.707509Z","shell.execute_reply":"2023-11-28T16:07:24.714060Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"[50258, 18435, 11, 703, 389, 345, 30, 220, 198, 50257, 314, 1101, 3734, 11, 5875, 345, 0, 50256]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Check max token","metadata":{}},{"cell_type":"code","source":"max_token_of_dataset = 0\nlistLongToken = []\nfor i in range(len(dataset[\"text\"])):\n    text = dataset[\"text\"][i]\n    token_len = len(tokenizer.encode(text))\n    max_token_of_dataset = max(token_len, max_token_of_dataset)\n    if token_len > MAX_TOKEN_MODEL:\n        print(i, token_len, \"\\n\")\n        listLongToken.append(i)\n#         print(text)\nprint(max_token_of_dataset)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-28T16:07:24.716497Z","iopub.execute_input":"2023-11-28T16:07:24.716866Z","iopub.status.idle":"2023-11-28T16:07:27.035866Z","shell.execute_reply.started":"2023-11-28T16:07:24.716832Z","shell.execute_reply":"2023-11-28T16:07:27.033035Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (673 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"71 673 \n\n120 594 \n\n239 637 \n\n329 558 \n\n353 566 \n\n361 779 \n\n367 590 \n\n380 592 \n\n388 584 \n\n396 574 \n\n397 796 \n\n399 645 \n\n400 793 \n\n435 671 \n\n440 590 \n\n454 606 \n\n455 531 \n\n458 716 \n\n463 555 \n\n477 542 \n\n483 632 \n\n484 750 \n\n486 667 \n\n487 808 \n\n491 629 \n\n494 769 \n\n529 565 \n\n536 897 \n\n537 563 \n\n539 841 \n\n540 868 \n\n543 527 \n\n546 694 \n\n563 518 \n\n570 828 \n\n578 514 \n\n589 568 \n\n591 903 \n\n592 635 \n\n603 709 \n\n604 664 \n\n614 525 \n\n617 616 \n\n621 515 \n\n622 726 \n\n660 1383 \n\n662 542 \n\n724 729 \n\n742 696 \n\n754 943 \n\n761 1032 \n\n764 680 \n\n766 908 \n\n784 531 \n\n799 549 \n\n802 845 \n\n814 758 \n\n815 571 \n\n823 533 \n\n839 611 \n\n844 582 \n\n845 647 \n\n847 700 \n\n854 844 \n\n862 592 \n\n864 847 \n\n873 577 \n\n902 537 \n\n927 535 \n\n958 602 \n\n961 536 \n\n965 630 \n\n976 667 \n\n997 514 \n\n998 819 \n\n1000 660 \n\n1013 749 \n\n1023 548 \n\n1024 778 \n\n1026 1131 \n\n1027 626 \n\n1028 1068 \n\n1031 519 \n\n1044 955 \n\n1051 814 \n\n1053 613 \n\n1057 574 \n\n1060 561 \n\n1063 586 \n\n1072 582 \n\n1073 685 \n\n1079 528 \n\n1383\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Remove long token","metadata":{}},{"cell_type":"code","source":"print(len(listLongToken))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:27.037295Z","iopub.execute_input":"2023-11-28T16:07:27.037896Z","iopub.status.idle":"2023-11-28T16:07:27.044759Z","shell.execute_reply.started":"2023-11-28T16:07:27.037864Z","shell.execute_reply":"2023-11-28T16:07:27.042730Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"92\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset[\"text\"] = [dataset[\"text\"][i] for i in range(len(dataset[\"text\"])) if i not in listLongToken]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:27.046370Z","iopub.execute_input":"2023-11-28T16:07:27.046712Z","iopub.status.idle":"2023-11-28T16:07:27.059456Z","shell.execute_reply.started":"2023-11-28T16:07:27.046677Z","shell.execute_reply":"2023-11-28T16:07:27.058228Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Check\nmax_token_of_dataset = 0\nfor i in range(len(dataset[\"text\"])):\n    text = dataset[\"text\"][i]\n    token_len = len(tokenizer.encode(text))\n    max_token_of_dataset = max(token_len, max_token_of_dataset)\n    if token_len > MAX_TOKEN_MODEL:\n        print(i, token_len, \"\\n\")\nprint(max_token_of_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:27.061277Z","iopub.execute_input":"2023-11-28T16:07:27.062228Z","iopub.status.idle":"2023-11-28T16:07:28.880388Z","shell.execute_reply.started":"2023-11-28T16:07:27.062158Z","shell.execute_reply":"2023-11-28T16:07:28.879392Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"512\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Nice stuct dataset","metadata":{}},{"cell_type":"code","source":"dataset = Dataset.from_dict(dataset)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:28.881745Z","iopub.execute_input":"2023-11-28T16:07:28.882090Z","iopub.status.idle":"2023-11-28T16:07:28.896194Z","shell.execute_reply.started":"2023-11-28T16:07:28.882063Z","shell.execute_reply":"2023-11-28T16:07:28.895172Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"### DataCollator","metadata":{}},{"cell_type":"code","source":"from trl import DataCollatorForCompletionOnlyLM","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:28.899518Z","iopub.execute_input":"2023-11-28T16:07:28.899887Z","iopub.status.idle":"2023-11-28T16:07:28.907124Z","shell.execute_reply.started":"2023-11-28T16:07:28.899857Z","shell.execute_reply":"2023-11-28T16:07:28.906338Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"instruction_template = DEFAULT_BOI_TOKEN\nresponse_template = DEFAULT_EOI_TOKEN\ncollator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer, mlm=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:28.908206Z","iopub.execute_input":"2023-11-28T16:07:28.908504Z","iopub.status.idle":"2023-11-28T16:07:28.921333Z","shell.execute_reply.started":"2023-11-28T16:07:28.908479Z","shell.execute_reply":"2023-11-28T16:07:28.920448Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"### Setup model for train","metadata":{}},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:28.922625Z","iopub.execute_input":"2023-11-28T16:07:28.923112Z","iopub.status.idle":"2023-11-28T16:07:28.937027Z","shell.execute_reply.started":"2023-11-28T16:07:28.923071Z","shell.execute_reply":"2023-11-28T16:07:28.935863Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50260, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50260, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"# model = prepare_model_for_kbit_training(model)\n# peft_config = LoraConfig(\n#     lora_alpha=16,\n#     lora_dropout=0.1,\n#     r=64,\n#     bias=\"none\",\n#     task_type=\"CAUSAL_LM\",\n#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n# )\n# model = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:28.938393Z","iopub.execute_input":"2023-11-28T16:07:28.939274Z","iopub.status.idle":"2023-11-28T16:07:28.949153Z","shell.execute_reply.started":"2023-11-28T16:07:28.939244Z","shell.execute_reply":"2023-11-28T16:07:28.948188Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=25,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=1000,\n    logging_steps=10,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    save_total_limit = 1,\n    warmup_ratio=0.03,\n#     group_by_length=True,\n#     lr_scheduler_type=\"constant\",\n    report_to=\"none\"\n#     report_to=\"wandb\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:31.210894Z","iopub.execute_input":"2023-11-28T16:07:31.211325Z","iopub.status.idle":"2023-11-28T16:07:31.220345Z","shell.execute_reply.started":"2023-11-28T16:07:31.211293Z","shell.execute_reply":"2023-11-28T16:07:31.219256Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# import bitsandbytes as bnb\n# from torch import nn\n# from transformers.trainer_pt_utils import get_parameter_names\n\n# no_decay = [\"bias\", \"LayerNorm.weight\"]\n# optimizer_grouped_parameters = [{\n#     \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n#     \"weight_decay\": training_arguments.weight_decay,\n# },\n# {\n#     \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n#     \"weight_decay\": 0.0,\n# }]\n\n\n# optimizer_kwargs = {\n#     \"betas\": (training_arguments.adam_beta1, training_arguments.adam_beta2),\n#     \"eps\": training_arguments.adam_epsilon,\n# }\n# optimizer_kwargs[\"lr\"] = training_arguments.learning_rate\n# adam_bnb_optim = bnb.optim.Adam8bit(\n#     optimizer_grouped_parameters,\n#     betas=(training_arguments.adam_beta1, training_arguments.adam_beta2),\n#     eps=training_arguments.adam_epsilon,\n#     lr=training_arguments.learning_rate,\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = model.to(\"cuda:0\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:34.867213Z","iopub.execute_input":"2023-11-28T16:07:34.867697Z","iopub.status.idle":"2023-11-28T16:07:34.877248Z","shell.execute_reply.started":"2023-11-28T16:07:34.867652Z","shell.execute_reply":"2023-11-28T16:07:34.875871Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"print_trainable_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:35.030970Z","iopub.execute_input":"2023-11-28T16:07:35.031389Z","iopub.status.idle":"2023-11-28T16:07:35.038575Z","shell.execute_reply.started":"2023-11-28T16:07:35.031356Z","shell.execute_reply":"2023-11-28T16:07:35.037407Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"trainable params: 124442112 || all params: 124442112 || trainable%: 100.0\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n#     model = base_model,\n    train_dataset=dataset,\n#     peft_config=peft_config,\n    max_seq_length=MAX_TOKEN_MODEL,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n    data_collator=collator,\n#     optimizers=(adam_bnb_optim, None)\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:38.731292Z","iopub.execute_input":"2023-11-28T16:07:38.732202Z","iopub.status.idle":"2023-11-28T16:07:40.626117Z","shell.execute_reply.started":"2023-11-28T16:07:38.732163Z","shell.execute_reply":"2023-11-28T16:07:40.624960Z"},"trusted":true},"execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c2429b7426a43d89eb9394f477f54b7"}},"metadata":{}}]},{"cell_type":"code","source":"model.config.use_cache = False","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:07:40.628170Z","iopub.execute_input":"2023-11-28T16:07:40.629023Z","iopub.status.idle":"2023-11-28T16:07:40.634025Z","shell.execute_reply.started":"2023-11-28T16:07:40.628952Z","shell.execute_reply":"2023-11-28T16:07:40.632920Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"### Train and save","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-28T16:07:42.842980Z","iopub.execute_input":"2023-11-28T16:07:42.843755Z","iopub.status.idle":"2023-11-28T16:25:53.461854Z","shell.execute_reply.started":"2023-11-28T16:07:42.843718Z","shell.execute_reply":"2023-11-28T16:25:53.461003Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1550' max='1550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1550/1550 18:05, Epoch 25/25]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1010</td>\n      <td>0.006200</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.005100</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.004500</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.004900</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.005200</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.003700</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.004700</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.004000</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.004100</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.004100</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.003100</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.004700</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.003700</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.004700</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.003200</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.003400</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.003000</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.003400</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.004000</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.004100</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.003100</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.002900</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.002900</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.002900</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.002400</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.002400</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.002700</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.002400</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.002900</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.002400</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.001700</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.001700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1550, training_loss=0.0010797570225211882, metrics={'train_runtime': 1088.7784, 'train_samples_per_second': 22.686, 'train_steps_per_second': 1.424, 'total_flos': 5776981065216000.0, 'train_loss': 0.0010797570225211882, 'epoch': 25.0})"},"metadata":{}}]},{"cell_type":"code","source":"# model.save_pretrained(new_model)\ntrainer.model.save_pretrained(new_model)\nmodel.config.to_json_file(os.path.join(new_model, \"config.json\"))\ntokenizer.save_pretrained(new_model)\nwandb.finish()\nmodel.config.use_cache = True","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:25:53.463585Z","iopub.execute_input":"2023-11-28T16:25:53.463878Z","iopub.status.idle":"2023-11-28T16:25:54.669215Z","shell.execute_reply.started":"2023-11-28T16:25:53.463853Z","shell.execute_reply":"2023-11-28T16:25:54.668248Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"!zip -r BK-BigAI-Math.zip BK-BigAI-Math","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:25:54.670428Z","iopub.execute_input":"2023-11-28T16:25:54.670722Z","iopub.status.idle":"2023-11-28T16:26:23.977330Z","shell.execute_reply.started":"2023-11-28T16:25:54.670698Z","shell.execute_reply":"2023-11-28T16:26:23.976176Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"updating: BK-BigAI-Math/ (stored 0%)\nupdating: BK-BigAI-Math/special_tokens_map.json (deflated 80%)\nupdating: BK-BigAI-Math/vocab.json (deflated 68%)\nupdating: BK-BigAI-Math/added_tokens.json (deflated 26%)\nupdating: BK-BigAI-Math/model.safetensors (deflated 7%)\nupdating: BK-BigAI-Math/generation_config.json (deflated 24%)\nupdating: BK-BigAI-Math/merges.txt (deflated 53%)\nupdating: BK-BigAI-Math/config.json (deflated 52%)\nupdating: BK-BigAI-Math/tokenizer_config.json (deflated 73%)\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'BK-BigAI-Math.zip')","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:26:23.980358Z","iopub.execute_input":"2023-11-28T16:26:23.980790Z","iopub.status.idle":"2023-11-28T16:26:23.988548Z","shell.execute_reply.started":"2023-11-28T16:26:23.980743Z","shell.execute_reply":"2023-11-28T16:26:23.987414Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/BK-BigAI-Math.zip","text/html":"<a href='BK-BigAI-Math.zip' target='_blank'>BK-BigAI-Math.zip</a><br>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Evalution","metadata":{}},{"cell_type":"markdown","source":"### Download model","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\nhf_hub_download(repo_id=\"hotamago/ZAIC-2023-Model\", filename=\"Hota-Math.zip\", repo_type=\"model\", local_dir=\"/kaggle/working/\", local_dir_use_symlinks=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip -q -o Hota-Math.zip -d ./","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load model","metadata":{}},{"cell_type":"code","source":"dataset = Dataset.from_dict(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name_or_path = \"/kaggle/working/BK-BigAI-Math\"\n# bnb_config = BitsAndBytesConfig(  \n#     load_in_4bit= True,\n#     bnb_4bit_quant_type= \"nf4\",\n#     bnb_4bit_compute_dtype= torch.bfloat16,\n#     bnb_4bit_use_double_quant= False,\n# )\n# model = AutoModelForCausalLM.from_pretrained(\n#         model_name_or_path,\n#         load_in_4bit=True,\n#         quantization_config=bnb_config,\n#         torch_dtype=torch.bfloat16,\n#         device_map=\"auto\",\n#         trust_remote_code=True,\n# )\n\n# model = AutoModelForCausalLM.from_pretrained(\n#         model_name_or_path,\n#         torch_dtype=torch.bfloat16,\n#         device_map=\"auto\",\n#         trust_remote_code=True,\n# )\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name_or_path,\n#     torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n#     load_in_4bit=True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    model_name_or_path,\n    model_max_length=512,\n    padding_side=\"right\",\n    use_fast=False,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# special_tokens_dict = {'additional_special_tokens': ['[INST]','[/INST]']}\n# num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setup pipeline with auto answer get","metadata":{}},{"cell_type":"code","source":"pipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n    temperature=0.01,\n    top_p=0.01,\n    top_k=3,\n    repetition_penalty=1.1,\n    pad_token_id=tokenizer.eos_token_id\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:29:51.502660Z","iopub.execute_input":"2023-11-28T16:29:51.503593Z","iopub.status.idle":"2023-11-28T16:29:51.509091Z","shell.execute_reply.started":"2023-11-28T16:29:51.503556Z","shell.execute_reply":"2023-11-28T16:29:51.507959Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"import random\n# globalRegxCompire = \"0-9a-zA-Z\\.\\:\\-\\^\\! \"\ndef niceValueToCompire(x):\n#     x = re.sub(\"[^{0}]\".format(globalRegxCompire), \"\", x)\n    x = re.sub(\"[ \\t\\n]\", \"\", x)\n    return x\ndef autoLLMFormat(question, choises = None, debug=False):\n    prompt_template = ApplyPromptTemplate(question, choises, \"prompt_input_run\")\n    res = pipe(prompt_template)[0]['generated_text']\n    if debug:\n        print(res)\n    x = re.findall(\"### Answer:[\\n ](.+)\", res)\n    \n    if choises == None:\n        return x\n    \n    choises_compare = [niceValueToCompire(choise_pred) for choise_pred in choises]\n\n    if len(x) == 0:\n        return choises[random.randrange(0, len(choises))]\n    \n    x = niceValueToCompire(x[0])\n    \n    if (x not in choises_compare):\n        return choises[random.randrange(0, len(choises))]\n    \n    for i in range(len(choises_compare)):\n        if x == choises_compare[i]:\n            return choises[i]\n    \n    return \"wtf\"","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:29:52.001721Z","iopub.execute_input":"2023-11-28T16:29:52.002782Z","iopub.status.idle":"2023-11-28T16:29:52.011645Z","shell.execute_reply.started":"2023-11-28T16:29:52.002747Z","shell.execute_reply":"2023-11-28T16:29:52.010567Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"### Run evalution","metadata":{}},{"cell_type":"code","source":"count_proc_testcase = 0\ncount_pass_testcase = 0","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:29:52.641475Z","iopub.execute_input":"2023-11-28T16:29:52.642255Z","iopub.status.idle":"2023-11-28T16:29:52.646525Z","shell.execute_reply.started":"2023-11-28T16:29:52.642222Z","shell.execute_reply":"2023-11-28T16:29:52.645532Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"while count_proc_testcase < len(tokenized_val_dataset_raw):\n    tvdo = tokenized_val_dataset_raw[count_proc_testcase]\n    token_len = len(tokenizer.encode(ApplyPromptTemplate(tvdo['question'], tvdo['choices'], \"prompt_input_run\")))\n    if token_len >= 512:\n        print(\"Too much token\", token_len)\n    \n    startTime()\n    answer = autoLLMFormat(tvdo['question'], tvdo['choices'], True)\n    deltaTime = getTime()\n\n    if answer == tvdo['answer']:\n        count_pass_testcase += 1\n    \n    count_proc_testcase += 1\n    print(\"Testcase {0}, time: {1}, answer: {2} | {3}, Passed: {4}, IsPass: {5}\".format(\n        count_proc_testcase,\n        deltaTime,\n        answer,\n        tvdo['answer'],\n        count_pass_testcase,\n        (answer == tvdo['answer'])\n    ))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-28T16:29:53.073767Z","iopub.execute_input":"2023-11-28T16:29:53.074588Z","iopub.status.idle":"2023-11-28T16:30:21.928708Z","shell.execute_reply.started":"2023-11-28T16:29:53.074552Z","shell.execute_reply":"2023-11-28T16:30:21.927144Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"<|human|> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nMột cái bánh pizza có giá 64 nghìn đồng. \\frac{1}{8} cái bánh có giá bao nhiêu nghìn đồng?\n\n### Choices:\nA. 7 nghìn đồng\nB. 9 nghìn đồng \nC. 8 nghìn đồng\nD. 10 nghìn đồng\n<|assistant|> \n\n### Explanation:\n Hãy suy nghĩ từng bước một.\n Vì vậy, số tiếp theo trong dây số lít quay phần thập phân của kém số cho hơn chục (vì 6 + 3 = 5).\n Tuy nhiên, ép chia đều vào các không làm việc nghìn, ghép yêu cầu tìm cùng một của ki-lông thị rằng chrom·eblink là 100. Do đó, đáp án C. 8 nghìn đồng.\n\n### Answer:\nC. 8 nghìn đồng \nTestcase 1, time: 3.9238297939300537, answer: C. 8 nghìn đồng | C. 8 nghìn đồng, Passed: 1, IsPass: True\n<|human|> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nMột chiếc ca nô xuôi dòng từ A đến B với vận tốc 25 km/giờ. Nếu vận tốc dòng nước là 2,5 km/giờ thì vận tốc của ca nô khi ngược dòng là:\n\n### Choices:\nA. 22,5 km/giờ\nB. 22 km/giờ\nC. 20 km/giờ\nD. 27,5 km/giờ\n<|assistant|> \n\n### Explanation:\n Hãy suy nghĩ từng bước một.\n Vậy, ta có 7 giờ trong việc với 50km (1 giờ = 10 gi).\n\nSauber đó, ta áp dù đáp án C:\n- Pgờ : 100 000;50 200 ; 65 210 = 26 000[pg]\nNếu vận tốc dòng nước là 2,5 km/giờ thì vận tốc của ca nô không có giờ.\n\n### Answer:\nC. 20 km/giờ \nTestcase 2, time: 3.316418170928955, answer: C. 20 km/giờ | C. 20 km/giờ, Passed: 2, IsPass: True\n<|human|> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nMột chiếc máy khâu được phát minh vào năm 1890. Hỏi chiếc máy khâu đó được phát minh vào thế kỉ nào?\n\n### Choices:\nA. Thế kỉ XVIII\nB.Thế kì XX\nC. Thế kỉ XIX\nD.Thế kỉ XXI\n<|assistant|> \n\n### Explanation:\n Hãy suy nghĩ từng bước một.\n Vậy chiếc được phát Minh vào năm là 18,000 (vì 18 x 9 = 37,5).\n\n### Answer:\nD. Thế kỉ XXI \nTestcase 3, time: 1.2347304821014404, answer: D.Thế kỉ XXI | C. Thế kỉ XIX, Passed: 2, IsPass: False\n<|human|> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nMột chiếc tàu ngược dòng với vận tốc 15km/giờ. Biết vận tốc của dòng nước là 3km/ giờ. Tính vận tốc xuôi dòng của chiếc tàu?\n\n### Choices:\nA. 21km/giờ\nB. 20km/giờ\nC. 18km/giờ\nD. 12km/giờ\n<|assistant|> \n\n### Explanation:\n Hãy suy nghĩ từng bước một.\n Vậy, để tính số ki-lô trong diện cho khi ta thực hiện phép chia 1000 times (vì 1 km = 10 giờ).\n\nTrong trường hợp này, chiếc tàu án D là đúng.\n\nTiếp theo, tiếp theo yêu cầu tìm không có giữan tổng hai ghép ban đầu.\n\nVậy, chiếc tàu ngược dòng của dòng là 3km/giờ.\n\n### Answer:\nD. 12km/giờ \nTestcase 4, time: 4.111404180526733, answer: D. 12km/giờ | A. 21km/giờ, Passed: 2, IsPass: False\n<|human|> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nMột chiếc tàu xuôi dòng từ A đến B với vận tốc 12km/giờ, sau đó lại ngược dòng từ B về A với vận tốc 8km/giờ. Tính thời gian chiều tàu xuôi dòng từ A đến B rồi từ B về A biết độ dài AB là 24km.\n\n### Choices:\nA. 5 giờ\nB. 4 giờ\nC. 3 giờ\nD. 6 giờ\n<|assistant|> \n\n### Explanation:\n Hãy suy nghĩ từng bước một.\n Ta có: 12 km/giờ = 1 000 (kg)Chiữ số hộp phân cho 10:\n Total kiờ = 1 200 (kg)Tổng số trong mỗi gam chia ở hệ số cho 10:\n Humidity ìm : 2 < 3 < 4 < 5\n Vậy thời gian chiều tàu xôi dòng từ A đến B rồi từ B về A biết độ dài AB là 24 km.\n\n### Answer:\nA. 5 giờ \nTestcase 5, time: 3.3874173164367676, answer: A. 5 giờ | A. 5 giờ, Passed: 3, IsPass: True\n<|human|> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n“m3 = ……..m3”. Số thập phân thích hợp điền vào chỗ chấm là:\n\n### Choices:\nA. 1,4\nB. 0,25\nC. 2,5\nD. 0,14\n<|assistant|> \n\n### Explanation:\n Hãy suy nghĩ từng bước một.\n Vì 3 + 7 + 4 = 10. Mỗi số yêu có chúng ta cần biết rằng 1 quyd kai bằng 10.000 mét khối số trong ngoặc trước, sau đó thực hiện việc loại bỏ số 2. Vì vậy, để xác định mỗi cho ghép lấim nhau, ta cần giải phương trình nhau.\n\n10.000 + (1,84) = 20.000 m2\n\nSauy hai kết thập phân thích hợp điền vào chỗ chấm là 0,14.\n\n### Answer:\nD. 0,14 \nTestcase 6, time: 4.849198818206787, answer: D. 0,14 | B. 0,25, Passed: 3, IsPass: False\n<|human|> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nMột chú chuột túi khi mới sinh ra chỉ nặng 1 g nhưng khi trưởng thành nặng đến 90 kg. Vậy chuột túi trưởng thành nặng gấp chuột túi lúc mới sinh số lần là:\n\n### Choices:\nA. 90 lần\nB. 900 lần\nC. 9 000 lần\nD. 90 000 lần\n<|assistant|> \n\n### Explanation:\n Hãy suy nghĩ từng bước một.\n Mỗi ghép chia hìnoscope sau đơn vị: 100 – 70 = 200 (kg)\n Chuột túi trưởng thành nặng gấp chuột túi lúc mới sinh số lần là: 190 : 2 = 9 000 (lần)\n Đáp số: 9 000 lần\n\n### Answer:\nC. 9 000 lần \nTestcase 7, time: 2.7087104320526123, answer: C. 9 000 lần | D. 90 000 lần, Passed: 3, IsPass: False\n<|human|> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nMột cốc trà sữa cỡ lớn có giá là 20 000 đồng, một cốc trà sữa cỡ nhỏ có giá là 14 500 đồng. An mua 1 cốc cỡ to và 2 cốc cỡ nhỏ. Vậy số tiền An cần trả là:\n\n### Choices:\nA. 34 500 đồng\nB. 49 000 đồng\nC. 48 000 đồng\nD. 35 000 đồng\n<|assistant|> \n\n### Explanation:\n Hãy suy nghĩ từng bước một.\n Mỗi cốn trả việc chuyển đổi từ ngếp phân 10 (vì 6 + 7 = 33,5) ta cần tìm số tiền An câu trả lời hữu số 5 thành 0. Số tiền An câu trả lời là 3 * $5 + 0,05 = 36$, cm old cốc trả là 36 000 đồng.\n\nSố tiền An cần trả là: 36 000 : 8 = 49 000 (đồng).\n\n### Answer:\nA. 34 500 đồng \nTestcase 8, time: 3.925727605819702, answer: A. 34 500 đồng | B. 49 000 đồng, Passed: 3, IsPass: False\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[58], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToo much token\u001b[39m\u001b[38;5;124m\"\u001b[39m, token_len)\n\u001b[1;32m      7\u001b[0m startTime()\n\u001b[0;32m----> 8\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mautoLLMFormat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtvdo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtvdo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchoices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m deltaTime \u001b[38;5;241m=\u001b[39m getTime()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m tvdo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n","Cell \u001b[0;32mIn[56], line 9\u001b[0m, in \u001b[0;36mautoLLMFormat\u001b[0;34m(question, choises, debug)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mautoLLMFormat\u001b[39m(question, choises \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      8\u001b[0m     prompt_template \u001b[38;5;241m=\u001b[39m ApplyPromptTemplate(question, choises, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_input_run\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m debug:\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28mprint\u001b[39m(res)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:208\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    168\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1134\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1146\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1147\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1148\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:271\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1719\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1711\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1712\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1713\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1714\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1715\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1716\u001b[0m     )\n\u001b[1;32m   1718\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1736\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1737\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1742\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1743\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2801\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2798\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2800\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2801\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2804\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2805\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2809\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1074\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:888\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    877\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    878\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m         output_attentions,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:426\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    423\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs \u001b[38;5;241m+\u001b[39m cross_attn_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# add cross attentions if we output attention weights\u001b[39;00m\n\u001b[1;32m    425\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 426\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:159\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 159\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:290\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                 fp16_statistics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_map[name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCB\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m    286\u001b[0m         set_module_tensor_to_device(\n\u001b[1;32m    287\u001b[0m             module, name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_map[name], fp16_statistics\u001b[38;5;241m=\u001b[39mfp16_statistics\n\u001b[1;32m    288\u001b[0m         )\n\u001b[0;32m--> 290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m)\u001b[49m, send_to_device(\n\u001b[1;32m    291\u001b[0m     kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device, skip_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_keys\n\u001b[1;32m    292\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:151\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03mRecursively sends the elements in a nested list/tuple/dictionary of tensors to a given device.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    The same data structure as `tensor` with all tensors sent to the proper device.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhonor_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, Mapping):\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(skip_keys, \u001b[38;5;28mstr\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:83\u001b[0m, in \u001b[0;36mhonor_type\u001b[0;34m(obj, generator)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(generator))\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:152\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03mRecursively sends the elements in a nested list/tuple/dictionary of tensors to a given device.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    The same data structure as `tensor` with all tensors sent to the proper device.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m honor_type(\n\u001b[0;32m--> 152\u001b[0m         tensor, (\u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tensor)\n\u001b[1;32m    153\u001b[0m     )\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, Mapping):\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(skip_keys, \u001b[38;5;28mstr\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:167\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"## Run public test dataset","metadata":{}},{"cell_type":"code","source":"result_test = []\nif os.path.exists(os.path.join(\"result\", \"result.txt\")):\n    with open(os.path.join(\"result\", \"result.txt\"), \"r\") as f:\n        result_test = f.read().split(\"\\n\")\ncount_id = len(result_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(test_data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"while count_id < len(test_data):\n    one_test_data = test_data[count_id]\n    startTime()\n    answer = autoLLMFormat(one_test_data['question'], one_test_data['choices'], False)\n    deltaTime = getTime()\n    result_test.append(\"{0}\".format(answer))\n    count_id += 1\n    if count_id%10 == 0:\n        with open(os.path.join(\"result\", \"result.txt\"), \"w\", encoding='utf-8') as f:\n            f.write(\"\\n\".join(result_test))\n    print(\"Testcase {0}, time: {1}, answer: {2}\".format(count_id, deltaTime, answer))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(os.path.join(\"result\", \"result.txt\"), \"w\", encoding='utf-8') as f:\n    f.write(\"\\n\".join(result_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n\".join(result_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}