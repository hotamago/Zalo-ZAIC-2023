{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install","metadata":{}},{"cell_type":"code","source":"!pip3 install huggingface-hub","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade openai\n!pip install --upgrade pydantic","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U peft\n%pip install -U accelerate\n%pip install -U trl","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !mkdir MetaMath-Mistral-7B\n# !huggingface-cli download meta-math/MetaMath-Mistral-7B --local-dir MetaMath-Mistral-7B --local-dir-use-symlinks False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install sentencepiece","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# VERSION = \"1.11\"\n# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py > /dev/null\n# !python pytorch-xla-env-setup.py --version $VERSION  > /dev/null","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import library","metadata":{}},{"cell_type":"code","source":"# import tensorflow as tf\n# import tensorflow_hub as hub\n# print(\"Tensorflow version \" + tf.__version__)\n# AUTO = tf.data.experimental.AUTOTUNE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Detect TPU, return appropriate distribution strategy\n# try:\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n#     print('Running on TPU ', tpu.master())\n# except ValueError:\n#     tpu = None\n\n# if tpu:\n#     tf.config.experimental_connect_to_cluster(tpu)\n#     tf.tpu.experimental.initialize_tpu_system(tpu)\n#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# else:\n#     strategy = tf.distribute.get_strategy() \n\n# print(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nimport os,torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:12:49.217208Z","iopub.execute_input":"2023-11-28T15:12:49.218231Z","iopub.status.idle":"2023-11-28T15:12:57.340659Z","shell.execute_reply.started":"2023-11-28T15:12:49.218193Z","shell.execute_reply":"2023-11-28T15:12:57.339546Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# # PyTorch XLA-specific imports\n# import torch_xla.core.xla_model as xm\n# import torch_xla.distributed.parallel_loader as pl\n# import torch_xla.distributed.xla_multiprocessing as xmp\n# import torch_xla.debug.metrics as met","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:12:57.342443Z","iopub.execute_input":"2023-11-28T15:12:57.343104Z","iopub.status.idle":"2023-11-28T15:12:57.347942Z","shell.execute_reply.started":"2023-11-28T15:12:57.343072Z","shell.execute_reply":"2023-11-28T15:12:57.346844Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nsecret_wandb = user_secrets.get_secret(\"wandb\")","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:12:57.349562Z","iopub.execute_input":"2023-11-28T15:12:57.349950Z","iopub.status.idle":"2023-11-28T15:12:58.185371Z","shell.execute_reply.started":"2023-11-28T15:12:57.349914Z","shell.execute_reply":"2023-11-28T15:12:58.184519Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login --token $secret_hf","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:12:58.188121Z","iopub.execute_input":"2023-11-28T15:12:58.188894Z","iopub.status.idle":"2023-11-28T15:12:59.755989Z","shell.execute_reply.started":"2023-11-28T15:12:58.188853Z","shell.execute_reply":"2023-11-28T15:12:59.754657Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# wandb.login(key = secret_wandb)\n# run = wandb.init(\n# #     project='Fine tuning MetaMath mistral 7B - ZAIC',\n#     project='Fine tuning openchat 7B - ZAIC',\n#     job_type=\"training\", \n#     anonymous=\"allow\"\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# base_model = \"meta-math/MetaMath-Mistral-7B\"\n# base_model = \"gpt2-xl\"\nbase_model = \"gpt2\"\n# base_model = \"openchat/openchat_3.5\"\nnew_model = \"BK-BigAI-Math\"\nmodel_hotamath_path = \"/kaggle/working/BK-BigAI-Math\"","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:12:59.757772Z","iopub.execute_input":"2023-11-28T15:12:59.758209Z","iopub.status.idle":"2023-11-28T15:12:59.764128Z","shell.execute_reply.started":"2023-11-28T15:12:59.758171Z","shell.execute_reply":"2023-11-28T15:12:59.763107Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Download dataset","metadata":{}},{"cell_type":"code","source":"!mkdir dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\nhf_hub_download(repo_id=\"hotamago/ZAIC-2023\", filename=\"Elementary Maths Solving/test.zip\", revision=\"main\", repo_type=\"dataset\", local_dir=\"dataset\", local_dir_use_symlinks=False)\nhf_hub_download(repo_id=\"hotamago/ZAIC-2023\", filename=\"Elementary Maths Solving/train.zip\", revision=\"main\", repo_type=\"dataset\", local_dir=\"dataset\", local_dir_use_symlinks=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo apt-get install unzip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir datasetRaw\n!unzip -q -o \"dataset/Elementary Maths Solving/test.zip\" -d \"datasetRaw\"\n!unzip -q -o \"dataset/Elementary Maths Solving/train.zip\" -d \"datasetRaw\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport re\nimport time","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:12:59.765542Z","iopub.execute_input":"2023-11-28T15:12:59.765821Z","iopub.status.idle":"2023-11-28T15:12:59.781813Z","shell.execute_reply.started":"2023-11-28T15:12:59.765798Z","shell.execute_reply":"2023-11-28T15:12:59.780943Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_data = None\ntest_data = None\nwith open(os.path.join(\"datasetRaw\", \"train\", \"/kaggle/working/datasetRaw/math_train.json\"), \"r\") as f:\n    train_data = json.loads(f.read())['data']\nwith open(os.path.join(\"datasetRaw\", \"test\", \"/kaggle/working/datasetRaw/math_test.json\"), \"r\") as f:\n    test_data = json.loads(f.read())['data']","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:12:59.783043Z","iopub.execute_input":"2023-11-28T15:12:59.783341Z","iopub.status.idle":"2023-11-28T15:12:59.808565Z","shell.execute_reply.started":"2023-11-28T15:12:59.783316Z","shell.execute_reply":"2023-11-28T15:12:59.807602Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_data[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:12:59.809781Z","iopub.execute_input":"2023-11-28T15:12:59.810123Z","iopub.status.idle":"2023-11-28T15:12:59.818357Z","shell.execute_reply.started":"2023-11-28T15:12:59.810097Z","shell.execute_reply":"2023-11-28T15:12:59.817276Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'id': '1',\n 'question': 'Một người bán hàng bỏ ra 80,000 đồng tiền vốn và bị lỗ 6%. Để tính số tiền lỗ ta phải tính',\n 'choices': ['A. 80,000 : 6',\n  'B. 80,000 x 6',\n  'C. 80,000 : (6 x 100)',\n  'D. (80,000 x 6) : 100'],\n 'explanation': 'Theo đề bài, số tiền lỗ bằng 6% của 80 000 đồng . Để tìm số tiền lỗ ta có thể lấy 80 000 chia cho 100 rồi nhân với 6 (tức là 80 000 : 100 × 6) hoặc lấy 80000 nhân với 6 rồi chia cho 100 (tức là 80 000 × 6 : 100).',\n 'answer': 'D. (80,000 x 6) : 100'}"},"metadata":{}}]},{"cell_type":"code","source":"test_data[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:12:59.819813Z","iopub.execute_input":"2023-11-28T15:12:59.820501Z","iopub.status.idle":"2023-11-28T15:12:59.829703Z","shell.execute_reply.started":"2023-11-28T15:12:59.820463Z","shell.execute_reply":"2023-11-28T15:12:59.828765Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'id': '01-0203',\n 'question': 'Một cửa hàng đã bán 30% số hàng hiện có và thu được 15 000 000 đồng. Hỏi nếu bán hết hàng thì cửa hàng thu được bao nhiêu tiền?',\n 'choices': ['A. 4 500 000 đồng',\n  'B. 45 000 000 đồng',\n  'C. 50 000 000 đồng',\n  'D. 450 000 000 đồng']}"},"metadata":{}}]},{"cell_type":"code","source":"MAX_TOKEN_MODEL = 512","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:12:59.832622Z","iopub.execute_input":"2023-11-28T15:12:59.832888Z","iopub.status.idle":"2023-11-28T15:12:59.840856Z","shell.execute_reply.started":"2023-11-28T15:12:59.832865Z","shell.execute_reply":"2023-11-28T15:12:59.840096Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"DEFAULT_PAD_TOKEN = \"[PAD]\"\nDEFAULT_EOS_TOKEN = \"<|endoftext|>\" # \"<|end_of_turn|>\" # \"</s>\"\nDEFAULT_BOS_TOKEN = \"<|endoftext|>\" # \"<s>\"\nDEFAULT_UNK_TOKEN = \"<|endoftext|>\" # \"<unk>\"\nDEFAULT_BOI_TOKEN = \"<|human|>\" # \"Human:\" # \"[INST]\"\nDEFAULT_EOI_TOKEN = \"<|assistant|>\" # \"Assistant:\" # \"[/INST]\"\nPROMPT_DICT = {\n    \"prompt_input\": (\n        \"Below is an instruction that describes a task, paired with the choices, one of the choices is the correct answer to the request. \"\n        \"Write a response that appropriately completes the request.\\n\\n\"\n        \"### Instruction:\\n{instruction}\\n\\n### Choices:\\n{choices}\"\n    ),\n    \"prompt_input_run\": (\n        DEFAULT_BOI_TOKEN + \" Below is an instruction that describes a task. \"\n        \"Write a response that appropriately completes the request.\\n\\n\"\n        \"### Instruction:\\n{instruction}\\n\\n### Choices:\\n{choices}\"\n        \"\\n\" + DEFAULT_EOI_TOKEN + \" \\n\\n\"\n#         \"### Explanation:\\n Let's think step by step.\\n\"\n        \"### Explanation:\\n Hãy suy nghĩ từng bước một.\\n\"\n    ),\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:12:59.842098Z","iopub.execute_input":"2023-11-28T15:12:59.842392Z","iopub.status.idle":"2023-11-28T15:12:59.853671Z","shell.execute_reply.started":"2023-11-28T15:12:59.842367Z","shell.execute_reply":"2023-11-28T15:12:59.852771Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"timeGlobal = 0\ndef startTime():\n    global timeGlobal\n    timeGlobal = time.time()\ndef getTime():\n    return (time.time() - timeGlobal)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:12:59.854909Z","iopub.execute_input":"2023-11-28T15:12:59.855280Z","iopub.status.idle":"2023-11-28T15:12:59.866303Z","shell.execute_reply.started":"2023-11-28T15:12:59.855248Z","shell.execute_reply":"2023-11-28T15:12:59.865554Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def ApplyPromptTemplate(instruction, choices, typeP = \"prompt_input\"):\n    return PROMPT_DICT[typeP].format(instruction = instruction, choices = \"\\n\".join(choices))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:12:59.867539Z","iopub.execute_input":"2023-11-28T15:12:59.868253Z","iopub.status.idle":"2023-11-28T15:12:59.878676Z","shell.execute_reply.started":"2023-11-28T15:12:59.868217Z","shell.execute_reply":"2023-11-28T15:12:59.877797Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Add explantion by GPT 3.5 Tubo","metadata":{}},{"cell_type":"code","source":"from openai import OpenAI\nclient = OpenAI(\n    api_key=user_secrets.get_secret(\"OPENAI_API_KEY\"),\n#     organization='org-j48waUrvSOM1n0J1SLXiAr8n',\n)\n\ndef autoGPTAddExplantion(problem, answer):\n    response = client.chat.completions.create(\n      model=\"gpt-3.5-turbo\",\n      messages=[\n        {\"role\": \"system\", \"content\": \"Explan by Vietnamese step-by-step for given answer to given problem.\\nRule:\\n- No markdown format\\n- Given answer always true for given problem\\n- No title\\n- Short explantion\\n\"},\n        {\"role\": \"user\", \"content\": f\"### Problem:\\n{problem}\\n\\n### Answer:\\n{answer}\"},\n        {\"role\": \"system\", \"content\": \"Giải thích: \"},\n      ],\n#       max_tokens=512,\n      temperature=0,\n      top_p=1.0,\n#       top_k=50,\n    )\n    return response.choices[0].message.content","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"startTime()\ncntSt = 0\ni = 0\ntimeWait = 20\nwhile i < len(train_data):\n    singleData = train_data[i]\n    if \"explanation\" in singleData.keys():\n        i += 1\n        continue\n    print(f\"Runing testcase ({i})\")\n    \n    try:\n        res = autoGPTAddExplantion(singleData['question'], singleData['answer'])\n    except Exception as inst:\n        print(type(inst))\n        print(inst)\n        \n        time.sleep(timeWait)\n        timeWait *= 2\n        if timeWait > 80:\n            break\n        print\n        continue\n    \n    timeWait = 20\n        \n    cntSt += 1\n    train_data[i]['explanation'] = res\n    print(f\"Done testcase ({i})\")\n    #     print(train_data[i])\n    if cntSt%3 == 0:\n        deltaTime = getTime()\n        if (60 - deltaTime) > -1:\n            time.sleep(60 - deltaTime + 2)\n        startTime()\n        cntSt = 0\n    \n    i += 1\nprint(\"Done all testcase\")","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/working/datasetRaw/math_train.json\", \"w\", encoding='utf-8') as f:\n    json.dump({\n        \"__count__\": len(train_data),\n        \"data\": train_data\n    }, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add data by education program","metadata":{}},{"cell_type":"markdown","source":"Todo: Create data generation algorithms for each chapter.\n\nBelow is class base, change \\_call function to a function that create radom question for that chapter","metadata":{}},{"cell_type":"code","source":"class BaseGenChapter:\n    def __init__(*args, **kwargs):\n        pass\n    \n    def _call(self, *args, **kwargs) -> str:\n        raise NotImplementedError(\"_call not Implement yet\")\n    \n    def call(self, *args, **kwargs) -> str:\n        return self._call(*args, **kwargs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocess data","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\nimport random","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:13:01.093103Z","iopub.execute_input":"2023-11-28T15:13:01.094112Z","iopub.status.idle":"2023-11-28T15:13:01.098202Z","shell.execute_reply.started":"2023-11-28T15:13:01.094068Z","shell.execute_reply":"2023-11-28T15:13:01.097273Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# random.shuffle(train_data)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:13:01.253141Z","iopub.execute_input":"2023-11-28T15:13:01.253542Z","iopub.status.idle":"2023-11-28T15:13:01.258296Z","shell.execute_reply.started":"2023-11-28T15:13:01.253510Z","shell.execute_reply":"2023-11-28T15:13:01.257214Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"num_train_dataset = len(train_data)\nvalition_radio = 0.1\ntokenized_train_dataset_raw = train_data[:-int(num_train_dataset*valition_radio)]\ntokenized_val_dataset_raw = train_data[-int(num_train_dataset*valition_radio):]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:13:01.413101Z","iopub.execute_input":"2023-11-28T15:13:01.413912Z","iopub.status.idle":"2023-11-28T15:13:01.419303Z","shell.execute_reply.started":"2023-11-28T15:13:01.413866Z","shell.execute_reply":"2023-11-28T15:13:01.418305Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"datasetStruct = {\"input\":[], \"output\":[]}\ndataset = {\"text\":[]}\nnum_train_dataset = len(tokenized_train_dataset_raw)\nfor i in range(num_train_dataset):\n    ttdro = tokenized_train_dataset_raw[i]\n    \n    if \"explanation\" not in ttdro.keys():\n        continue\n    \n    input_content = \"{0} {1}\".format(\n        DEFAULT_BOI_TOKEN,\n        ApplyPromptTemplate(ttdro['question'], ttdro['choices']),\n    )\n    datasetStruct[\"input\"].append(input_content)\n    \n    if \"explanation\" not in ttdro.keys():\n        output_content = \"\\n{0} \\n\\n{1}\\n\\n{2} {3}\".format(\n            DEFAULT_EOI_TOKEN,\n            \"### Explanation:\\nNo explanation{0}\",\n            \"### Answer:\\n{0}\".format(ttdro['answer']),\n            DEFAULT_EOS_TOKEN,\n        )\n    else:\n        output_content = \"\\n{0} \\n\\n{1}\\n\\n{2} {3}\".format(\n            DEFAULT_EOI_TOKEN,\n            \"### Explanation:\\n{0}\".format(ttdro['explanation']),\n            \"### Answer:\\n{0}\".format(ttdro['answer']),\n            DEFAULT_EOS_TOKEN,\n        )\n    datasetStruct[\"output\"].append(output_content)\n    \n    dataset[\"text\"].append(input_content + output_content)\n#     <s>[INST][/INST] </s>","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:13:01.521335Z","iopub.execute_input":"2023-11-28T15:13:01.521727Z","iopub.status.idle":"2023-11-28T15:13:01.539336Z","shell.execute_reply.started":"2023-11-28T15:13:01.521696Z","shell.execute_reply":"2023-11-28T15:13:01.538327Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print(dataset[\"text\"][32])","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:13:01.656527Z","iopub.execute_input":"2023-11-28T15:13:01.656875Z","iopub.status.idle":"2023-11-28T15:13:01.662392Z","shell.execute_reply.started":"2023-11-28T15:13:01.656850Z","shell.execute_reply":"2023-11-28T15:13:01.661260Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"<|human|> Below is an instruction that describes a task, paired with the choices, one of the choices is the correct answer to the request. Write a response that appropriately completes the request.\n\n### Instruction:\nNgày thứ nhất, bác Thái thu hoạch được 250 kg nhãn. Ngày thứ hai, số ki- lô-gam nhãn bác Thái thu hoạch được đã giảm đi 2 lần so với ngày thứ nhất. Vậy cả hai ngày bác Thái thu hoạch được số ki-lô-gam nhãn là:\n\n### Choices:\nA. 500 kg\nB. 750 kg\nC. 125kg\nD. 375 kg\n<|assistant|> \n\n### Explanation:\nNgày thứ hai, bác Thái thu hoạch được số ki-lô-ham nhãn là: 250 : 2 = 125 (kg) \n Cả hai ngày bác Thái thu hoạch được số ki-lô-gam nhãn là: $250 + 125 = 375$ (kg)\n Đáp số: 375 kg\n\n### Answer:\nD. 375 kg <|endoftext|>\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(dataset[\"text\"]))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:13:02.140703Z","iopub.execute_input":"2023-11-28T15:13:02.141648Z","iopub.status.idle":"2023-11-28T15:13:02.146619Z","shell.execute_reply.started":"2023-11-28T15:13:02.141615Z","shell.execute_reply":"2023-11-28T15:13:02.145437Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"1080\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Train model","metadata":{}},{"cell_type":"markdown","source":"### Load model","metadata":{}},{"cell_type":"code","source":"# bnb_config = BitsAndBytesConfig(  \n#     load_in_4bit= True,\n#     bnb_4bit_quant_type= \"nf4\",\n#     bnb_4bit_compute_dtype= torch.bfloat16,\n#     bnb_4bit_use_double_quant= False,\n# )\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n#         model_hotamath_path,\n#     load_in_4bit=True,\n#     load_in_8bit= True,\n#     quantization_config=bnb_config,\n#     torch_dtype=torch.bfloat16,\n#     torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-28T15:13:04.912801Z","iopub.execute_input":"2023-11-28T15:13:04.913544Z","iopub.status.idle":"2023-11-28T15:13:11.163619Z","shell.execute_reply.started":"2023-11-28T15:13:04.913503Z","shell.execute_reply":"2023-11-28T15:13:11.162669Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    base_model,\n    model_max_length=MAX_TOKEN_MODEL,\n    padding_side=\"right\",\n    use_fast=False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:13:11.165185Z","iopub.execute_input":"2023-11-28T15:13:11.165527Z","iopub.status.idle":"2023-11-28T15:13:11.625158Z","shell.execute_reply.started":"2023-11-28T15:13:11.165500Z","shell.execute_reply":"2023-11-28T15:13:11.624187Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"special_tokens_dict = {\n    'additional_special_tokens': [DEFAULT_BOI_TOKEN, DEFAULT_EOI_TOKEN],\n    'pad_token': DEFAULT_PAD_TOKEN,\n    'bos_token': DEFAULT_BOS_TOKEN,\n    'eos_token': DEFAULT_EOS_TOKEN,\n    'unk_token': DEFAULT_UNK_TOKEN,\n}\nnum_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:13:11.626475Z","iopub.execute_input":"2023-11-28T15:13:11.626809Z","iopub.status.idle":"2023-11-28T15:13:11.645332Z","shell.execute_reply.started":"2023-11-28T15:13:11.626781Z","shell.execute_reply":"2023-11-28T15:13:11.644235Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"Embedding(50260, 768)"},"metadata":{}}]},{"cell_type":"code","source":"print(tokenizer.encode(\"{0} Hello, how are you? \\n{1} I'm fine, thank you!{2}\".format(\n    DEFAULT_BOI_TOKEN,\n    DEFAULT_EOI_TOKEN,\n    DEFAULT_EOS_TOKEN,\n)))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:13:11.647416Z","iopub.execute_input":"2023-11-28T15:13:11.647772Z","iopub.status.idle":"2023-11-28T15:13:11.664486Z","shell.execute_reply.started":"2023-11-28T15:13:11.647744Z","shell.execute_reply":"2023-11-28T15:13:11.663372Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"[50258, 18435, 11, 703, 389, 345, 30, 220, 198, 50257, 314, 1101, 3734, 11, 5875, 345, 0, 50256]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Check max token","metadata":{}},{"cell_type":"code","source":"max_token_of_dataset = 0\nlistLongToken = []\nfor i in range(len(dataset[\"text\"])):\n    text = dataset[\"text\"][i]\n    token_len = len(tokenizer.encode(text))\n    max_token_of_dataset = max(token_len, max_token_of_dataset)\n    if token_len > MAX_TOKEN_MODEL:\n        print(i, token_len, \"\\n\")\n        listLongToken.append(i)\n#         print(text)\nprint(max_token_of_dataset)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-28T15:14:47.106279Z","iopub.execute_input":"2023-11-28T15:14:47.107056Z","iopub.status.idle":"2023-11-28T15:14:49.263971Z","shell.execute_reply.started":"2023-11-28T15:14:47.107020Z","shell.execute_reply":"2023-11-28T15:14:49.262875Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"71 673 \n\n120 594 \n\n239 637 \n\n329 558 \n\n353 566 \n\n361 779 \n\n367 590 \n\n380 592 \n\n388 584 \n\n396 574 \n\n397 796 \n\n399 645 \n\n400 793 \n\n435 671 \n\n440 590 \n\n454 606 \n\n455 531 \n\n458 716 \n\n463 555 \n\n477 542 \n\n483 632 \n\n484 750 \n\n486 667 \n\n487 808 \n\n491 629 \n\n494 769 \n\n529 565 \n\n536 897 \n\n537 563 \n\n539 841 \n\n540 868 \n\n543 527 \n\n546 694 \n\n563 518 \n\n570 828 \n\n578 514 \n\n589 568 \n\n591 903 \n\n592 635 \n\n603 709 \n\n604 664 \n\n614 525 \n\n617 616 \n\n621 515 \n\n622 726 \n\n660 1383 \n\n662 542 \n\n724 729 \n\n742 696 \n\n754 943 \n\n761 1032 \n\n764 680 \n\n766 908 \n\n784 531 \n\n799 549 \n\n802 845 \n\n814 758 \n\n815 571 \n\n823 533 \n\n839 611 \n\n844 582 \n\n845 647 \n\n847 700 \n\n854 844 \n\n862 592 \n\n864 847 \n\n873 577 \n\n902 537 \n\n927 535 \n\n958 602 \n\n961 536 \n\n965 630 \n\n976 667 \n\n997 514 \n\n998 819 \n\n1000 660 \n\n1013 749 \n\n1023 548 \n\n1024 778 \n\n1026 1131 \n\n1027 626 \n\n1028 1068 \n\n1031 519 \n\n1044 955 \n\n1051 814 \n\n1053 613 \n\n1057 574 \n\n1060 561 \n\n1063 586 \n\n1072 582 \n\n1073 685 \n\n1079 528 \n\n1383\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Remove long token","metadata":{}},{"cell_type":"code","source":"print(len(listLongToken))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:14:49.928842Z","iopub.execute_input":"2023-11-28T15:14:49.929297Z","iopub.status.idle":"2023-11-28T15:14:49.934770Z","shell.execute_reply.started":"2023-11-28T15:14:49.929264Z","shell.execute_reply":"2023-11-28T15:14:49.933683Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"92\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset[\"text\"] = [dataset[\"text\"][i] for i in range(len(dataset[\"text\"])) if i not in listLongToken]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:14:52.200675Z","iopub.execute_input":"2023-11-28T15:14:52.201076Z","iopub.status.idle":"2023-11-28T15:14:52.207649Z","shell.execute_reply.started":"2023-11-28T15:14:52.201045Z","shell.execute_reply":"2023-11-28T15:14:52.206610Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Check\nmax_token_of_dataset = 0\nfor i in range(len(dataset[\"text\"])):\n    text = dataset[\"text\"][i]\n    token_len = len(tokenizer.encode(text))\n    max_token_of_dataset = max(token_len, max_token_of_dataset)\n    if token_len > MAX_TOKEN_MODEL:\n        print(i, token_len, \"\\n\")\nprint(max_token_of_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:14:53.144896Z","iopub.execute_input":"2023-11-28T15:14:53.145990Z","iopub.status.idle":"2023-11-28T15:14:54.982963Z","shell.execute_reply.started":"2023-11-28T15:14:53.145947Z","shell.execute_reply":"2023-11-28T15:14:54.981858Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"512\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Nice stuct dataset","metadata":{}},{"cell_type":"code","source":"dataset = Dataset.from_dict(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DataCollator","metadata":{}},{"cell_type":"code","source":"from trl import DataCollatorForCompletionOnlyLM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"instruction_template = DEFAULT_BOI_TOKEN\nresponse_template = DEFAULT_EOI_TOKEN\ncollator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer, mlm=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setup model for train","metadata":{}},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = prepare_model_for_kbit_training(model)\n# peft_config = LoraConfig(\n#     lora_alpha=16,\n#     lora_dropout=0.1,\n#     r=64,\n#     bias=\"none\",\n#     task_type=\"CAUSAL_LM\",\n#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n# )\n# model = get_peft_model(model, peft_config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=50,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=1000,\n    logging_steps=10,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    save_total_limit = 1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n#     lr_scheduler_type=\"constant\",\n    report_to=\"none\"\n#     report_to=\"wandb\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import bitsandbytes as bnb\n# from torch import nn\n# from transformers.trainer_pt_utils import get_parameter_names\n\n# no_decay = [\"bias\", \"LayerNorm.weight\"]\n# optimizer_grouped_parameters = [{\n#     \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n#     \"weight_decay\": training_arguments.weight_decay,\n# },\n# {\n#     \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n#     \"weight_decay\": 0.0,\n# }]\n\n\n# optimizer_kwargs = {\n#     \"betas\": (training_arguments.adam_beta1, training_arguments.adam_beta2),\n#     \"eps\": training_arguments.adam_epsilon,\n# }\n# optimizer_kwargs[\"lr\"] = training_arguments.learning_rate\n# adam_bnb_optim = bnb.optim.Adam8bit(\n#     optimizer_grouped_parameters,\n#     betas=(training_arguments.adam_beta1, training_arguments.adam_beta2),\n#     eps=training_arguments.adam_epsilon,\n#     lr=training_arguments.learning_rate,\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = model.to(\"cuda:0\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_trainable_parameters(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n#     model = base_model,\n    train_dataset=dataset,\n#     peft_config=peft_config,\n    max_seq_length=MAX_TOKEN_MODEL,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n    data_collator=collator,\n#     optimizers=(adam_bnb_optim, None)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.config.use_cache = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train and save","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.save_pretrained(new_model)\ntrainer.model.save_pretrained(new_model)\nmodel.config.to_json_file(os.path.join(new_model, \"config.json\"))\ntokenizer.save_pretrained(new_model)\nwandb.finish()\nmodel.config.use_cache = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r BK-BigAI-Math.zip BK-BigAI-Math","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'BK-BigAI-Math.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evalution","metadata":{}},{"cell_type":"markdown","source":"### Download model","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\nhf_hub_download(repo_id=\"hotamago/ZAIC-2023-Model\", filename=\"Hota-Math.zip\", repo_type=\"model\", local_dir=\"/kaggle/working/\", local_dir_use_symlinks=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip -q -o Hota-Math.zip -d ./","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load model","metadata":{}},{"cell_type":"code","source":"dataset = Dataset.from_dict(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name_or_path = \"/kaggle/working/BK-BigAI-Math\"\n# bnb_config = BitsAndBytesConfig(  \n#     load_in_4bit= True,\n#     bnb_4bit_quant_type= \"nf4\",\n#     bnb_4bit_compute_dtype= torch.bfloat16,\n#     bnb_4bit_use_double_quant= False,\n# )\n# model = AutoModelForCausalLM.from_pretrained(\n#         model_name_or_path,\n#         load_in_4bit=True,\n#         quantization_config=bnb_config,\n#         torch_dtype=torch.bfloat16,\n#         device_map=\"auto\",\n#         trust_remote_code=True,\n# )\n\n# model = AutoModelForCausalLM.from_pretrained(\n#         model_name_or_path,\n#         torch_dtype=torch.bfloat16,\n#         device_map=\"auto\",\n#         trust_remote_code=True,\n# )\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name_or_path,\n#     torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n#     load_in_4bit=True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    model_name_or_path,\n    model_max_length=512,\n    padding_side=\"right\",\n    use_fast=False,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# special_tokens_dict = {'additional_special_tokens': ['[INST]','[/INST]']}\n# num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setup pipeline with auto answer get","metadata":{}},{"cell_type":"code","source":"pipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n    temperature=0.01,\n    top_p=0.3,\n    top_k=5,\n    repetition_penalty=1.1,\n    pad_token_id=tokenizer.eos_token_id\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n# globalRegxCompire = \"0-9a-zA-Z\\.\\:\\-\\^\\! \"\ndef niceValueToCompire(x):\n#     x = re.sub(\"[^{0}]\".format(globalRegxCompire), \"\", x)\n    x = re.sub(\"[ \\t\\n]\", \"\", x)\n    return x\ndef autoLLMFormat(question, choises = None, debug=False):\n    prompt_template = ApplyPromptTemplate(question, choises, \"prompt_input_run\")\n    res = pipe(prompt_template)[0]['generated_text']\n    if debug:\n        print(res)\n    x = re.findall(\"### Answer:[\\n ](.+)\", res)\n    \n    if choises == None:\n        return x\n    \n    choises_compare = [niceValueToCompire(choise_pred) for choise_pred in choises]\n\n    if len(x) == 0:\n        return choises[random.randrange(0, len(choises))]\n    \n    x = niceValueToCompire(x[0])\n    \n    if (x not in choises_compare):\n        return choises[random.randrange(0, len(choises))]\n    \n    for i in range(len(choises_compare)):\n        if x == choises_compare[i]:\n            return choises[i]\n    \n    return \"wtf\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Run evalution","metadata":{}},{"cell_type":"code","source":"count_proc_testcase = 0\ncount_pass_testcase = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"while count_proc_testcase < len(tokenized_val_dataset_raw):\n    tvdo = tokenized_val_dataset_raw[count_proc_testcase]\n    token_len = len(tokenizer.encode(ApplyPromptTemplate(tvdo['question'], tvdo['choices'], \"prompt_input_run\")))\n    if token_len >= 512:\n        print(\"Too much token\", token_len)\n    \n    startTime()\n    answer = autoLLMFormat(tvdo['question'], tvdo['choices'], True)\n    deltaTime = getTime()\n\n    if answer == tvdo['answer']:\n        count_pass_testcase += 1\n    \n    count_proc_testcase += 1\n    print(\"Testcase {0}, time: {1}, answer: {2} | {3}, Passed: {4}, IsPass: {5}\".format(\n        count_proc_testcase,\n        deltaTime,\n        answer,\n        tvdo['answer'],\n        count_pass_testcase,\n        (answer == tvdo['answer'])\n    ))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run public test dataset","metadata":{}},{"cell_type":"code","source":"result_test = []\nif os.path.exists(os.path.join(\"result\", \"result.txt\")):\n    with open(os.path.join(\"result\", \"result.txt\"), \"r\") as f:\n        result_test = f.read().split(\"\\n\")\ncount_id = len(result_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(test_data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"while count_id < len(test_data):\n    one_test_data = test_data[count_id]\n    startTime()\n    answer = autoLLMFormat(one_test_data['question'], one_test_data['choices'], False)\n    deltaTime = getTime()\n    result_test.append(\"{0}\".format(answer))\n    count_id += 1\n    if count_id%10 == 0:\n        with open(os.path.join(\"result\", \"result.txt\"), \"w\", encoding='utf-8') as f:\n            f.write(\"\\n\".join(result_test))\n    print(\"Testcase {0}, time: {1}, answer: {2}\".format(count_id, deltaTime, answer))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(os.path.join(\"result\", \"result.txt\"), \"w\", encoding='utf-8') as f:\n    f.write(\"\\n\".join(result_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n\".join(result_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}