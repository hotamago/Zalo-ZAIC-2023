{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install","metadata":{}},{"cell_type":"code","source":"!pip3 install huggingface-hub","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade openai\n!pip install --upgrade pydantic","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U peft\n%pip install -U accelerate\n%pip install -U trl","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !mkdir MetaMath-Mistral-7B\n# !huggingface-cli download meta-math/MetaMath-Mistral-7B --local-dir MetaMath-Mistral-7B --local-dir-use-symlinks False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install sentencepiece","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# VERSION = \"1.11\"\n# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py > /dev/null\n# !python pytorch-xla-env-setup.py --version $VERSION  > /dev/null","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import library","metadata":{}},{"cell_type":"code","source":"# import tensorflow as tf\n# import tensorflow_hub as hub\n# print(\"Tensorflow version \" + tf.__version__)\n# AUTO = tf.data.experimental.AUTOTUNE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Detect TPU, return appropriate distribution strategy\n# try:\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n#     print('Running on TPU ', tpu.master())\n# except ValueError:\n#     tpu = None\n\n# if tpu:\n#     tf.config.experimental_connect_to_cluster(tpu)\n#     tf.tpu.experimental.initialize_tpu_system(tpu)\n#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# else:\n#     strategy = tf.distribute.get_strategy() \n\n# print(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nimport os,torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:39:04.555918Z","iopub.execute_input":"2023-11-28T16:39:04.556712Z","iopub.status.idle":"2023-11-28T16:39:12.296469Z","shell.execute_reply.started":"2023-11-28T16:39:04.556675Z","shell.execute_reply":"2023-11-28T16:39:12.295323Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# # PyTorch XLA-specific imports\n# import torch_xla.core.xla_model as xm\n# import torch_xla.distributed.parallel_loader as pl\n# import torch_xla.distributed.xla_multiprocessing as xmp\n# import torch_xla.debug.metrics as met","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:39:12.298255Z","iopub.execute_input":"2023-11-28T16:39:12.298890Z","iopub.status.idle":"2023-11-28T16:39:12.303202Z","shell.execute_reply.started":"2023-11-28T16:39:12.298858Z","shell.execute_reply":"2023-11-28T16:39:12.302168Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nsecret_wandb = user_secrets.get_secret(\"wandb\")","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:39:12.304690Z","iopub.execute_input":"2023-11-28T16:39:12.305107Z","iopub.status.idle":"2023-11-28T16:39:13.012157Z","shell.execute_reply.started":"2023-11-28T16:39:12.305068Z","shell.execute_reply":"2023-11-28T16:39:13.011256Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login --token $secret_hf","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:39:13.014931Z","iopub.execute_input":"2023-11-28T16:39:13.015259Z","iopub.status.idle":"2023-11-28T16:39:14.534239Z","shell.execute_reply.started":"2023-11-28T16:39:13.015232Z","shell.execute_reply":"2023-11-28T16:39:14.533068Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# wandb.login(key = secret_wandb)\n# run = wandb.init(\n# #     project='Fine tuning MetaMath mistral 7B - ZAIC',\n#     project='Fine tuning openchat 7B - ZAIC',\n#     job_type=\"training\", \n#     anonymous=\"allow\"\n# )","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:39:14.535908Z","iopub.execute_input":"2023-11-28T16:39:14.536489Z","iopub.status.idle":"2023-11-28T16:39:14.541647Z","shell.execute_reply.started":"2023-11-28T16:39:14.536436Z","shell.execute_reply":"2023-11-28T16:39:14.540570Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# base_model = \"meta-math/MetaMath-Mistral-7B\"\n# base_model = \"gpt2-xl\"\nbase_model = \"gpt2-large\"\n# base_model = \"gpt2\"\n# base_model = \"openchat/openchat_3.5\"\nnew_model = \"BK-BigAI-Math\"\nmodel_hotamath_path = \"/kaggle/working/BK-BigAI-Math\"","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:39:14.543048Z","iopub.execute_input":"2023-11-28T16:39:14.543387Z","iopub.status.idle":"2023-11-28T16:39:14.555805Z","shell.execute_reply.started":"2023-11-28T16:39:14.543359Z","shell.execute_reply":"2023-11-28T16:39:14.554950Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Download dataset","metadata":{}},{"cell_type":"code","source":"!mkdir dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\nhf_hub_download(repo_id=\"hotamago/ZAIC-2023\", filename=\"Elementary Maths Solving/test.zip\", revision=\"main\", repo_type=\"dataset\", local_dir=\"dataset\", local_dir_use_symlinks=False)\nhf_hub_download(repo_id=\"hotamago/ZAIC-2023\", filename=\"Elementary Maths Solving/train.zip\", revision=\"main\", repo_type=\"dataset\", local_dir=\"dataset\", local_dir_use_symlinks=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo apt-get install unzip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir datasetRaw\n!unzip -q -o \"dataset/Elementary Maths Solving/test.zip\" -d \"datasetRaw\"\n!unzip -q -o \"dataset/Elementary Maths Solving/train.zip\" -d \"datasetRaw\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport re\nimport time","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:39:14.557070Z","iopub.execute_input":"2023-11-28T16:39:14.557412Z","iopub.status.idle":"2023-11-28T16:39:14.567959Z","shell.execute_reply.started":"2023-11-28T16:39:14.557351Z","shell.execute_reply":"2023-11-28T16:39:14.567016Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_data = None\ntest_data = None\nwith open(os.path.join(\"datasetRaw\", \"train\", \"/kaggle/working/datasetRaw/math_train.json\"), \"r\") as f:\n    train_data = json.loads(f.read())['data']\nwith open(os.path.join(\"datasetRaw\", \"test\", \"/kaggle/working/datasetRaw/math_test.json\"), \"r\") as f:\n    test_data = json.loads(f.read())['data']","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:39:14.569327Z","iopub.execute_input":"2023-11-28T16:39:14.569649Z","iopub.status.idle":"2023-11-28T16:39:14.600454Z","shell.execute_reply.started":"2023-11-28T16:39:14.569623Z","shell.execute_reply":"2023-11-28T16:39:14.599735Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_data[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:39:14.601637Z","iopub.execute_input":"2023-11-28T16:39:14.602350Z","iopub.status.idle":"2023-11-28T16:39:14.610472Z","shell.execute_reply.started":"2023-11-28T16:39:14.602317Z","shell.execute_reply":"2023-11-28T16:39:14.609424Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'id': '1',\n 'question': 'Một người bán hàng bỏ ra 80,000 đồng tiền vốn và bị lỗ 6%. Để tính số tiền lỗ ta phải tính',\n 'choices': ['A. 80,000 : 6',\n  'B. 80,000 x 6',\n  'C. 80,000 : (6 x 100)',\n  'D. (80,000 x 6) : 100'],\n 'explanation': 'Theo đề bài, số tiền lỗ bằng 6% của 80 000 đồng . Để tìm số tiền lỗ ta có thể lấy 80 000 chia cho 100 rồi nhân với 6 (tức là 80 000 : 100 × 6) hoặc lấy 80000 nhân với 6 rồi chia cho 100 (tức là 80 000 × 6 : 100).',\n 'answer': 'D. (80,000 x 6) : 100'}"},"metadata":{}}]},{"cell_type":"code","source":"test_data[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:39:14.613207Z","iopub.execute_input":"2023-11-28T16:39:14.613548Z","iopub.status.idle":"2023-11-28T16:39:14.625741Z","shell.execute_reply.started":"2023-11-28T16:39:14.613515Z","shell.execute_reply":"2023-11-28T16:39:14.623619Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'id': '01-0203',\n 'question': 'Một cửa hàng đã bán 30% số hàng hiện có và thu được 15 000 000 đồng. Hỏi nếu bán hết hàng thì cửa hàng thu được bao nhiêu tiền?',\n 'choices': ['A. 4 500 000 đồng',\n  'B. 45 000 000 đồng',\n  'C. 50 000 000 đồng',\n  'D. 450 000 000 đồng']}"},"metadata":{}}]},{"cell_type":"code","source":"MAX_TOKEN_MODEL = 512","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:39:14.627054Z","iopub.execute_input":"2023-11-28T16:39:14.627459Z","iopub.status.idle":"2023-11-28T16:39:14.635227Z","shell.execute_reply.started":"2023-11-28T16:39:14.627415Z","shell.execute_reply":"2023-11-28T16:39:14.634280Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"DEFAULT_PAD_TOKEN = \"[PAD]\"\nDEFAULT_EOS_TOKEN = \"<|endoftext|>\" # \"<|end_of_turn|>\" # \"</s>\"\nDEFAULT_BOS_TOKEN = \"<|endoftext|>\" # \"<s>\"\nDEFAULT_UNK_TOKEN = \"<|endoftext|>\" # \"<unk>\"\nDEFAULT_BOI_TOKEN = \"<|human|>\" # \"Human:\" # \"[INST]\"\nDEFAULT_EOI_TOKEN = \"<|assistant|>\" # \"Assistant:\" # \"[/INST]\"\nPROMPT_DICT = {\n    \"prompt_input\": (\n        \"Below is an instruction that describes a task, paired with the choices, one of the choices is the correct answer to the request. \"\n        \"Write a response that appropriately completes the request.\\n\\n\"\n        \"### Instruction:\\n{instruction}\\n\\n### Choices:\\n{choices}\"\n    ),\n    \"prompt_input_run\": (\n        DEFAULT_BOI_TOKEN + \" Below is an instruction that describes a task. paired with the choices, one of the choices is the correct answer to the request. \"\n        \"Write a response that appropriately completes the request.\\n\\n\"\n        \"### Instruction:\\n{instruction}\\n\\n### Choices:\\n{choices}\"\n#         \"\\n\" + DEFAULT_EOI_TOKEN + \" \\n\\n\"\n#         \"### Explanation:\\n Let's think step by step.\\n\"\n#         \"### Explanation:\\n Hãy suy nghĩ từng bước một.\\n\"\n    ),\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-28T19:35:13.932530Z","iopub.execute_input":"2023-11-28T19:35:13.933468Z","iopub.status.idle":"2023-11-28T19:35:13.939044Z","shell.execute_reply.started":"2023-11-28T19:35:13.933433Z","shell.execute_reply":"2023-11-28T19:35:13.938148Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"timeGlobal = 0\ndef startTime():\n    global timeGlobal\n    timeGlobal = time.time()\ndef getTime():\n    return (time.time() - timeGlobal)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:39:14.650065Z","iopub.execute_input":"2023-11-28T16:39:14.650344Z","iopub.status.idle":"2023-11-28T16:39:14.660408Z","shell.execute_reply.started":"2023-11-28T16:39:14.650320Z","shell.execute_reply":"2023-11-28T16:39:14.659361Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def ApplyPromptTemplate(instruction, choices, typeP = \"prompt_input\"):\n    return PROMPT_DICT[typeP].format(instruction = instruction, choices = \"\\n\".join(choices))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T19:35:16.899739Z","iopub.execute_input":"2023-11-28T19:35:16.900126Z","iopub.status.idle":"2023-11-28T19:35:16.905130Z","shell.execute_reply.started":"2023-11-28T19:35:16.900094Z","shell.execute_reply":"2023-11-28T19:35:16.904277Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"### Add explantion by GPT 3.5 Tubo","metadata":{}},{"cell_type":"code","source":"from openai import OpenAI\nclient = OpenAI(\n    api_key=user_secrets.get_secret(\"OPENAI_API_KEY\"),\n#     organization='org-j48waUrvSOM1n0J1SLXiAr8n',\n)\n\ndef autoGPTAddExplantion(problem, answer):\n    response = client.chat.completions.create(\n      model=\"gpt-3.5-turbo\",\n      messages=[\n        {\"role\": \"system\", \"content\": \"Explan by Vietnamese step-by-step for given answer to given problem.\\nRule:\\n- No markdown format\\n- Given answer always true for given problem\\n- No title\\n- Short explantion\\n\"},\n        {\"role\": \"user\", \"content\": f\"### Problem:\\n{problem}\\n\\n### Answer:\\n{answer}\"},\n        {\"role\": \"system\", \"content\": \"Giải thích: \"},\n      ],\n#       max_tokens=512,\n      temperature=0,\n      top_p=1.0,\n#       top_k=50,\n    )\n    return response.choices[0].message.content","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"startTime()\ncntSt = 0\ni = 0\ntimeWait = 20\nwhile i < len(train_data):\n    singleData = train_data[i]\n    if \"explanation\" in singleData.keys():\n        i += 1\n        continue\n    print(f\"Runing testcase ({i})\")\n    \n    try:\n        res = autoGPTAddExplantion(singleData['question'], singleData['answer'])\n    except Exception as inst:\n        print(type(inst))\n        print(inst)\n        \n        time.sleep(timeWait)\n        timeWait *= 2\n        if timeWait > 80:\n            break\n        print\n        continue\n    \n    timeWait = 20\n        \n    cntSt += 1\n    train_data[i]['explanation'] = res\n    print(f\"Done testcase ({i})\")\n    #     print(train_data[i])\n    if cntSt%3 == 0:\n        deltaTime = getTime()\n        if (60 - deltaTime) > -1:\n            time.sleep(60 - deltaTime + 2)\n        startTime()\n        cntSt = 0\n    \n    i += 1\nprint(\"Done all testcase\")","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/working/datasetRaw/math_train.json\", \"w\", encoding='utf-8') as f:\n    json.dump({\n        \"__count__\": len(train_data),\n        \"data\": train_data\n    }, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add data by education program","metadata":{}},{"cell_type":"markdown","source":"Todo: Create data generation algorithms for each chapter.\n\nBelow is class base, change \\_call function to a function that create radom question for that chapter","metadata":{}},{"cell_type":"code","source":"class BaseGenChapter:\n    def __init__(*args, **kwargs):\n        pass\n    \n    def _call(self, *args, **kwargs) -> str:\n        raise NotImplementedError(\"_call not Implement yet\")\n    \n    def call(self, *args, **kwargs) -> str:\n        return self._call(*args, **kwargs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocess data","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\nimport random","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:39:19.742547Z","iopub.execute_input":"2023-11-28T16:39:19.743321Z","iopub.status.idle":"2023-11-28T16:39:19.747930Z","shell.execute_reply.started":"2023-11-28T16:39:19.743283Z","shell.execute_reply":"2023-11-28T16:39:19.746439Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# random.shuffle(train_data)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:39:20.258699Z","iopub.execute_input":"2023-11-28T16:39:20.259649Z","iopub.status.idle":"2023-11-28T16:39:20.264254Z","shell.execute_reply.started":"2023-11-28T16:39:20.259612Z","shell.execute_reply":"2023-11-28T16:39:20.263067Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"num_train_dataset = len(train_data)\nvalition_radio = 0.1\ntokenized_train_dataset_raw = train_data[:-int(num_train_dataset*valition_radio)]\ntokenized_val_dataset_raw = train_data[-int(num_train_dataset*valition_radio):]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:39:20.450682Z","iopub.execute_input":"2023-11-28T16:39:20.451094Z","iopub.status.idle":"2023-11-28T16:39:20.456409Z","shell.execute_reply.started":"2023-11-28T16:39:20.451061Z","shell.execute_reply":"2023-11-28T16:39:20.455467Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"datasetStruct = {\"input\":[], \"output\":[]}\ndataset = {\"text\":[]}\nnum_train_dataset = len(tokenized_train_dataset_raw)\nfor i in range(num_train_dataset):\n    ttdro = tokenized_train_dataset_raw[i]\n    \n    if \"explanation\" not in ttdro.keys():\n        continue\n    \n    input_content = \"{0} {1}\".format(\n        DEFAULT_BOI_TOKEN,\n        ApplyPromptTemplate(ttdro['question'], ttdro['choices']),\n    )\n    datasetStruct[\"input\"].append(input_content)\n    \n    if \"explanation\" not in ttdro.keys():\n        output_content = \"\\n{0} \\n\\n{1}\\n\\n{2} {3}\".format(\n            DEFAULT_EOI_TOKEN,\n            \"### Explanation:\\nNo explanation\",\n            \"### Answer:\\n{0}\".format(ttdro['answer']),\n            DEFAULT_EOS_TOKEN,\n        )\n    else:\n        output_content = \"\\n{0} \\n\\n{1}\\n\\n{2} {3}\".format(\n            DEFAULT_EOI_TOKEN,\n            \"### Explanation:\\n{0}\".format(ttdro['explanation']),\n            \"### Answer:\\n{0}\".format(ttdro['answer']),\n            DEFAULT_EOS_TOKEN,\n        )\n    datasetStruct[\"output\"].append(output_content)\n    \n    dataset[\"text\"].append(input_content + output_content)\n    \n    # No explantion\n#     output_content = \"\\n{0} \\n\\n{1}\\n\\n{2} {3}\".format(\n#             DEFAULT_EOI_TOKEN,\n#             \"### Explanation:\\nNo explanation\",\n#             \"### Answer:\\n{0}\".format(ttdro['answer']),\n#             DEFAULT_EOS_TOKEN,\n#         )\n#     datasetStruct[\"output\"].append(output_content)\n    \n#     dataset[\"text\"].append(input_content + output_content)\n#     <s>[INST][/INST] </s>","metadata":{"execution":{"iopub.status.busy":"2023-11-28T19:36:53.181747Z","iopub.execute_input":"2023-11-28T19:36:53.182540Z","iopub.status.idle":"2023-11-28T19:36:53.200679Z","shell.execute_reply.started":"2023-11-28T19:36:53.182506Z","shell.execute_reply":"2023-11-28T19:36:53.199782Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"print(dataset[\"text\"][33])","metadata":{"execution":{"iopub.status.busy":"2023-11-28T19:36:53.381013Z","iopub.execute_input":"2023-11-28T19:36:53.381763Z","iopub.status.idle":"2023-11-28T19:36:53.386888Z","shell.execute_reply.started":"2023-11-28T19:36:53.381726Z","shell.execute_reply":"2023-11-28T19:36:53.385853Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"<|human|> Below is an instruction that describes a task, paired with the choices, one of the choices is the correct answer to the request. Write a response that appropriately completes the request.\n\n### Instruction:\nNhà Hiền có 7 con lợn. Số ga nhiều hơn số lợn 63 con. Vậy số gà gấp số lợn là:\n\n### Choices:\nA. 7 lần\nB. 8 lần\nC. 9 lần\nD. 10 lần\n<|assistant|> \n\n### Explanation:\nNhà Hiền có số con gà là: 63 + 7 = 70 (con)\n Số gà gấp số lợn là: 70 : 7 = 10 (lần)\n\n### Answer:\nD. 10 lần <|endoftext|>\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(dataset[\"text\"]))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T19:36:53.553500Z","iopub.execute_input":"2023-11-28T19:36:53.554043Z","iopub.status.idle":"2023-11-28T19:36:53.561216Z","shell.execute_reply.started":"2023-11-28T19:36:53.553999Z","shell.execute_reply":"2023-11-28T19:36:53.559937Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"1080\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Train model","metadata":{}},{"cell_type":"markdown","source":"### Load model","metadata":{}},{"cell_type":"code","source":"# bnb_config = BitsAndBytesConfig(  \n#     load_in_4bit= True,\n#     bnb_4bit_quant_type= \"nf4\",\n#     bnb_4bit_compute_dtype= torch.bfloat16,\n#     bnb_4bit_use_double_quant= False,\n# )\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n#         model_hotamath_path,\n#     load_in_4bit=True,\n#     load_in_8bit= True,\n#     quantization_config=bnb_config,\n#     torch_dtype=torch.bfloat16,\n#     torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-28T16:39:29.399439Z","iopub.execute_input":"2023-11-28T16:39:29.400504Z","iopub.status.idle":"2023-11-28T16:39:48.481061Z","shell.execute_reply.started":"2023-11-28T16:39:29.400467Z","shell.execute_reply":"2023-11-28T16:39:48.480205Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4043477129c42f1b62e10b4a24bd070"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af46d1a65f38454daf9264cc099d5bbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6774a1130d564502a0cf488cbc028783"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    base_model,\n    model_max_length=MAX_TOKEN_MODEL,\n    padding_side=\"right\",\n    use_fast=False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:39:48.482721Z","iopub.execute_input":"2023-11-28T16:39:48.483045Z","iopub.status.idle":"2023-11-28T16:39:49.449313Z","shell.execute_reply.started":"2023-11-28T16:39:48.483019Z","shell.execute_reply":"2023-11-28T16:39:49.448154Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baf0ef49a1bd4d48b58c3bf295f013fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc606c8acc9a4720ba3e9cf8f8e92dd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ac1b6794f1f48d085ca481675038357"}},"metadata":{}}]},{"cell_type":"code","source":"special_tokens_dict = {\n    'additional_special_tokens': [DEFAULT_BOI_TOKEN, DEFAULT_EOI_TOKEN],\n    'pad_token': DEFAULT_PAD_TOKEN,\n    'bos_token': DEFAULT_BOS_TOKEN,\n    'eos_token': DEFAULT_EOS_TOKEN,\n    'unk_token': DEFAULT_UNK_TOKEN,\n}\nnum_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:39:49.450843Z","iopub.execute_input":"2023-11-28T16:39:49.451698Z","iopub.status.idle":"2023-11-28T16:39:55.747813Z","shell.execute_reply.started":"2023-11-28T16:39:49.451660Z","shell.execute_reply":"2023-11-28T16:39:55.746674Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"Embedding(50260, 1280)"},"metadata":{}}]},{"cell_type":"code","source":"print(tokenizer.encode(\"{0} Hello, how are you? \\n{1} I'm fine, thank you!{2}\".format(\n    DEFAULT_BOI_TOKEN,\n    DEFAULT_EOI_TOKEN,\n    DEFAULT_EOS_TOKEN,\n)))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:39:55.750462Z","iopub.execute_input":"2023-11-28T16:39:55.751223Z","iopub.status.idle":"2023-11-28T16:39:55.840141Z","shell.execute_reply.started":"2023-11-28T16:39:55.751195Z","shell.execute_reply":"2023-11-28T16:39:55.839033Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"[50258, 18435, 11, 703, 389, 345, 30, 220, 198, 50257, 314, 1101, 3734, 11, 5875, 345, 0, 50256]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Check max token","metadata":{}},{"cell_type":"code","source":"max_token_of_dataset = 0\nlistLongToken = []\nfor i in range(len(dataset[\"text\"])):\n    text = dataset[\"text\"][i]\n    token_len = len(tokenizer.encode(text))\n    max_token_of_dataset = max(token_len, max_token_of_dataset)\n    if token_len > MAX_TOKEN_MODEL:\n        print(i, token_len, \"\\n\")\n        listLongToken.append(i)\n#         print(text)\nprint(max_token_of_dataset)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-28T19:37:18.046324Z","iopub.execute_input":"2023-11-28T19:37:18.047164Z","iopub.status.idle":"2023-11-28T19:37:20.157374Z","shell.execute_reply.started":"2023-11-28T19:37:18.047121Z","shell.execute_reply":"2023-11-28T19:37:20.156314Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"71 673 \n\n120 594 \n\n239 637 \n\n329 558 \n\n353 566 \n\n361 779 \n\n367 590 \n\n380 592 \n\n388 584 \n\n396 574 \n\n397 796 \n\n399 645 \n\n400 793 \n\n435 671 \n\n440 590 \n\n454 606 \n\n455 531 \n\n458 716 \n\n463 555 \n\n477 542 \n\n483 632 \n\n484 750 \n\n486 667 \n\n487 808 \n\n491 629 \n\n494 769 \n\n529 565 \n\n536 897 \n\n537 563 \n\n539 841 \n\n540 868 \n\n543 527 \n\n546 694 \n\n563 518 \n\n570 828 \n\n578 514 \n\n589 568 \n\n591 903 \n\n592 635 \n\n603 709 \n\n604 664 \n\n614 525 \n\n617 616 \n\n621 515 \n\n622 726 \n\n660 1383 \n\n662 542 \n\n724 729 \n\n742 696 \n\n754 943 \n\n761 1032 \n\n764 680 \n\n766 908 \n\n784 531 \n\n799 549 \n\n802 845 \n\n814 758 \n\n815 571 \n\n823 533 \n\n839 611 \n\n844 582 \n\n845 647 \n\n847 700 \n\n854 844 \n\n862 592 \n\n864 847 \n\n873 577 \n\n902 537 \n\n927 535 \n\n958 602 \n\n961 536 \n\n965 630 \n\n976 667 \n\n997 514 \n\n998 819 \n\n1000 660 \n\n1013 749 \n\n1023 548 \n\n1024 778 \n\n1026 1131 \n\n1027 626 \n\n1028 1068 \n\n1031 519 \n\n1044 955 \n\n1051 814 \n\n1053 613 \n\n1057 574 \n\n1060 561 \n\n1063 586 \n\n1072 582 \n\n1073 685 \n\n1079 528 \n\n1383\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Remove long token","metadata":{}},{"cell_type":"code","source":"print(len(listLongToken))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T19:37:20.158884Z","iopub.execute_input":"2023-11-28T19:37:20.159229Z","iopub.status.idle":"2023-11-28T19:37:20.163839Z","shell.execute_reply.started":"2023-11-28T19:37:20.159202Z","shell.execute_reply":"2023-11-28T19:37:20.162992Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"92\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset[\"text\"] = [dataset[\"text\"][i] for i in range(len(dataset[\"text\"])) if i not in listLongToken]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T19:37:20.165021Z","iopub.execute_input":"2023-11-28T19:37:20.165294Z","iopub.status.idle":"2023-11-28T19:37:20.177409Z","shell.execute_reply.started":"2023-11-28T19:37:20.165271Z","shell.execute_reply":"2023-11-28T19:37:20.176446Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"# Check\nmax_token_of_dataset = 0\nfor i in range(len(dataset[\"text\"])):\n    text = dataset[\"text\"][i]\n    token_len = len(tokenizer.encode(text))\n    max_token_of_dataset = max(token_len, max_token_of_dataset)\n    if token_len > MAX_TOKEN_MODEL:\n        print(i, token_len, \"\\n\")\nprint(max_token_of_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T19:37:20.992806Z","iopub.execute_input":"2023-11-28T19:37:20.993871Z","iopub.status.idle":"2023-11-28T19:37:22.720673Z","shell.execute_reply.started":"2023-11-28T19:37:20.993827Z","shell.execute_reply":"2023-11-28T19:37:22.719737Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"512\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Nice stuct dataset","metadata":{}},{"cell_type":"code","source":"dataset = Dataset.from_dict(dataset)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T19:37:22.722460Z","iopub.execute_input":"2023-11-28T19:37:22.722760Z","iopub.status.idle":"2023-11-28T19:37:22.733824Z","shell.execute_reply.started":"2023-11-28T19:37:22.722734Z","shell.execute_reply":"2023-11-28T19:37:22.732845Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"markdown","source":"### DataCollator","metadata":{}},{"cell_type":"code","source":"from trl import DataCollatorForCompletionOnlyLM","metadata":{"execution":{"iopub.status.busy":"2023-11-28T19:37:24.535361Z","iopub.execute_input":"2023-11-28T19:37:24.536106Z","iopub.status.idle":"2023-11-28T19:37:24.540380Z","shell.execute_reply.started":"2023-11-28T19:37:24.536070Z","shell.execute_reply":"2023-11-28T19:37:24.539356Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"instruction_template = DEFAULT_BOI_TOKEN\nresponse_template = DEFAULT_EOI_TOKEN\ncollator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer, mlm=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T19:37:24.687541Z","iopub.execute_input":"2023-11-28T19:37:24.688149Z","iopub.status.idle":"2023-11-28T19:37:24.693342Z","shell.execute_reply.started":"2023-11-28T19:37:24.688118Z","shell.execute_reply":"2023-11-28T19:37:24.692174Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"markdown","source":"### Setup model for train","metadata":{}},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:40:02.505864Z","iopub.execute_input":"2023-11-28T16:40:02.506737Z","iopub.status.idle":"2023-11-28T16:40:02.514079Z","shell.execute_reply.started":"2023-11-28T16:40:02.506699Z","shell.execute_reply":"2023-11-28T16:40:02.512971Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50260, 1280)\n    (wpe): Embedding(1024, 1280)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-35): 36 x GPT2Block(\n        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1280, out_features=50260, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"# model = prepare_model_for_kbit_training(model)\n# peft_config = LoraConfig(\n#     lora_alpha=16,\n#     lora_dropout=0.1,\n#     r=64,\n#     bias=\"none\",\n#     task_type=\"CAUSAL_LM\",\n#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n# )\n# model = get_peft_model(model, peft_config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=25,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=1000,\n    logging_steps=10,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    save_total_limit = 1,\n    warmup_ratio=0.03,\n#     group_by_length=True,\n#     lr_scheduler_type=\"constant\",\n    report_to=\"none\"\n#     report_to=\"wandb\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:40:33.826509Z","iopub.execute_input":"2023-11-28T16:40:33.826847Z","iopub.status.idle":"2023-11-28T16:40:33.835430Z","shell.execute_reply.started":"2023-11-28T16:40:33.826822Z","shell.execute_reply":"2023-11-28T16:40:33.834418Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# import bitsandbytes as bnb\n# from torch import nn\n# from transformers.trainer_pt_utils import get_parameter_names\n\n# no_decay = [\"bias\", \"LayerNorm.weight\"]\n# optimizer_grouped_parameters = [{\n#     \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n#     \"weight_decay\": training_arguments.weight_decay,\n# },\n# {\n#     \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n#     \"weight_decay\": 0.0,\n# }]\n\n\n# optimizer_kwargs = {\n#     \"betas\": (training_arguments.adam_beta1, training_arguments.adam_beta2),\n#     \"eps\": training_arguments.adam_epsilon,\n# }\n# optimizer_kwargs[\"lr\"] = training_arguments.learning_rate\n# adam_bnb_optim = bnb.optim.Adam8bit(\n#     optimizer_grouped_parameters,\n#     betas=(training_arguments.adam_beta1, training_arguments.adam_beta2),\n#     eps=training_arguments.adam_epsilon,\n#     lr=training_arguments.learning_rate,\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = model.to(\"cuda:0\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:40:37.022358Z","iopub.execute_input":"2023-11-28T16:40:37.022939Z","iopub.status.idle":"2023-11-28T16:40:37.030779Z","shell.execute_reply.started":"2023-11-28T16:40:37.022895Z","shell.execute_reply":"2023-11-28T16:40:37.029585Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"print_trainable_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:40:37.173574Z","iopub.execute_input":"2023-11-28T16:40:37.174272Z","iopub.status.idle":"2023-11-28T16:40:37.181341Z","shell.execute_reply.started":"2023-11-28T16:40:37.174240Z","shell.execute_reply":"2023-11-28T16:40:37.180392Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"trainable params: 774033920 || all params: 774033920 || trainable%: 100.0\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n#     model = base_model,\n    train_dataset=dataset,\n#     peft_config=peft_config,\n    max_seq_length=MAX_TOKEN_MODEL,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n    data_collator=collator,\n#     optimizers=(adam_bnb_optim, None)\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T19:37:29.426166Z","iopub.execute_input":"2023-11-28T19:37:29.426848Z","iopub.status.idle":"2023-11-28T19:37:31.264045Z","shell.execute_reply.started":"2023-11-28T19:37:29.426815Z","shell.execute_reply":"2023-11-28T19:37:31.262754Z"},"trusted":true},"execution_count":89,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e20fa3c3a2f40e69ce985832d3d9e72"}},"metadata":{}}]},{"cell_type":"code","source":"model.config.use_cache = False","metadata":{"execution":{"iopub.status.busy":"2023-11-28T19:37:31.266878Z","iopub.execute_input":"2023-11-28T19:37:31.267909Z","iopub.status.idle":"2023-11-28T19:37:31.273294Z","shell.execute_reply.started":"2023-11-28T19:37:31.267857Z","shell.execute_reply":"2023-11-28T19:37:31.272034Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":"### Train and save","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-28T20:28:46.327554Z","iopub.execute_input":"2023-11-28T20:28:46.328218Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1001' max='6175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1001/6175 51:05 < 4:24:37, 0.33 it/s, Epoch 4.05/25]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.042400</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.043700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.039000</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.030200</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.017100</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.059200</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.051200</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.023900</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.035000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.045700</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.024500</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.046400</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.027300</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.036100</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.032400</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.034000</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.044800</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.050000</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.054200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.040400</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.042200</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.050400</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.053300</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.067800</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.062200</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.043700</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.049900</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.067500</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.052300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.056400</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.058000</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.069700</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.047200</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.063100</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.058200</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.050200</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.058500</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.065500</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.040200</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.046000</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.065300</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.078400</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.060000</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.041600</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.059100</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.056000</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.052100</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.057600</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.052500</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.053000</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.042900</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.062900</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.039500</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.049700</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.071700</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.059500</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.068600</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.046600</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.055100</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.048100</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.050000</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.071800</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.059400</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.056300</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.058500</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.043400</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.050900</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.047600</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.039000</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.070300</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.046500</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.065300</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.049100</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.055200</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.049900</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.063500</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.048300</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.058400</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.054500</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.049400</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.036500</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.044500</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.064500</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.041100</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.054100</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.055200</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.051200</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.055400</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.048900</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.041700</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.057600</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.043400</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.049400</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.041300</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.051100</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.047900</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.039100</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.042900</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.035700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"# model.save_pretrained(new_model)\ntrainer.model.save_pretrained(new_model)\nmodel.config.to_json_file(os.path.join(new_model, \"config.json\"))\ntokenizer.sacd ve_pretrained(new_model)\nwandb.finish()\nmodel.config.use_cache = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r BK-BigAI-Math.zip BK-BigAI-Math","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'BK-BigAI-Math.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evalution","metadata":{}},{"cell_type":"markdown","source":"### Download model","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\nhf_hub_download(repo_id=\"hotamago/ZAIC-2023-Model\", filename=\"Hota-Math.zip\", repo_type=\"model\", local_dir=\"/kaggle/working/\", local_dir_use_symlinks=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip -q -o Hota-Math.zip -d ./","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load model","metadata":{}},{"cell_type":"code","source":"dataset = Dataset.from_dict(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name_or_path = \"/kaggle/working/BK-BigAI-Math\"\n# bnb_config = BitsAndBytesConfig(  \n#     load_in_4bit= True,\n#     bnb_4bit_quant_type= \"nf4\",\n#     bnb_4bit_compute_dtype= torch.bfloat16,\n#     bnb_4bit_use_double_quant= False,\n# )\n# model = AutoModelForCausalLM.from_pretrained(\n#         model_name_or_path,\n#         load_in_4bit=True,\n#         quantization_config=bnb_config,\n#         torch_dtype=torch.bfloat16,\n#         device_map=\"auto\",\n#         trust_remote_code=True,\n# )\n\n# model = AutoModelForCausalLM.from_pretrained(\n#         model_name_or_path,\n#         torch_dtype=torch.bfloat16,\n#         device_map=\"auto\",\n#         trust_remote_code=True,\n# )\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name_or_path,\n#     torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n#     load_in_4bit=True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    model_name_or_path,\n    model_max_length=512,\n    padding_side=\"right\",\n    use_fast=False,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# special_tokens_dict = {'additional_special_tokens': ['[INST]','[/INST]']}\n# num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setup pipeline with auto answer get","metadata":{}},{"cell_type":"code","source":"pipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n    temperature=0.01,\n    top_p=0.2,\n    top_k=5,\n    repetition_penalty=1.1,\n    pad_token_id=tokenizer.eos_token_id\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:27:05.907791Z","iopub.execute_input":"2023-11-28T20:27:05.908242Z","iopub.status.idle":"2023-11-28T20:27:05.916406Z","shell.execute_reply.started":"2023-11-28T20:27:05.908200Z","shell.execute_reply":"2023-11-28T20:27:05.915141Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"import random\n# globalRegxCompire = \"0-9a-zA-Z\\.\\:\\-\\^\\! \"\ndef niceValueToCompire(x):\n#     x = re.sub(\"[^{0}]\".format(globalRegxCompire), \"\", x)\n    x = re.sub(\"[ \\t\\n]\", \"\", x)\n    return x\ndef autoLLMFormat(question, choises = None, debug=False):\n    prompt_template = ApplyPromptTemplate(question, choises, \"prompt_input_run\")\n    res = pipe(prompt_template)[0]['generated_text']\n    if debug:\n        print(res)\n    x = re.findall(\"### Answer:[\\n ](.+)\", res)\n    \n    if choises == None:\n        return x\n    \n    choises_compare = [niceValueToCompire(choise_pred) for choise_pred in choises]\n\n    if len(x) == 0:\n        return choises[random.randrange(0, len(choises))]\n    \n    x = niceValueToCompire(x[0])\n    \n    if (x not in choises_compare):\n        return choises[random.randrange(0, len(choises))]\n    \n    for i in range(len(choises_compare)):\n        if x == choises_compare[i]:\n            return choises[i]\n    \n    return \"wtf\"","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:27:06.102356Z","iopub.execute_input":"2023-11-28T20:27:06.102923Z","iopub.status.idle":"2023-11-28T20:27:06.111642Z","shell.execute_reply.started":"2023-11-28T20:27:06.102896Z","shell.execute_reply":"2023-11-28T20:27:06.110730Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":"### Run evalution","metadata":{}},{"cell_type":"code","source":"count_proc_testcase = 0\ncount_pass_testcase = 0","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:27:07.142926Z","iopub.execute_input":"2023-11-28T20:27:07.143846Z","iopub.status.idle":"2023-11-28T20:27:07.147743Z","shell.execute_reply.started":"2023-11-28T20:27:07.143811Z","shell.execute_reply":"2023-11-28T20:27:07.146901Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"while count_proc_testcase < len(tokenized_val_dataset_raw):\n    tvdo = tokenized_val_dataset_raw[count_proc_testcase]\n    token_len = len(tokenizer.encode(ApplyPromptTemplate(tvdo['question'], tvdo['choices'], \"prompt_input_run\")))\n    if token_len >= 512:\n        print(\"Too much token\", token_len)\n    \n    startTime()\n    answer = autoLLMFormat(tvdo['question'], tvdo['choices'], True)\n    deltaTime = getTime()\n\n    if answer == tvdo['answer']:\n        count_pass_testcase += 1\n    \n    count_proc_testcase += 1\n    print(\"Testcase {0}, time: {1}, answer: {2} | {3}, Passed: {4}, IsPass: {5}\".format(\n        count_proc_testcase,\n        deltaTime,\n        answer,\n        tvdo['answer'],\n        count_pass_testcase,\n        (answer == tvdo['answer'])\n    ))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-28T20:27:07.322493Z","iopub.execute_input":"2023-11-28T20:27:07.323027Z","iopub.status.idle":"2023-11-28T20:28:36.970013Z","shell.execute_reply.started":"2023-11-28T20:27:07.322999Z","shell.execute_reply":"2023-11-28T20:28:36.968047Z"},"trusted":true},"execution_count":97,"outputs":[{"name":"stdout","text":"<|human|> Below is an instruction that describes a task. paired with the choices, one of the choices is the correct answer to the request. Write a response that appropriately completes the request.\n\n### Instruction:\nMột cái bánh pizza có giá 64 nghìn đồng. \\frac{1}{8} cái bánh có giá bao nhiêu nghìn đồng?\n\n### Choices:\nA. 7 nghìn đồng\nB. 9 nghìn đồng \nC. 8 nghìn đồng\nD. 10 nghìn đồng\n\n### Explanation:\nMột cái bánh có giá 64 nghìn đồng. \\frac{1}{8} cái bánh có giá là 8 nghìn đồng.\n\n### Answer:\nB. 9 nghìn đồng \nTestcase 1, time: 4.511938571929932, answer: B. 9 nghìn đồng  | C. 8 nghìn đồng, Passed: 0, IsPass: False\n<|human|> Below is an instruction that describes a task. paired with the choices, one of the choices is the correct answer to the request. Write a response that appropriately completes the request.\n\n### Instruction:\nMột chiếc ca nô xuôi dòng từ A đến B với vận tốc 25 km/giờ. Nếu vận tốc dòng nước là 2,5 km/giờ thì vận tốc của ca nô khi ngược dòng là:\n\n### Choices:\nA. 22,5 km/giờ\nB. 22 km/giờ\nC. 20 km/giờ\nD. 27,5 km/giờ\n\n### Explanation:\nNgườa: 25 = 2,5 (km/giờ)\n Nếu vận tốc dòng là: 2,5 ${\\times}$ 2,5 = 22,5 (km/giờ)\n\n### Answer:\nB. 22 km/giờ \nTestcase 2, time: 4.707080125808716, answer: B. 22 km/giờ | C. 20 km/giờ, Passed: 0, IsPass: False\n<|human|> Below is an instruction that describes a task. paired with the choices, one of the choices is the correct answer to the request. Write a response that appropriately completes the request.\n\n### Instruction:\nMột chiếc máy khâu được phát minh vào năm 1890. Hỏi chiếc máy khâu đó được phát minh vào thế kỉ nào?\n\n### Choices:\nA. Thế kỉ XVIII\nB.Thế kì XX\nC. Thế kỉ XIX\nD.Thế kỉ XXIên ta cần tìm số kiền trong mỗi phần tử trong mỗi phần tử trong mỗi phần tử.\n\nTrong trường hợp này, số kiền trong mỗi phần tử là 6 và XYZ là một phần tử.\n\nTuy nhiên, trong các lựa chộn tử cho, chữ số hợp khất vào năm.\n\nVì vậy, chữ số hợp khất vào năm là thế kỉ vào thế kỉ nào.\n\n### Answer:\nB. Thế kì XX \nTestcase 3, time: 13.234682559967041, answer: B.Thế kì XX | C. Thế kỉ XIX, Passed: 0, IsPass: False\n<|human|> Below is an instruction that describes a task. paired with the choices, one of the choices is the correct answer to the request. Write a response that appropriately completes the request.\n\n### Instruction:\nMột chiếc tàu ngược dòng với vận tốc 15km/giờ. Biết vận tốc của dòng nước là 3km/ giờ. Tính vận tốc xuôi dòng của chiếc tàu?\n\n### Choices:\nA. 21km/giờ\nB. 20km/giờ\nC. 18km/giờ\nD. 12km/giờ\n\n### Explanation:\nĐể tính vờn tốc xuôi dòng của chiếc tàu, ta cộng trung bình của chiếc (tức là 3) với số (tức). \n\nTrung bình của chiếc là: $15 + 3 = 22$\n\nVậy tổng của vận tốc là 22cm / 3 = 1.33cm.\n\nTuy nhiên, trong các lựa chọn phân số. \n\nVì vận tốc là 3,14, ta có:\n\nDiện tích: 3 > 3 < 3 < 4 < 5 < 6.\n\nVậy tổng của dòng là 21cm/giờ.\n\n### Answer:\nA. 21km/giờ \nTestcase 4, time: 14.326921463012695, answer: A. 21km/giờ | A. 21km/giờ, Passed: 1, IsPass: True\n<|human|> Below is an instruction that describes a task. paired with the choices, one of the choices is the correct answer to the request. Write a response that appropriately completes the request.\n\n### Instruction:\nMột chiếc tàu xuôi dòng từ A đến B với vận tốc 12km/giờ, sau đó lại ngược dòng từ B về A với vận tốc 8km/giờ. Tính thời gian chiều tàu xuôi dòng từ A đến B rồi từ B về A biết độ dài AB là 24km.\n\n### Choices:\nA. 5 giờ\nB. 4 giờ\nC. 3 giờ\nD. 6 giờ\n\nVậy tổng của chiều tàu là 24 + 8 = 48$ (giờ)\n Ta có:\n 8 / 24 = 6 giờ\n\nĐể tính trong mỗi giờ và câu trả lời, ta cần xét chữ số giờ trong mỗi giờ cho số giờ.\n\nTrong trường hợp này, chiều tàu là 24cm.\n\nVì vậy, đáp án D là đúng.\n\n### Answer:\nD. 6 giờ \nTestcase 5, time: 10.234233617782593, answer: D. 6 giờ | A. 5 giờ, Passed: 1, IsPass: False\n<|human|> Below is an instruction that describes a task. paired with the choices, one of the choices is the correct answer to the request. Write a response that appropriately completes the request.\n\n### Instruction:\n“m3 = ……..m3”. Số thập phân thích hợp điền vào chỗ chấm là:\n\n### Choices:\nA. 1,4\nB. 0,25\nC. 2,5\nD. 0,14.\n\n### Explanation:\nM3 = 1,4m3.\n\n### Answer:\nA. 1,4 \nTestcase 6, time: 1.5290021896362305, answer: A. 1,4 | B. 0,25, Passed: 1, IsPass: False\n<|human|> Below is an instruction that describes a task. paired with the choices, one of the choices is the correct answer to the request. Write a response that appropriately completes the request.\n\n### Instruction:\nMột chú chuột túi khi mới sinh ra chỉ nặng 1 g nhưng khi trưởng thành nặng đến 90 kg. Vậy chuột túi trưởng thành nặng gấp chuột túi lúc mới sinh số lần là:\n\n### Choices:\nA. 90 lần\nB. 900 lần\nC. 9 000 lần\nD. 90 000 lần\n\n### Explanation:\nChuột túi trưởng thành nặng lúc mớ linh số lần là: 90 : 1 = 90 (lần)\n Đáp số: 90 lần\n\n### Answer:\nA. 90 lần \nTestcase 7, time: 5.042543172836304, answer: A. 90 lần | D. 90 000 lần, Passed: 1, IsPass: False\n<|human|> Below is an instruction that describes a task. paired with the choices, one of the choices is the correct answer to the request. Write a response that appropriately completes the request.\n\n### Instruction:\nMột cốc trà sữa cỡ lớn có giá là 20 000 đồng, một cốc trà sữa cỡ nhỏ có giá là 14 500 đồng. An mua 1 cốc cỡ to và 2 cốc cỡ nhỏ. Vậy số tiền An cần trả là:\n\n### Choices:\nA. 34 500 đồng\nB. 49 000 đồng\nC. 48 000 đồng\nD. 35 000 đồng \n Số tiền An cần trả là:20 : 14 = 5 (đồng)\n Số tiền An cần trả là:20 : 5 = 3 (vớ̂y)\n Số tiền An cần trả là:20 : 3 = 8 (cồng)\n\n### Explanation:\nSố tiền An cần trả là:20 : 8 = 3 (vộy)\n Số tiền An cần trả là:20 : 5 = 5 (vộy)\n\n### Answer:\nD. 35 000 đồng \nTestcase 8, time: 10.115795612335205, answer: D. 35 000 đồng | B. 49 000 đồng, Passed: 1, IsPass: False\n<|human|> Below is an instruction that describes a task. paired with the choices, one of the choices is the correct answer to the request. Write a response that appropriately completes the request.\n\n### Instruction:\nMột con thỏ có thể chạy với vận tốc 5,5m/giây. Một con ngựa có thể chạy với vận tốc 25,2km/giờ. Hỏi trong 1 phút, con nào di chuyển được quãng đường dài hơn và dài hơn bao nhiêu mét?\n\n### Choices:\nA. Con thỏ; 9m\nB. Con ngựa; 9m\nC. Con ngựa; 90m\nD. Con thỏ; 90m\n\n### Explanation:\nĐể tính số con thỏ có một con nhân 5,5m/giờ và con nào di chuyển được.\n\nTrong trường hợp này, một con nào có thể chạy với vận tốc 25,2 km/giờ.\n\nÁp dụng công thức, ta có:\n\nDi = 25,2 x 5,5 x 7 = 1,3 m\n\nVậy, trong 1 phút là 1 phút.\n\n### Answer:\nA. con thỏ; 9m \nTestcase 9, time: 10.104972839355469, answer: A. Con thỏ; 9m | C. Con ngựa; 90m, Passed: 1, IsPass: False\n<|human|> Below is an instruction that describes a task. paired with the choices, one of the choices is the correct answer to the request. Write a response that appropriately completes the request.\n\n### Instruction:\nMột công ty, tuần một sản xuất được 3692 sản phẩm. Tuần hai, họ chỉ sản xuất được 2978 sản phẩm. Cả hai tuần, công ty đó sản xuất được số sản phẩm là:\n\n### Choices:\nA. 6570 sản phẩm\nB. 6660 sản phẩm\nC. 6670 sản phẩm\nD. 6560 sản phẩm\n\n### Explanation:\nĐể tính số cả hai tuần, ta trừ số cả hai tuần hay khỏi số phẩm:\n\nSố cả hai tuần = 2978 / 2 = 67.\n\nTuần hai tuần = 2978 * 2 = 29.\n\nVậy, số cả hai tuần là 29.\n\n### Answer:\nA. 6570 sản phẩm \nTestcase 10, time: 7.945563554763794, answer: A. 6570 sản phẩm | C. 6670 sản phẩm, Passed: 1, IsPass: False\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[97], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToo much token\u001b[39m\u001b[38;5;124m\"\u001b[39m, token_len)\n\u001b[1;32m      7\u001b[0m startTime()\n\u001b[0;32m----> 8\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mautoLLMFormat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtvdo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtvdo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchoices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m deltaTime \u001b[38;5;241m=\u001b[39m getTime()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m tvdo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n","Cell \u001b[0;32mIn[95], line 9\u001b[0m, in \u001b[0;36mautoLLMFormat\u001b[0;34m(question, choises, debug)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mautoLLMFormat\u001b[39m(question, choises \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      8\u001b[0m     prompt_template \u001b[38;5;241m=\u001b[39m ApplyPromptTemplate(question, choises, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_input_run\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m debug:\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28mprint\u001b[39m(res)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:208\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    168\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1134\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1146\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1147\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1148\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:271\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1719\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1711\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1712\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1713\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1714\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1715\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1716\u001b[0m     )\n\u001b[1;32m   1718\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1736\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1737\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1742\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1743\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2801\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2798\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2800\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2801\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2804\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2805\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2809\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1074\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:888\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    877\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    878\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m         output_attentions,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 390\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:331\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    329\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    334\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:183\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_attn\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, key, value, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, head_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 183\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_attn_weights:\n\u001b[1;32m    186\u001b[0m         attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull(\n\u001b[1;32m    187\u001b[0m             [], value\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mattn_weights\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mattn_weights\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    188\u001b[0m         )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"## Run public test dataset","metadata":{}},{"cell_type":"code","source":"result_test = []\nif os.path.exists(os.path.join(\"result\", \"result.txt\")):\n    with open(os.path.join(\"result\", \"result.txt\"), \"r\") as f:\n        result_test = f.read().split(\"\\n\")\ncount_id = len(result_test)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T19:02:46.959555Z","iopub.status.idle":"2023-11-28T19:02:46.960012Z","shell.execute_reply.started":"2023-11-28T19:02:46.959772Z","shell.execute_reply":"2023-11-28T19:02:46.959791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(test_data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"while count_id < len(test_data):\n    one_test_data = test_data[count_id]\n    startTime()\n    answer = autoLLMFormat(one_test_data['question'], one_test_data['choices'], False)\n    deltaTime = getTime()\n    result_test.append(\"{0}\".format(answer))\n    count_id += 1\n    if count_id%10 == 0:\n        with open(os.path.join(\"result\", \"result.txt\"), \"w\", encoding='utf-8') as f:\n            f.write(\"\\n\".join(result_test))\n    print(\"Testcase {0}, time: {1}, answer: {2}\".format(count_id, deltaTime, answer))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(os.path.join(\"result\", \"result.txt\"), \"w\", encoding='utf-8') as f:\n    f.write(\"\\n\".join(result_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n\".join(result_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert json to csv","metadata":{}},{"cell_type":"code","source":"!pip install pandas","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"json_result = []\nfor i in range(len(test_data)):\n    one_test_data = test_data[i]\n    json_result.append({\n        \"id\": one_test_data[\"id\"],\n        \"answer\": result_test[i]\n    })","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\njson_result_str = json.dumps(json_result)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_json(json_result_str)\ndf.to_csv(os.path.join(\"result\", \"result.csv\"))","metadata":{},"execution_count":null,"outputs":[]}]}