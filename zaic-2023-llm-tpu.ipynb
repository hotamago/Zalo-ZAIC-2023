{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[],"dockerImageVersionId":30589,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip3 install transformers datasets sentencepiece langchain peft trl -q\n!pip install torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q #Updating torch since we need the latest version\n!pip install torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q\n!pip uninstall tensorflow -y #If we don't do this, TF will take over TPU and cause permission error for PT\n!cp /kaggle/input/utils-xla/spmd_util.py . #From this repo: https://github.com/HeegyuKim/torch-xla-SPMD","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/HeegyuKim/torch-xla-SPMD","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp /kaggle/working/torch-xla-SPMD/spmd_util.py . #From this repo: https://github.com/HeegyuKim/torch-xla-SPMD","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport datasets\nimport torch.optim as optim\nimport torch_xla.debug.profiler as xp\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp #We also import mp modules if we wanna use that for some reason\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.test.test_utils as test_utils\nimport torch\nimport torch.nn as nn\nimport re\nimport torch_xla.experimental.xla_sharding as xs\nimport torch_xla.core.xla_model as xm\nfrom trl import DataCollatorForCompletionOnlyLM\nfrom transformers import (\n    GPTNeoXConfig, T5Config, LlamaConfig, AutoTokenizer, AutoModelForCausalLM, DataCollatorWithPadding, AutoConfig\n) # You can use any of models with those configs (even flan T5 xxl!). Other models are not supported.\n\nfrom transformers import logging as hf_logging\nimport torch.nn.functional as F\nimport torch_xla.runtime as xr\n\nxr.use_spmd()\n\nimport torch_xla.experimental.xla_sharding as xs # \"experimental\" prefix always means you're gonna have a good time LMAO\nfrom torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\nfrom torch_xla.experimental.xla_sharding import Mesh\n\nfrom peft import LoraConfig, TaskType, get_peft_model # If we wanna use peft. Quantazation requiers GPU though.\n# from spmd_util import partition_module                # You could experiment with using already quantazed models like 4bit/Llama-2-7b-Chat-GPTQ if you're feeling funny\nfrom langchain.prompts import PromptTemplate          # Please share your experiements if you find something :)\nfrom datasets import Dataset, load_dataset, concatenate_datasets\nfrom dataclasses import dataclass\nfrom tqdm import tqdm\n\n!export USE_TORCH=True #If we don't do this, transformers will seemingly bork the session upon import. Really weird error.\nos.environ[\"PJRT_DEVICE\"] = \"TPU\"\nos.environ.pop('TPU_PROCESS_ADDRESSES')\nos.environ.pop('CLOUD_TPU_TASK_ID')\nhf_logging.set_verbosity_error() # It can still display warnings which is a bit annoying but whatever","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:58:53.887274Z","iopub.execute_input":"2023-11-28T20:58:53.887526Z","iopub.status.idle":"2023-11-28T20:59:16.483984Z","shell.execute_reply.started":"2023-11-28T20:58:53.887501Z","shell.execute_reply":"2023-11-28T20:59:16.483077Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/usr/local/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport re\nimport torch_xla.experimental.xla_sharding as xs\nimport torch_xla.core.xla_model as xm\nfrom transformers import (\n    GPTNeoXConfig, T5Config, LlamaConfig, MistralConfig\n)\n\n# ends with $ to prevent sharding lora parameters\nGPTNEOX_RULES = (\n    # embeddings\n    (\"gpt_neox\\\\.embed_in\", (\"mp\", \"fsdp\")),\n    # atention\n    (\"attention\\\\.query_key_value$\", (\"fsdp\", \"mp\")),\n    (\"attention\\\\.dense$\", (\"mp\", \"fsdp\")),\n    # mlp\n    (\"mlp\\\\.dense_h_to_4h$\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.dense_4h_to_h$\", (\"mp\", \"fsdp\")),\n    # output\n    (\"embed_out\", (\"fsdp\", \"mp\")),\n)\n\nT5_RULES = (\n    # embeddings\n    (\"shared$\", (\"mp\", \"fsdp\")),\n    (\"embed_tokens$\", (\"mp\", \"fsdp\")),\n    \n    # attention\n    (\"q$\", (\"fsdp\", \"mp\")),\n    (\"k$\", (\"fsdp\", \"mp\")),\n    (\"v$\", (\"fsdp\", \"mp\")),\n    (\"o$\", (\"mp\", \"fsdp\")),\n\n    # mlp\n    (\"w$\", (\"fsdp\", \"mp\")),\n    (\"wi_0$\", (\"fsdp\", \"mp\")),\n    (\"wi_1$\", (\"fsdp\", \"mp\")),\n    (\"wo$\", (\"mp\", \"fsdp\")),\n\n    # seq2seq lm head\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n)\n\nLLAMA_RULES = (\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n    )\n    \nALL_RULES = [\n    (GPTNeoXConfig, GPTNEOX_RULES),\n    (T5Config, T5_RULES),\n    (LlamaConfig, LLAMA_RULES),\n    (MistralConfig, LLAMA_RULES)\n]\n\ndef find_rule(model):\n    for config, rule in ALL_RULES:\n        if model.config.__class__ == config:\n            return rule\n    raise Exception(\"unsupported model to partitioning\")\n\nstrkey2id = {\n    \"dp\": 0,\n    \"fsdp\": 1,\n    \"mp\": 2\n}\n\ndef partition_module(model, mesh, device=xm.xla_device(), verbose=False):\n    partition_specs = find_rule(model)\n    rule = [(k, tuple([strkey2id[x] for x in v])) for k, v in partition_specs]\n        \n    # print(rule)\n\n    for name, module in model.named_modules():\n        module.to(device)\n        # print(name, module.__class__.__name__)\n        if isinstance(module, (nn.Embedding, nn.Linear)):\n            for rule_pattern, spec in rule:\n                if re.findall(rule_pattern, name):\n                    if verbose:\n                        print(\"match\", rule_pattern, name)\n                    \n                    xs.mark_sharding(module.weight, mesh, spec)\n                    break\n        \ndef partition_module_dp(model, mesh, device=xm.xla_device(), verbose=False):\n    spec = (1, 2)\n\n    for name, module in model.named_modules():\n        module.to(device)\n        if isinstance(module, (nn.Embedding, nn.Linear)):\n            xs.mark_sharding(module.weight, mesh, spec)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T21:08:09.090637Z","iopub.execute_input":"2023-11-28T21:08:09.091028Z","iopub.status.idle":"2023-11-28T21:08:09.104935Z","shell.execute_reply.started":"2023-11-28T21:08:09.090996Z","shell.execute_reply":"2023-11-28T21:08:09.104040Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n# secret_wandb = user_secrets.get_secret(\"wandb\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login --token $secret_hf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = \"meta-math/MetaMath-Mistral-7B\"\n# base_model = \"EleutherAI/gpt-neo-2.7B\"\n# base_model = \"gpt2-xl\"\n# base_model = \"gpt2-large\"\n# base_model = \"gpt2\"\n# base_model = \"openchat/openchat_3.5\"\nnew_model = \"BK-BigAI-Math\"\nmodel_hotamath_path = \"/kaggle/working/BK-BigAI-Math\"","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:59:16.500514Z","iopub.execute_input":"2023-11-28T20:59:16.500779Z","iopub.status.idle":"2023-11-28T20:59:16.514082Z","shell.execute_reply.started":"2023-11-28T20:59:16.500738Z","shell.execute_reply":"2023-11-28T20:59:16.513504Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Download dataset","metadata":{}},{"cell_type":"code","source":"!mkdir dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\nhf_hub_download(repo_id=\"hotamago/ZAIC-2023\", filename=\"Elementary Maths Solving/test.zip\", revision=\"main\", repo_type=\"dataset\", local_dir=\"dataset\", local_dir_use_symlinks=False)\nhf_hub_download(repo_id=\"hotamago/ZAIC-2023\", filename=\"Elementary Maths Solving/train.zip\", revision=\"main\", repo_type=\"dataset\", local_dir=\"dataset\", local_dir_use_symlinks=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo apt-get install unzip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir datasetRaw\n!unzip -q -o \"dataset/Elementary Maths Solving/test.zip\" -d \"datasetRaw\"\n!unzip -q -o \"dataset/Elementary Maths Solving/train.zip\" -d \"datasetRaw\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport re\nimport time","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:59:26.454175Z","iopub.execute_input":"2023-11-28T20:59:26.454543Z","iopub.status.idle":"2023-11-28T20:59:26.458548Z","shell.execute_reply.started":"2023-11-28T20:59:26.454512Z","shell.execute_reply":"2023-11-28T20:59:26.457771Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_data = None\ntest_data = None\nwith open(os.path.join(\"datasetRaw\", \"train\", \"/kaggle/working/datasetRaw/math_train.json\"), \"r\") as f:\n    train_data = json.loads(f.read())['data']\nwith open(os.path.join(\"datasetRaw\", \"test\", \"/kaggle/working/datasetRaw/math_test.json\"), \"r\") as f:\n    test_data = json.loads(f.read())['data']","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:59:26.954109Z","iopub.execute_input":"2023-11-28T20:59:26.954443Z","iopub.status.idle":"2023-11-28T20:59:26.973871Z","shell.execute_reply.started":"2023-11-28T20:59:26.954415Z","shell.execute_reply":"2023-11-28T20:59:26.973273Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_data[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:59:26.993462Z","iopub.execute_input":"2023-11-28T20:59:26.993720Z","iopub.status.idle":"2023-11-28T20:59:27.001307Z","shell.execute_reply.started":"2023-11-28T20:59:26.993695Z","shell.execute_reply":"2023-11-28T20:59:27.000646Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'id': '1',\n 'question': 'Một người bán hàng bỏ ra 80,000 đồng tiền vốn và bị lỗ 6%. Để tính số tiền lỗ ta phải tính',\n 'choices': ['A. 80,000 : 6',\n  'B. 80,000 x 6',\n  'C. 80,000 : (6 x 100)',\n  'D. (80,000 x 6) : 100'],\n 'explanation': 'Theo đề bài, số tiền lỗ bằng 6% của 80 000 đồng . Để tìm số tiền lỗ ta có thể lấy 80 000 chia cho 100 rồi nhân với 6 (tức là 80 000 : 100 × 6) hoặc lấy 80000 nhân với 6 rồi chia cho 100 (tức là 80 000 × 6 : 100).',\n 'answer': 'D. (80,000 x 6) : 100'}"},"metadata":{}}]},{"cell_type":"code","source":"test_data[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:59:27.025477Z","iopub.execute_input":"2023-11-28T20:59:27.025718Z","iopub.status.idle":"2023-11-28T20:59:27.030263Z","shell.execute_reply.started":"2023-11-28T20:59:27.025694Z","shell.execute_reply":"2023-11-28T20:59:27.029638Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'id': '01-0203',\n 'question': 'Một cửa hàng đã bán 30% số hàng hiện có và thu được 15 000 000 đồng. Hỏi nếu bán hết hàng thì cửa hàng thu được bao nhiêu tiền?',\n 'choices': ['A. 4 500 000 đồng',\n  'B. 45 000 000 đồng',\n  'C. 50 000 000 đồng',\n  'D. 450 000 000 đồng']}"},"metadata":{}}]},{"cell_type":"code","source":"MAX_TOKEN_MODEL = 512","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:59:27.065426Z","iopub.execute_input":"2023-11-28T20:59:27.065672Z","iopub.status.idle":"2023-11-28T20:59:27.068930Z","shell.execute_reply.started":"2023-11-28T20:59:27.065647Z","shell.execute_reply":"2023-11-28T20:59:27.068238Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"DEFAULT_PAD_TOKEN = \"[PAD]\"\nDEFAULT_EOS_TOKEN = \"</s>\" # \"<|end_of_turn|>\" # \"</s>\" \"<|endoftext|>\"\nDEFAULT_BOS_TOKEN = \"<s>\" # \"<s>\" \"<|endoftext|>\"\nDEFAULT_UNK_TOKEN = \"<unk>\" # \"<unk>\" \"<|endoftext|>\"\nDEFAULT_BOI_TOKEN = \"[INST]\" # \"Human:\" # \"[INST]\" \"<|human|>\"\nDEFAULT_EOI_TOKEN = \"[/INST]\" # \"Assistant:\" # \"[/INST]\" \"<|human|>\"\nPROMPT_DICT = {\n    \"prompt_input\": (\n        \"Below is an instruction that describes a task, paired with the choices, one of the choices is the correct answer to the request. \"\n        \"Write a response that appropriately completes the request.\\n\\n\"\n        \"### Instruction:\\n{instruction}\\n\\n### Choices:\\n{choices}\"\n    ),\n    \"prompt_input_run\": (\n        DEFAULT_BOI_TOKEN + \" Below is an instruction that describes a task. paired with the choices, one of the choices is the correct answer to the request. \"\n        \"Write a response that appropriately completes the request.\\n\\n\"\n        \"### Instruction:\\n{instruction}\\n\\n### Choices:\\n{choices}\"\n#         \"\\n\" + DEFAULT_EOI_TOKEN + \" \\n\\n\"\n#         \"### Explanation:\\n Let's think step by step.\\n\"\n#         \"### Explanation:\\n Hãy suy nghĩ từng bước một.\\n\"\n    ),\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:59:27.109628Z","iopub.execute_input":"2023-11-28T20:59:27.109902Z","iopub.status.idle":"2023-11-28T20:59:27.114606Z","shell.execute_reply.started":"2023-11-28T20:59:27.109874Z","shell.execute_reply":"2023-11-28T20:59:27.113927Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"timeGlobal = 0\ndef startTime():\n    global timeGlobal\n    timeGlobal = time.time()\ndef getTime():\n    return (time.time() - timeGlobal)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:59:27.638027Z","iopub.execute_input":"2023-11-28T20:59:27.638372Z","iopub.status.idle":"2023-11-28T20:59:27.642432Z","shell.execute_reply.started":"2023-11-28T20:59:27.638342Z","shell.execute_reply":"2023-11-28T20:59:27.641810Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def ApplyPromptTemplate(instruction, choices, typeP = \"prompt_input\"):\n    return PROMPT_DICT[typeP].format(instruction = instruction, choices = \"\\n\".join(choices))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:59:27.989858Z","iopub.execute_input":"2023-11-28T20:59:27.990198Z","iopub.status.idle":"2023-11-28T20:59:27.994125Z","shell.execute_reply.started":"2023-11-28T20:59:27.990172Z","shell.execute_reply":"2023-11-28T20:59:27.993503Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Preprocess data","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\nimport random","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:59:29.305780Z","iopub.execute_input":"2023-11-28T20:59:29.306087Z","iopub.status.idle":"2023-11-28T20:59:29.309778Z","shell.execute_reply.started":"2023-11-28T20:59:29.306059Z","shell.execute_reply":"2023-11-28T20:59:29.309071Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"num_train_dataset = len(train_data)\nvalition_radio = 0.1\ntokenized_train_dataset_raw = train_data[:-int(num_train_dataset*valition_radio)]\ntokenized_val_dataset_raw = train_data[-int(num_train_dataset*valition_radio):]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:59:29.341383Z","iopub.execute_input":"2023-11-28T20:59:29.341642Z","iopub.status.idle":"2023-11-28T20:59:29.345778Z","shell.execute_reply.started":"2023-11-28T20:59:29.341617Z","shell.execute_reply":"2023-11-28T20:59:29.345080Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"datasetStruct = {\"input\":[], \"output\":[]}\ndataset = {\"text\":[]}\nnum_train_dataset = len(tokenized_train_dataset_raw)\nfor i in range(num_train_dataset):\n    ttdro = tokenized_train_dataset_raw[i]\n    \n    if \"explanation\" not in ttdro.keys():\n        continue\n    \n    input_content = \"{0} {1}\".format(\n        DEFAULT_BOI_TOKEN,\n        ApplyPromptTemplate(ttdro['question'], ttdro['choices']),\n    )\n    datasetStruct[\"input\"].append(input_content)\n    \n    if \"explanation\" not in ttdro.keys():\n        output_content = \"\\n{0} \\n\\n{1}\\n\\n{2} {3}\".format(\n            DEFAULT_EOI_TOKEN,\n            \"### Explanation:\\nNo explanation\",\n            \"### Answer:\\n{0}\".format(ttdro['answer']),\n            DEFAULT_EOS_TOKEN,\n        )\n    else:\n        output_content = \"\\n{0} \\n\\n{1}\\n\\n{2} {3}\".format(\n            DEFAULT_EOI_TOKEN,\n            \"### Explanation:\\n{0}\".format(ttdro['explanation']),\n            \"### Answer:\\n{0}\".format(ttdro['answer']),\n            DEFAULT_EOS_TOKEN,\n        )\n    datasetStruct[\"output\"].append(output_content)\n    \n    dataset[\"text\"].append(input_content + output_content)\n    \n    # No explantion\n#     output_content = \"\\n{0} \\n\\n{1}\\n\\n{2} {3}\".format(\n#             DEFAULT_EOI_TOKEN,\n#             \"### Explanation:\\nNo explanation\",\n#             \"### Answer:\\n{0}\".format(ttdro['answer']),\n#             DEFAULT_EOS_TOKEN,\n#         )\n#     datasetStruct[\"output\"].append(output_content)\n    \n#     dataset[\"text\"].append(input_content + output_content)\n#     <s>[INST][/INST] </s>","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:59:29.389449Z","iopub.execute_input":"2023-11-28T20:59:29.389710Z","iopub.status.idle":"2023-11-28T20:59:29.402808Z","shell.execute_reply.started":"2023-11-28T20:59:29.389683Z","shell.execute_reply":"2023-11-28T20:59:29.402163Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(dataset[\"text\"][33])","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:59:29.961796Z","iopub.execute_input":"2023-11-28T20:59:29.962131Z","iopub.status.idle":"2023-11-28T20:59:29.966398Z","shell.execute_reply.started":"2023-11-28T20:59:29.962102Z","shell.execute_reply":"2023-11-28T20:59:29.965754Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"[INST] Below is an instruction that describes a task, paired with the choices, one of the choices is the correct answer to the request. Write a response that appropriately completes the request.\n\n### Instruction:\nNhà Hiền có 7 con lợn. Số ga nhiều hơn số lợn 63 con. Vậy số gà gấp số lợn là:\n\n### Choices:\nA. 7 lần\nB. 8 lần\nC. 9 lần\nD. 10 lần\n[/INST] \n\n### Explanation:\nNhà Hiền có số con gà là: 63 + 7 = 70 (con)\n Số gà gấp số lợn là: 70 : 7 = 10 (lần)\n\n### Answer:\nD. 10 lần </s>\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(dataset[\"text\"]))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:59:30.161658Z","iopub.execute_input":"2023-11-28T20:59:30.161932Z","iopub.status.idle":"2023-11-28T20:59:30.165944Z","shell.execute_reply.started":"2023-11-28T20:59:30.161906Z","shell.execute_reply":"2023-11-28T20:59:30.165170Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"1080\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Train model","metadata":{}},{"cell_type":"markdown","source":"### Config","metadata":{}},{"cell_type":"code","source":"FLAGS = {'MAX_INPUT': MAX_TOKEN_MODEL,\n         'LOGGING_STEPS': 100,\n         'NUM_EPOCHS': 5,\n         'BATCH_SIZE': 8, #Making batch_size lower then 8 will result in slower training, but will take more memory. Fortunately, we have 128GBs. Setting higher batch_size doesn't seem to improve time.\n          'NUM_STEPS': len(dataset['text'])} ","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:59:31.661987Z","iopub.execute_input":"2023-11-28T20:59:31.662315Z","iopub.status.idle":"2023-11-28T20:59:31.666191Z","shell.execute_reply.started":"2023-11-28T20:59:31.662286Z","shell.execute_reply":"2023-11-28T20:59:31.665558Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Load model","metadata":{}},{"cell_type":"code","source":"# bnb_config = BitsAndBytesConfig(  \n#     load_in_4bit= True,\n#     bnb_4bit_quant_type= \"nf4\",\n#     bnb_4bit_compute_dtype= torch.bfloat16,\n#     bnb_4bit_use_double_quant= False,\n# )\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n#         model_hotamath_path,\n#     load_in_4bit=True,\n#     load_in_8bit= True,\n#     quantization_config=bnb_config,\n    torch_dtype=torch.bfloat16,\n#     torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\ncnt = 0\nfor param in model.parameters():\n    cnt += 1\n    param.requires_grad = False\n    if cnt > 270:\n        param.requires_grad = True","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-28T20:59:32.770566Z","iopub.execute_input":"2023-11-28T20:59:32.770949Z","iopub.status.idle":"2023-11-28T20:59:43.301569Z","shell.execute_reply.started":"2023-11-28T20:59:32.770914Z","shell.execute_reply":"2023-11-28T20:59:43.300834Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.99s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    base_model,\n    model_max_length=MAX_TOKEN_MODEL,\n    padding_side=\"right\",\n    use_fast=False,\n)\n# tokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:59:43.302870Z","iopub.execute_input":"2023-11-28T20:59:43.303136Z","iopub.status.idle":"2023-11-28T20:59:43.415247Z","shell.execute_reply.started":"2023-11-28T20:59:43.303110Z","shell.execute_reply":"2023-11-28T20:59:43.414611Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"special_tokens_dict = {\n    'additional_special_tokens': [DEFAULT_BOI_TOKEN, DEFAULT_EOI_TOKEN],\n    'pad_token': DEFAULT_PAD_TOKEN,\n    'bos_token': DEFAULT_BOS_TOKEN,\n    'eos_token': DEFAULT_EOS_TOKEN,\n    'unk_token': DEFAULT_UNK_TOKEN,\n}\nnum_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:59:43.416123Z","iopub.execute_input":"2023-11-28T20:59:43.416366Z","iopub.status.idle":"2023-11-28T20:59:58.182592Z","shell.execute_reply.started":"2023-11-28T20:59:43.416342Z","shell.execute_reply":"2023-11-28T20:59:58.181794Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"Embedding(32003, 4096)"},"metadata":{}}]},{"cell_type":"code","source":"print(tokenizer.encode(\"{0} Hello, how are you? \\n{1} I'm fine, thank you!{2}\".format(\n    DEFAULT_BOI_TOKEN,\n    DEFAULT_EOI_TOKEN,\n    DEFAULT_EOS_TOKEN,\n)))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:59:58.184386Z","iopub.execute_input":"2023-11-28T20:59:58.184633Z","iopub.status.idle":"2023-11-28T20:59:58.188812Z","shell.execute_reply.started":"2023-11-28T20:59:58.184609Z","shell.execute_reply":"2023-11-28T20:59:58.188208Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"[1, 32001, 28705, 22557, 28725, 910, 460, 368, 28804, 28705, 13, 32002, 28705, 315, 28742, 28719, 4433, 28725, 6979, 368, 28808, 2]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Check model","metadata":{}},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:59:58.189662Z","iopub.execute_input":"2023-11-28T20:59:58.189999Z","iopub.status.idle":"2023-11-28T20:59:58.200990Z","shell.execute_reply.started":"2023-11-28T20:59:58.189964Z","shell.execute_reply":"2023-11-28T20:59:58.200292Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"MistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(32003, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): MistralMLP(\n          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): MistralRMSNorm()\n        (post_attention_layernorm): MistralRMSNorm()\n      )\n    )\n    (norm): MistralRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32003, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\nprint_trainable_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:59:58.201869Z","iopub.execute_input":"2023-11-28T20:59:58.202202Z","iopub.status.idle":"2023-11-28T20:59:58.211797Z","shell.execute_reply.started":"2023-11-28T20:59:58.202169Z","shell.execute_reply":"2023-11-28T20:59:58.211089Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"trainable params: 698400768 || all params: 7241756672 || trainable%: 9.644079463486287\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Check testcase max token","metadata":{}},{"cell_type":"code","source":"max_token_of_dataset = 0\nlistLongToken = []\nfor i in range(len(dataset[\"text\"])):\n    text = dataset[\"text\"][i]\n    token_len = len(tokenizer.encode(text))\n    max_token_of_dataset = max(token_len, max_token_of_dataset)\n    if token_len > MAX_TOKEN_MODEL:\n        print(i, token_len, \"\\n\")\n        listLongToken.append(i)\n#         print(text)\nprint(max_token_of_dataset)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-28T20:59:58.212821Z","iopub.execute_input":"2023-11-28T20:59:58.213095Z","iopub.status.idle":"2023-11-28T20:59:59.369762Z","shell.execute_reply.started":"2023-11-28T20:59:58.213070Z","shell.execute_reply":"2023-11-28T20:59:59.368805Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"71 560 \n\n239 530 \n\n361 618 \n\n397 650 \n\n399 554 \n\n400 599 \n\n435 594 \n\n458 574 \n\n463 763 \n\n467 552 \n\n483 515 \n\n484 590 \n\n486 546 \n\n487 650 \n\n494 691 \n\n536 755 \n\n539 697 \n\n540 751 \n\n546 611 \n\n570 659 \n\n582 536 \n\n591 730 \n\n592 546 \n\n603 580 \n\n604 525 \n\n617 522 \n\n622 606 \n\n660 1050 \n\n724 677 \n\n742 584 \n\n754 769 \n\n761 800 \n\n764 563 \n\n766 729 \n\n802 659 \n\n814 789 \n\n847 575 \n\n854 790 \n\n864 752 \n\n873 553 \n\n965 551 \n\n976 626 \n\n998 724 \n\n1000 528 \n\n1013 617 \n\n1024 630 \n\n1026 882 \n\n1028 848 \n\n1044 755 \n\n1051 709 \n\n1053 541 \n\n1057 520 \n\n1073 600 \n\n1050\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Remove long token testcase","metadata":{}},{"cell_type":"code","source":"print(len(listLongToken))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:59:59.370519Z","iopub.execute_input":"2023-11-28T20:59:59.370808Z","iopub.status.idle":"2023-11-28T20:59:59.375866Z","shell.execute_reply.started":"2023-11-28T20:59:59.370772Z","shell.execute_reply":"2023-11-28T20:59:59.374946Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"53\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset[\"text\"] = [dataset[\"text\"][i] for i in range(len(dataset[\"text\"])) if i not in listLongToken]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:59:59.376895Z","iopub.execute_input":"2023-11-28T20:59:59.377252Z","iopub.status.idle":"2023-11-28T20:59:59.386182Z","shell.execute_reply.started":"2023-11-28T20:59:59.377226Z","shell.execute_reply":"2023-11-28T20:59:59.385144Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Check\nmax_token_of_dataset = 0\nfor i in range(len(dataset[\"text\"])):\n    text = dataset[\"text\"][i]\n    token_len = len(tokenizer.encode(text))\n    max_token_of_dataset = max(token_len, max_token_of_dataset)\n    if token_len > MAX_TOKEN_MODEL:\n        print(i, token_len, \"\\n\")\nprint(max_token_of_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T20:59:59.388596Z","iopub.execute_input":"2023-11-28T20:59:59.388855Z","iopub.status.idle":"2023-11-28T21:00:00.415993Z","shell.execute_reply.started":"2023-11-28T20:59:59.388828Z","shell.execute_reply":"2023-11-28T21:00:00.415313Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"511\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Nice stuct dataset","metadata":{}},{"cell_type":"code","source":"dataset = Dataset.from_dict(dataset)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T21:00:00.416902Z","iopub.execute_input":"2023-11-28T21:00:00.417149Z","iopub.status.idle":"2023-11-28T21:00:00.429064Z","shell.execute_reply.started":"2023-11-28T21:00:00.417124Z","shell.execute_reply":"2023-11-28T21:00:00.428437Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def preprocess_dataset_function(example):\n    text_tokens = tokenizer(example[\"text\"], truncation=True, max_length=MAX_TOKEN_MODEL, padding='max_length').input_ids #You can try 'padding_to_multiple_of=128'\n    return {\n        \"input_ids\": text_tokens,\n    }\n\ndataset = dataset.map(preprocess_dataset_function, batched=False, remove_columns=['text'], num_proc=96)\ndataset","metadata":{"execution":{"iopub.status.busy":"2023-11-28T21:00:00.429967Z","iopub.execute_input":"2023-11-28T21:00:00.430208Z","iopub.status.idle":"2023-11-28T21:00:14.841703Z","shell.execute_reply.started":"2023-11-28T21:00:00.430183Z","shell.execute_reply":"2023-11-28T21:00:14.840708Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"Map (num_proc=96): 100%|██████████| 1027/1027 [00:00<00:00, 1039.59 examples/s]\n/usr/local/lib/python3.10/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n/usr/local/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n  table = cls._concat_blocks(blocks, axis=0)\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids'],\n    num_rows: 1027\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"### DataCollator","metadata":{}},{"cell_type":"code","source":"from trl import DataCollatorForCompletionOnlyLM","metadata":{"execution":{"iopub.status.busy":"2023-11-28T21:00:14.843242Z","iopub.execute_input":"2023-11-28T21:00:14.843560Z","iopub.status.idle":"2023-11-28T21:00:14.848285Z","shell.execute_reply.started":"2023-11-28T21:00:14.843525Z","shell.execute_reply":"2023-11-28T21:00:14.847502Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"instruction_template = DEFAULT_BOI_TOKEN\nresponse_template = DEFAULT_EOI_TOKEN\ncollator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer, mlm=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T21:00:14.849298Z","iopub.execute_input":"2023-11-28T21:00:14.849610Z","iopub.status.idle":"2023-11-28T21:00:14.860011Z","shell.execute_reply.started":"2023-11-28T21:00:14.849577Z","shell.execute_reply":"2023-11-28T21:00:14.859254Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"### XLA Trick","metadata":{}},{"cell_type":"code","source":"training_loader = torch.utils.data.DataLoader(dataset, batch_size=FLAGS['BATCH_SIZE'], collate_fn=collator)\ndevice = xm.xla_device()","metadata":{"execution":{"iopub.status.busy":"2023-11-28T21:00:14.860858Z","iopub.execute_input":"2023-11-28T21:00:14.861071Z","iopub.status.idle":"2023-11-28T21:00:14.869297Z","shell.execute_reply.started":"2023-11-28T21:00:14.861049Z","shell.execute_reply":"2023-11-28T21:00:14.868614Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"model.config.__class__","metadata":{"execution":{"iopub.status.busy":"2023-11-28T21:00:14.870224Z","iopub.execute_input":"2023-11-28T21:00:14.870460Z","iopub.status.idle":"2023-11-28T21:00:14.878940Z","shell.execute_reply.started":"2023-11-28T21:00:14.870438Z","shell.execute_reply":"2023-11-28T21:00:14.878226Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"transformers.models.mistral.configuration_mistral.MistralConfig"},"metadata":{}}]},{"cell_type":"code","source":"config = AutoConfig.from_pretrained(base_model)\nnum_devices = xr.global_runtime_device_count()\nmesh_shape = (1, num_devices, 1)\ndevice_ids = np.array(range(num_devices))\nmesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))\npartition_module(model, mesh) # After this, the model is sharded between cores but still has the same API as if it was on single device. Neat.","metadata":{"execution":{"iopub.status.busy":"2023-11-28T21:08:27.065755Z","iopub.execute_input":"2023-11-28T21:08:27.066151Z","iopub.status.idle":"2023-11-28T21:08:46.540293Z","shell.execute_reply.started":"2023-11-28T21:08:27.066118Z","shell.execute_reply":"2023-11-28T21:08:46.539462Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"!export XLA_USE_BF16=1 #I'm not even sure that exporting does anything\ndef train(FLAGS):\n    num_iterations = int(FLAGS['NUM_STEPS'] / FLAGS['BATCH_SIZE'])\n    lr = 1e-5\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=FLAGS['NUM_STEPS'] * FLAGS['BATCH_SIZE']) #You would probably wanna use cosine scheduler or something it's really easy to change\n    for epoch in range(1, FLAGS['NUM_EPOCHS'] + 1):\n        model.train()\n        xm.master_print('Epoch {} train begin {}'.format(epoch, test_utils.now())) # master print is meant to be used inside xmp function to not have it printed 8 times but whatever\n        for step, batch in enumerate(training_loader):\n            optimizer.zero_grad()\n            input_ids, attention_mask, labels = batch.input_ids.to(device), batch.attention_mask.to(device), batch.labels.to(device)\n            xs.mark_sharding(input_ids, mesh, (0, 1))\n            xs.mark_sharding(attention_mask, mesh, (0, 1))\n            xs.mark_sharding(labels, mesh, (0, 1))\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            xm.mark_step()\n            if (step + 1) % FLAGS['LOGGING_STEPS'] == 0:\n                print(f'loss: {loss.item()}, time: {test_utils.now()}, step: {step}')\n            scheduler.step()\n        xm.master_print('Epoch {} train end {}'.format(epoch, test_utils.now()))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T21:08:46.541626Z","iopub.execute_input":"2023-11-28T21:08:46.541889Z","iopub.status.idle":"2023-11-28T21:08:47.230198Z","shell.execute_reply.started":"2023-11-28T21:08:46.541863Z","shell.execute_reply":"2023-11-28T21:08:47.228907Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"### Train","metadata":{}},{"cell_type":"code","source":"train(FLAGS) # \"unlimited power\" palpatine meme","metadata":{"execution":{"iopub.status.busy":"2023-11-28T21:08:47.231472Z","iopub.execute_input":"2023-11-28T21:08:47.231722Z","iopub.status.idle":"2023-11-28T21:17:34.486293Z","shell.execute_reply.started":"2023-11-28T21:08:47.231694Z","shell.execute_reply":"2023-11-28T21:17:34.484951Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Epoch 1 train begin 21:08:47\nloss: 0.39653581380844116, time: 21:11:45, step: 99\nEpoch 1 train end 21:12:39\nEpoch 2 train begin 21:12:39\nloss: 0.22950127720832825, time: 21:13:38, step: 99\nEpoch 2 train end 21:13:46\nEpoch 3 train begin 21:13:46\nloss: 0.14491340517997742, time: 21:14:54, step: 99\nEpoch 3 train end 21:15:02\nEpoch 4 train begin 21:15:02\nloss: 0.08223885297775269, time: 21:16:10, step: 99\nEpoch 4 train end 21:16:18\nEpoch 5 train begin 21:16:18\nloss: 0.058491867035627365, time: 21:17:27, step: 99\nEpoch 5 train end 21:17:34\n","output_type":"stream"}]},{"cell_type":"code","source":"model = model.cpu()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained(new_model)\nmodel.config.to_json_file(os.path.join(new_model, \"config.json\"))\ntokenizer.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T21:36:13.157601Z","iopub.execute_input":"2023-11-28T21:36:13.157987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r BK-BigAI-Math.zip BK-BigAI-Math","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'BK-BigAI-Math.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evalution","metadata":{}},{"cell_type":"markdown","source":"### load model by GPU","metadata":{}},{"cell_type":"code","source":"dataset = Dataset.from_dict(dataset)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name_or_path = \"/kaggle/working/BK-BigAI-Math\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name_or_path,\n#     torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n#     load_in_4bit=True,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    model_name_or_path,\n    model_max_length=512,\n    padding_side=\"right\",\n    use_fast=False,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.resize_token_embeddings(len(tokenizer))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Init function evalution","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline","metadata":{"execution":{"iopub.status.busy":"2023-11-28T21:25:45.701967Z","iopub.execute_input":"2023-11-28T21:25:45.702362Z","iopub.status.idle":"2023-11-28T21:25:45.768541Z","shell.execute_reply.started":"2023-11-28T21:25:45.702330Z","shell.execute_reply":"2023-11-28T21:25:45.767587Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"pipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n    temperature=0.01,\n    top_p=0.3,\n    top_k=5,\n    repetition_penalty=1.1,\n    pad_token_id=tokenizer.eos_token_id\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T21:25:46.981377Z","iopub.execute_input":"2023-11-28T21:25:46.981750Z","iopub.status.idle":"2023-11-28T21:26:31.192557Z","shell.execute_reply.started":"2023-11-28T21:25:46.981718Z","shell.execute_reply":"2023-11-28T21:26:31.191454Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"import random\n# globalRegxCompire = \"0-9a-zA-Z\\.\\:\\-\\^\\! \"\ndef niceValueToCompire(x):\n#     x = re.sub(\"[^{0}]\".format(globalRegxCompire), \"\", x)\n    x = re.sub(\"[ \\t\\n]\", \"\", x)\n    return x\ndef autoLLMFormat(question, choises = None, debug=False):\n    prompt_template = ApplyPromptTemplate(question, choises, \"prompt_input_run\")\n    res = pipe(prompt_template)[0]['generated_text']\n    if debug:\n        print(res)\n    x = re.findall(\"### Answer:[\\n ](.+)\", res)\n    \n    if choises == None:\n        return x\n    \n    choises_compare = [niceValueToCompire(choise_pred) for choise_pred in choises]\n\n    if len(x) == 0:\n        return choises[random.randrange(0, len(choises))]\n    \n    x = niceValueToCompire(x[0])\n    \n    if (x not in choises_compare):\n        return choises[random.randrange(0, len(choises))]\n    \n    for i in range(len(choises_compare)):\n        if x == choises_compare[i]:\n            return choises[i]\n    \n    return \"wtf\"","metadata":{"execution":{"iopub.status.busy":"2023-11-28T21:24:55.113075Z","iopub.execute_input":"2023-11-28T21:24:55.113376Z","iopub.status.idle":"2023-11-28T21:25:01.139814Z","shell.execute_reply.started":"2023-11-28T21:24:55.113346Z","shell.execute_reply":"2023-11-28T21:25:01.138523Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"### Run","metadata":{}},{"cell_type":"code","source":"count_proc_testcase = 0\ncount_pass_testcase = 0","metadata":{"execution":{"iopub.status.busy":"2023-11-28T21:29:07.854409Z","iopub.execute_input":"2023-11-28T21:29:07.854830Z","iopub.status.idle":"2023-11-28T21:29:07.859819Z","shell.execute_reply.started":"2023-11-28T21:29:07.854794Z","shell.execute_reply":"2023-11-28T21:29:07.858786Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"while count_proc_testcase < len(tokenized_val_dataset_raw):\n    tvdo = tokenized_val_dataset_raw[count_proc_testcase]\n    startTime()\n    answer = autoLLMFormat(tvdo['question'], tvdo['choices'], True)\n    deltaTime = getTime()\n\n    if answer == tvdo['answer']:\n        count_pass_testcase += 1\n    \n    count_proc_testcase += 1\n    print(\"Testcase {0}, time: {1}, answer: {2} | {3}, Passed: {4}, IsPass: {5}\".format(\n        count_proc_testcase,\n        deltaTime,\n        answer,\n        tvdo['answer'],\n        count_pass_testcase,\n        (answer == tvdo['answer'])\n    ))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T21:29:08.012777Z","iopub.execute_input":"2023-11-28T21:29:08.013023Z","iopub.status.idle":"2023-11-28T21:36:04.413305Z","shell.execute_reply.started":"2023-11-28T21:29:08.012999Z","shell.execute_reply":"2023-11-28T21:36:04.411580Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[50], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m tvdo \u001b[38;5;241m=\u001b[39m tokenized_val_dataset_raw[count_proc_testcase]\n\u001b[1;32m      3\u001b[0m startTime()\n\u001b[0;32m----> 4\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mautoLLMFormat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtvdo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtvdo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchoices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m deltaTime \u001b[38;5;241m=\u001b[39m getTime()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m tvdo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n","Cell \u001b[0;32mIn[46], line 9\u001b[0m, in \u001b[0;36mautoLLMFormat\u001b[0;34m(question, choises, debug)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mautoLLMFormat\u001b[39m(question, choises \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      8\u001b[0m     prompt_template \u001b[38;5;241m=\u001b[39m ApplyPromptTemplate(question, choises, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_input_run\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m debug:\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28mprint\u001b[39m(res)\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:208\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    168\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/pipelines/base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1134\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/pipelines/base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1146\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1147\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1148\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:271\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py:1719\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1711\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1712\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1713\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1714\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1715\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1716\u001b[0m     )\n\u001b[1;32m   1718\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1736\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1737\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1742\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1743\u001b[0m     )\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py:2801\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2798\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2800\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2801\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2804\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2805\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2809\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:1009\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1006\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1009\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1022\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:897\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    887\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    888\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    889\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    894\u001b[0m         use_cache,\n\u001b[1;32m    895\u001b[0m     )\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 897\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:639\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    638\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 639\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    642\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:175\u001b[0m, in \u001b[0;36mMistralMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"## Run public test dataset","metadata":{}},{"cell_type":"code","source":"result_test = []\nif os.path.exists(os.path.join(\"result\", \"result.txt\")):\n    with open(os.path.join(\"result\", \"result.txt\"), \"r\") as f:\n        result_test = f.read().split(\"\\n\")\ncount_id = len(result_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir result","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(test_data))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"while count_id < len(test_data):\n    one_test_data = test_data[count_id]\n    startTime()\n    answer = autoLLMFormat(one_test_data['question'], one_test_data['choices'], False)\n    deltaTime = getTime()\n    result_test.append(\"{0}\".format(answer))\n    count_id += 1\n    if count_id%10 == 0:\n        with open(os.path.join(\"result\", \"result.txt\"), \"w\", encoding='utf-8') as f:\n            f.write(\"\\n\".join(result_test))\n    print(\"Testcase {0}, time: {1}, answer: {2}\".format(count_id, deltaTime, answer))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(os.path.join(\"result\", \"result.txt\"), \"w\", encoding='utf-8') as f:\n    f.write(\"\\n\".join(result_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n\".join(result_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert json to csv","metadata":{}},{"cell_type":"code","source":"!pip install pandas","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"json_result = []\nfor i in range(len(test_data)):\n    one_test_data = test_data[i]\n    json_result.append({\n        \"id\": one_test_data[\"id\"],\n        \"answer\": result_test[i]\n    })","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\njson_result_str = json.dumps(json_result)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_json(json_result_str)\ndf.to_csv(os.path.join(\"result\", \"result.csv\"))","metadata":{},"execution_count":null,"outputs":[]}]}