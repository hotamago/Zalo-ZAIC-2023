{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install","metadata":{}},{"cell_type":"code","source":"!pip3 install huggingface-hub","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade openai\n!pip install --upgrade pydantic","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U peft\n%pip install -U accelerate\n%pip install -U trl","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !mkdir MetaMath-Mistral-7B\n# !huggingface-cli download meta-math/MetaMath-Mistral-7B --local-dir MetaMath-Mistral-7B --local-dir-use-symlinks False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install sentencepiece","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# VERSION = \"1.11\"\n# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py > /dev/null\n# !python pytorch-xla-env-setup.py --version $VERSION  > /dev/null","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import library","metadata":{}},{"cell_type":"code","source":"# import tensorflow as tf\n# import tensorflow_hub as hub\n# print(\"Tensorflow version \" + tf.__version__)\n# AUTO = tf.data.experimental.AUTOTUNE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Detect TPU, return appropriate distribution strategy\n# try:\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n#     print('Running on TPU ', tpu.master())\n# except ValueError:\n#     tpu = None\n\n# if tpu:\n#     tf.config.experimental_connect_to_cluster(tpu)\n#     tf.tpu.experimental.initialize_tpu_system(tpu)\n#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# else:\n#     strategy = tf.distribute.get_strategy() \n\n# print(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nimport os,torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:45:41.899591Z","iopub.execute_input":"2023-11-28T15:45:41.900011Z","iopub.status.idle":"2023-11-28T15:45:50.788259Z","shell.execute_reply.started":"2023-11-28T15:45:41.899983Z","shell.execute_reply":"2023-11-28T15:45:50.787317Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# # PyTorch XLA-specific imports\n# import torch_xla.core.xla_model as xm\n# import torch_xla.distributed.parallel_loader as pl\n# import torch_xla.distributed.xla_multiprocessing as xmp\n# import torch_xla.debug.metrics as met","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:45:50.793431Z","iopub.execute_input":"2023-11-28T15:45:50.793795Z","iopub.status.idle":"2023-11-28T15:45:50.798161Z","shell.execute_reply.started":"2023-11-28T15:45:50.793752Z","shell.execute_reply":"2023-11-28T15:45:50.797290Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n# secret_wandb = user_secrets.get_secret(\"wandb\")","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:45:50.799309Z","iopub.execute_input":"2023-11-28T15:45:50.799586Z","iopub.status.idle":"2023-11-28T15:45:51.201762Z","shell.execute_reply.started":"2023-11-28T15:45:50.799563Z","shell.execute_reply":"2023-11-28T15:45:51.200857Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login --token $secret_hf","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:45:51.204295Z","iopub.execute_input":"2023-11-28T15:45:51.204594Z","iopub.status.idle":"2023-11-28T15:45:53.212184Z","shell.execute_reply.started":"2023-11-28T15:45:51.204568Z","shell.execute_reply":"2023-11-28T15:45:53.211077Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# wandb.login(key = secret_wandb)\n# run = wandb.init(\n# #     project='Fine tuning MetaMath mistral 7B - ZAIC',\n#     project='Fine tuning openchat 7B - ZAIC',\n#     job_type=\"training\", \n#     anonymous=\"allow\"\n# )","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:45:53.213858Z","iopub.execute_input":"2023-11-28T15:45:53.214246Z","iopub.status.idle":"2023-11-28T15:45:53.218973Z","shell.execute_reply.started":"2023-11-28T15:45:53.214213Z","shell.execute_reply":"2023-11-28T15:45:53.218102Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"base_model = \"meta-math/MetaMath-Mistral-7B\"\n# base_model = \"gpt2-xl\"\n# base_model = \"openchat/openchat_3.5\"\nnew_model = \"BK-BigAI-Math\"\nmodel_hotamath_path = \"/kaggle/working/BK-BigAI-Math\"","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:45:53.220371Z","iopub.execute_input":"2023-11-28T15:45:53.221411Z","iopub.status.idle":"2023-11-28T15:45:53.231200Z","shell.execute_reply.started":"2023-11-28T15:45:53.221377Z","shell.execute_reply":"2023-11-28T15:45:53.230501Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Download dataset","metadata":{}},{"cell_type":"code","source":"!mkdir dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\nhf_hub_download(repo_id=\"hotamago/ZAIC-2023\", filename=\"Elementary Maths Solving/test.zip\", revision=\"main\", repo_type=\"dataset\", local_dir=\"dataset\", local_dir_use_symlinks=False)\nhf_hub_download(repo_id=\"hotamago/ZAIC-2023\", filename=\"Elementary Maths Solving/train.zip\", revision=\"main\", repo_type=\"dataset\", local_dir=\"dataset\", local_dir_use_symlinks=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo apt-get install unzip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir datasetRaw\n!unzip -q -o \"dataset/Elementary Maths Solving/test.zip\" -d \"datasetRaw\"\n!unzip -q -o \"dataset/Elementary Maths Solving/train.zip\" -d \"datasetRaw\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport re\nimport time","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:45:53.232152Z","iopub.execute_input":"2023-11-28T15:45:53.232423Z","iopub.status.idle":"2023-11-28T15:45:53.242163Z","shell.execute_reply.started":"2023-11-28T15:45:53.232400Z","shell.execute_reply":"2023-11-28T15:45:53.241222Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_data = None\ntest_data = None\nwith open(os.path.join(\"datasetRaw\", \"train\", \"/kaggle/working/datasetRaw/math_train.json\"), \"r\") as f:\n    train_data = json.loads(f.read())['data']\nwith open(os.path.join(\"datasetRaw\", \"test\", \"/kaggle/working/datasetRaw/math_test.json\"), \"r\") as f:\n    test_data = json.loads(f.read())['data']","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:45:53.243515Z","iopub.execute_input":"2023-11-28T15:45:53.243861Z","iopub.status.idle":"2023-11-28T15:45:53.273815Z","shell.execute_reply.started":"2023-11-28T15:45:53.243828Z","shell.execute_reply":"2023-11-28T15:45:53.273073Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_data[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:45:53.274845Z","iopub.execute_input":"2023-11-28T15:45:53.275113Z","iopub.status.idle":"2023-11-28T15:45:53.282510Z","shell.execute_reply.started":"2023-11-28T15:45:53.275088Z","shell.execute_reply":"2023-11-28T15:45:53.281660Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'id': '1',\n 'question': 'Một người bán hàng bỏ ra 80,000 đồng tiền vốn và bị lỗ 6%. Để tính số tiền lỗ ta phải tính',\n 'choices': ['A. 80,000 : 6',\n  'B. 80,000 x 6',\n  'C. 80,000 : (6 x 100)',\n  'D. (80,000 x 6) : 100'],\n 'explanation': 'Theo đề bài, số tiền lỗ bằng 6% của 80 000 đồng . Để tìm số tiền lỗ ta có thể lấy 80 000 chia cho 100 rồi nhân với 6 (tức là 80 000 : 100 × 6) hoặc lấy 80000 nhân với 6 rồi chia cho 100 (tức là 80 000 × 6 : 100).',\n 'answer': 'D. (80,000 x 6) : 100'}"},"metadata":{}}]},{"cell_type":"code","source":"test_data[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:45:53.283779Z","iopub.execute_input":"2023-11-28T15:45:53.284108Z","iopub.status.idle":"2023-11-28T15:45:53.294034Z","shell.execute_reply.started":"2023-11-28T15:45:53.284077Z","shell.execute_reply":"2023-11-28T15:45:53.293228Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'id': '01-0203',\n 'question': 'Một cửa hàng đã bán 30% số hàng hiện có và thu được 15 000 000 đồng. Hỏi nếu bán hết hàng thì cửa hàng thu được bao nhiêu tiền?',\n 'choices': ['A. 4 500 000 đồng',\n  'B. 45 000 000 đồng',\n  'C. 50 000 000 đồng',\n  'D. 450 000 000 đồng']}"},"metadata":{}}]},{"cell_type":"code","source":"DEFAULT_PAD_TOKEN = \"[PAD]\"\nDEFAULT_EOS_TOKEN = \"</s>\" # \"<|end_of_turn|>\" # \"</s>\"\nDEFAULT_BOS_TOKEN = \"<s>\"\nDEFAULT_UNK_TOKEN = \"<unk>\"\nDEFAULT_BOI_TOKEN = \"[INST]\" # \"Human:\" # \"[INST]\"\nDEFAULT_EOI_TOKEN = \"[/INST]\" # \"Assistant:\" # \"[/INST]\"\nPROMPT_DICT = {\n    \"prompt_input\": (\n        \"Below is an instruction that describes a task, paired with the choices, one of the choices is the correct answer to the request. \"\n        \"Write a response that appropriately completes the request.\\n\\n\"\n        \"### Instruction:\\n{instruction}\\n\\n### Choices:\\n{choices}\"\n    ),\n    \"prompt_input_run\": (\n        DEFAULT_BOI_TOKEN  + \" Below is an instruction that describes a task. \"\n        \"Write a response that appropriately completes the request.\\n\\n\"\n        \"### Instruction:\\n{instruction}\\n\\n### Choices:\\n{choices}\"\n        \"\\n\" + DEFAULT_EOI_TOKEN + \" \\n\\n\"\n#         \"### Explanation:\\n Let's think step by step.\\n\"\n        \"### Explanation:\\n Hãy suy nghĩ từng bước một.\\n\"\n    ),\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:45:53.295227Z","iopub.execute_input":"2023-11-28T15:45:53.295530Z","iopub.status.idle":"2023-11-28T15:45:53.302678Z","shell.execute_reply.started":"2023-11-28T15:45:53.295506Z","shell.execute_reply":"2023-11-28T15:45:53.301826Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"timeGlobal = 0\ndef startTime():\n    global timeGlobal\n    timeGlobal = time.time()\ndef getTime():\n    return (time.time() - timeGlobal)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:45:53.303710Z","iopub.execute_input":"2023-11-28T15:45:53.304042Z","iopub.status.idle":"2023-11-28T15:45:53.317045Z","shell.execute_reply.started":"2023-11-28T15:45:53.304010Z","shell.execute_reply":"2023-11-28T15:45:53.316298Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def ApplyPromptTemplate(instruction, choices, typeP = \"prompt_input\"):\n    return PROMPT_DICT[typeP].format(instruction = instruction, choices = \"\\n\".join(choices))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:45:53.320107Z","iopub.execute_input":"2023-11-28T15:45:53.320393Z","iopub.status.idle":"2023-11-28T15:45:53.327893Z","shell.execute_reply.started":"2023-11-28T15:45:53.320369Z","shell.execute_reply":"2023-11-28T15:45:53.327091Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Add explantion by GPT 3.5 Tubo","metadata":{}},{"cell_type":"code","source":"from openai import OpenAI\nclient = OpenAI(\n    api_key=user_secrets.get_secret(\"OPENAI_API_KEY\"),\n#     organization='org-j48waUrvSOM1n0J1SLXiAr8n',\n)\n\ndef autoGPTAddExplantion(problem, answer):\n    response = client.chat.completions.create(\n      model=\"gpt-3.5-turbo\",\n      messages=[\n        {\"role\": \"system\", \"content\": \"Explan by Vietnamese step-by-step for given answer to given problem.\\nRule:\\n- No markdown format\\n- Given answer always true for given problem\\n- No title\\n- Short explantion\\n\"},\n        {\"role\": \"user\", \"content\": f\"### Problem:\\n{problem}\\n\\n### Answer:\\n{answer}\"},\n        {\"role\": \"system\", \"content\": \"Giải thích: \"},\n      ],\n#       max_tokens=512,\n      temperature=0,\n      top_p=1.0,\n#       top_k=50,\n    )\n    return response.choices[0].message.content","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"startTime()\ncntSt = 0\ni = 0\ntimeWait = 20\nwhile i < len(train_data):\n    singleData = train_data[i]\n    if \"explanation\" in singleData.keys():\n        i += 1\n        continue\n    print(f\"Runing testcase ({i})\")\n    \n    try:\n        res = autoGPTAddExplantion(singleData['question'], singleData['answer'])\n    except Exception as inst:\n        print(type(inst))\n        print(inst)\n        \n        time.sleep(timeWait)\n        timeWait *= 2\n        if timeWait > 80:\n            break\n        print\n        continue\n    \n    timeWait = 20\n        \n    cntSt += 1\n    train_data[i]['explanation'] = res\n    print(f\"Done testcase ({i})\")\n    #     print(train_data[i])\n    if cntSt%3 == 0:\n        deltaTime = getTime()\n        if (60 - deltaTime) > -1:\n            time.sleep(60 - deltaTime + 2)\n        startTime()\n        cntSt = 0\n    \n    i += 1\nprint(\"Done all testcase\")","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/working/datasetRaw/math_train.json\", \"w\", encoding='utf-8') as f:\n    json.dump({\n        \"__count__\": len(train_data),\n        \"data\": train_data\n    }, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add data by education program","metadata":{}},{"cell_type":"markdown","source":"Todo: Create data generation algorithms for each chapter.\n\nBelow is class base, change \\_call function to a function that create radom question for that chapter","metadata":{}},{"cell_type":"code","source":"class BaseGenChapter:\n    def __init__(*args, **kwargs):\n        pass\n    \n    def _call(self, *args, **kwargs) -> str:\n        raise NotImplementedError(\"_call not Implement yet\")\n    \n    def call(self, *args, **kwargs) -> str:\n        return self._call(*args, **kwargs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocess data","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\nimport random","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:45:53.328894Z","iopub.execute_input":"2023-11-28T15:45:53.329153Z","iopub.status.idle":"2023-11-28T15:45:53.338005Z","shell.execute_reply.started":"2023-11-28T15:45:53.329131Z","shell.execute_reply":"2023-11-28T15:45:53.337111Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# random.shuffle(train_data)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:45:53.407818Z","iopub.execute_input":"2023-11-28T15:45:53.408102Z","iopub.status.idle":"2023-11-28T15:45:53.411996Z","shell.execute_reply.started":"2023-11-28T15:45:53.408077Z","shell.execute_reply":"2023-11-28T15:45:53.411058Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"num_train_dataset = len(train_data)\nvalition_radio = 0.1\ntokenized_train_dataset_raw = train_data[:-int(num_train_dataset*valition_radio)]\ntokenized_val_dataset_raw = train_data[-int(num_train_dataset*valition_radio):]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:45:53.577634Z","iopub.execute_input":"2023-11-28T15:45:53.578254Z","iopub.status.idle":"2023-11-28T15:45:53.582651Z","shell.execute_reply.started":"2023-11-28T15:45:53.578226Z","shell.execute_reply":"2023-11-28T15:45:53.581758Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"datasetStruct = {\"input\":[], \"output\":[]}\ndataset = {\"text\":[]}\nnum_train_dataset = len(tokenized_train_dataset_raw)\nfor i in range(num_train_dataset):\n    ttdro = tokenized_train_dataset_raw[i]\n    \n    if \"explanation\" not in ttdro.keys():\n        continue\n    \n    input_content = \"{0} {1}\".format(\n        DEFAULT_BOI_TOKEN,\n        ApplyPromptTemplate(ttdro['question'], ttdro['choices']),\n    )\n    datasetStruct[\"input\"].append(input_content)\n    \n    if \"explanation\" not in ttdro.keys():\n        output_content = \"\\n{0} \\n\\n{1}\\n\\n{2} {3}\".format(\n            DEFAULT_EOI_TOKEN,\n            \"### Explanation:\\nNo explanation{0}\",\n            \"### Answer:\\n{0}\".format(ttdro['answer']),\n            DEFAULT_EOS_TOKEN,\n        )\n    else:\n        output_content = \"\\n{0} \\n\\n{1}\\n\\n{2} {3}\".format(\n            DEFAULT_EOI_TOKEN,\n            \"### Explanation:\\n{0}\".format(ttdro['explanation']),\n            \"### Answer:\\n{0}\".format(ttdro['answer']),\n            DEFAULT_EOS_TOKEN,\n        )\n    datasetStruct[\"output\"].append(output_content)\n    \n    dataset[\"text\"].append(input_content + output_content)\n#     <s>[INST][/INST] </s>","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:45:53.720123Z","iopub.execute_input":"2023-11-28T15:45:53.720696Z","iopub.status.idle":"2023-11-28T15:45:53.738429Z","shell.execute_reply.started":"2023-11-28T15:45:53.720668Z","shell.execute_reply":"2023-11-28T15:45:53.737188Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print(dataset[\"text\"][32])","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:45:53.943206Z","iopub.execute_input":"2023-11-28T15:45:53.943608Z","iopub.status.idle":"2023-11-28T15:45:53.948767Z","shell.execute_reply.started":"2023-11-28T15:45:53.943577Z","shell.execute_reply":"2023-11-28T15:45:53.947892Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"[INST] Below is an instruction that describes a task, paired with the choices, one of the choices is the correct answer to the request. Write a response that appropriately completes the request.\n\n### Instruction:\nNgày thứ nhất, bác Thái thu hoạch được 250 kg nhãn. Ngày thứ hai, số ki- lô-gam nhãn bác Thái thu hoạch được đã giảm đi 2 lần so với ngày thứ nhất. Vậy cả hai ngày bác Thái thu hoạch được số ki-lô-gam nhãn là:\n\n### Choices:\nA. 500 kg\nB. 750 kg\nC. 125kg\nD. 375 kg\n[/INST] \n\n### Explanation:\nNgày thứ hai, bác Thái thu hoạch được số ki-lô-ham nhãn là: 250 : 2 = 125 (kg) \n Cả hai ngày bác Thái thu hoạch được số ki-lô-gam nhãn là: $250 + 125 = 375$ (kg)\n Đáp số: 375 kg\n\n### Answer:\nD. 375 kg </s>\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(dataset[\"text\"]))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:45:54.352331Z","iopub.execute_input":"2023-11-28T15:45:54.352706Z","iopub.status.idle":"2023-11-28T15:45:54.357658Z","shell.execute_reply.started":"2023-11-28T15:45:54.352677Z","shell.execute_reply":"2023-11-28T15:45:54.356734Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"1080\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Train model","metadata":{}},{"cell_type":"markdown","source":"### Load model","metadata":{}},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(  \n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.bfloat16,\n    bnb_4bit_use_double_quant= False,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n#         model_hotamath_path,\n    load_in_4bit=True,\n#     load_in_8bit= True,\n    quantization_config=bnb_config,\n    torch_dtype=torch.bfloat16,\n#     torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    base_model,\n    model_max_length=512,\n    padding_side=\"right\",\n    use_fast=False,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"special_tokens_dict = {\n    'additional_special_tokens': [DEFAULT_BOI_TOKEN, DEFAULT_EOI_TOKEN],\n    'pad_token': DEFAULT_PAD_TOKEN,\n    'bos_token': DEFAULT_BOS_TOKEN,\n    'eos_token': DEFAULT_EOS_TOKEN,\n    'unk_token': DEFAULT_UNK_TOKEN,\n}\nnum_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.encode(\"{0} Hello, how are you? \\n{1} I'm fine, thank you!{2}\".format(\n    DEFAULT_BOI_TOKEN,\n    DEFAULT_EOI_TOKEN,\n    DEFAULT_EOS_TOKEN,\n)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check max token","metadata":{}},{"cell_type":"code","source":"max_token_of_dataset = 0\nlistLongToken = []\nfor i in range(len(dataset[\"text\"])):\n    text = dataset[\"text\"][i]\n    token_len = len(tokenizer.encode(text))\n    max_token_of_dataset = max(token_len, max_token_of_dataset)\n    if token_len > 512:\n        print(i, token_len, \"\\n\")\n        listLongToken.append(i)\n#         print(text)\nprint(max_token_of_dataset)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove long token","metadata":{}},{"cell_type":"code","source":"print(len(listLongToken))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[\"text\"] = [dataset[\"text\"][i] for i in range(len(dataset[\"text\"])) if i not in listLongToken]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check\nmax_token_of_dataset = 0\nfor i in range(len(dataset[\"text\"])):\n    text = dataset[\"text\"][i]\n    token_len = len(tokenizer.encode(text))\n    max_token_of_dataset = max(token_len, max_token_of_dataset)\n    if token_len > 512:\n        print(i, token_len, \"\\n\")\nprint(max_token_of_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Nice stuct dataset","metadata":{}},{"cell_type":"code","source":"dataset = Dataset.from_dict(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DataCollator","metadata":{}},{"cell_type":"code","source":"from trl import DataCollatorForCompletionOnlyLM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"instruction_template = DEFAULT_BOI_TOKEN\nresponse_template = DEFAULT_EOI_TOKEN\ncollator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer, mlm=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setup model for train","metadata":{}},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=2,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=200,\n    logging_steps=10,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    save_total_limit = 1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    report_to=\"none\"\n#     report_to=\"wandb\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import bitsandbytes as bnb\n# from torch import nn\n# from transformers.trainer_pt_utils import get_parameter_names\n\n# no_decay = [\"bias\", \"LayerNorm.weight\"]\n# optimizer_grouped_parameters = [{\n#     \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n#     \"weight_decay\": training_arguments.weight_decay,\n# },\n# {\n#     \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n#     \"weight_decay\": 0.0,\n# }]\n\n\n# optimizer_kwargs = {\n#     \"betas\": (training_arguments.adam_beta1, training_arguments.adam_beta2),\n#     \"eps\": training_arguments.adam_epsilon,\n# }\n# optimizer_kwargs[\"lr\"] = training_arguments.learning_rate\n# adam_bnb_optim = bnb.optim.Adam8bit(\n#     optimizer_grouped_parameters,\n#     betas=(training_arguments.adam_beta1, training_arguments.adam_beta2),\n#     eps=training_arguments.adam_epsilon,\n#     lr=training_arguments.learning_rate,\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = model.to(\"cuda:0\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_trainable_parameters(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n#     model = base_model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    max_seq_length=512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n    data_collator=collator,\n#     optimizers=(adam_bnb_optim, None)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.config.use_cache = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train and save","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.save_pretrained(new_model)\ntrainer.model.save_pretrained(new_model)\nmodel.config.to_json_file(os.path.join(new_model, \"config.json\"))\ntokenizer.save_pretrained(new_model)\nwandb.finish()\nmodel.config.use_cache = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r BK-BigAI-Math.zip BK-BigAI-Math","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'BK-BigAI-Math.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evalution","metadata":{}},{"cell_type":"markdown","source":"### Download model","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\nhf_hub_download(repo_id=\"hotamago/ZAIC-2023-Model\", filename=\"Hota-Math.zip\", repo_type=\"model\", local_dir=\"/kaggle/working/\", local_dir_use_symlinks=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip -q -o Hota-Math.zip -d ./","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load model","metadata":{}},{"cell_type":"code","source":"dataset = Dataset.from_dict(dataset)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name_or_path = \"/kaggle/working/BK-BigAI-Math\"\n# bnb_config = BitsAndBytesConfig(  \n#     load_in_4bit= True,\n#     bnb_4bit_quant_type= \"nf4\",\n#     bnb_4bit_compute_dtype= torch.bfloat16,\n#     bnb_4bit_use_double_quant= False,\n# )\n# model = AutoModelForCausalLM.from_pretrained(\n#         model_name_or_path,\n#         load_in_4bit=True,\n#         quantization_config=bnb_config,\n#         torch_dtype=torch.bfloat16,\n#         device_map=\"auto\",\n#         trust_remote_code=True,\n# )\n\n# model = AutoModelForCausalLM.from_pretrained(\n#         model_name_or_path,\n#         torch_dtype=torch.bfloat16,\n#         device_map=\"auto\",\n#         trust_remote_code=True,\n# )\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name_or_path,\n#     torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    load_in_4bit=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:46:36.640470Z","iopub.execute_input":"2023-11-28T15:46:36.641210Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eb9698ad8084406a7b6fff2b0a27c40"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    model_name_or_path,\n    model_max_length=512,\n    padding_side=\"right\",\n    use_fast=False,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# special_tokens_dict = {'additional_special_tokens': ['[INST]','[/INST]']}\n# num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setup pipeline with auto answer get","metadata":{}},{"cell_type":"code","source":"pipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n    temperature=0.01,\n    top_p=0.3,\n    top_k=5,\n    repetition_penalty=1.1,\n    pad_token_id=tokenizer.eos_token_id\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n# globalRegxCompire = \"0-9a-zA-Z\\.\\:\\-\\^\\! \"\ndef niceValueToCompire(x):\n#     x = re.sub(\"[^{0}]\".format(globalRegxCompire), \"\", x)\n    x = re.sub(\"[ \\t\\n]\", \"\", x)\n    return x\ndef autoLLMFormat(question, choises = None, debug=False):\n    prompt_template = ApplyPromptTemplate(question, choises, \"prompt_input_run\")\n    res = pipe(prompt_template)[0]['generated_text']\n    if debug:\n        print(res)\n    x = re.findall(\"### Answer:[\\n ](.+)\", res)\n    \n    if choises == None:\n        return x\n    \n    choises_compare = [niceValueToCompire(choise_pred) for choise_pred in choises]\n\n    if len(x) == 0:\n        return choises[random.randrange(0, len(choises))]\n    \n    x = niceValueToCompire(x[0])\n    \n    if (x not in choises_compare):\n        return choises[random.randrange(0, len(choises))]\n    \n    for i in range(len(choises_compare)):\n        if x == choises_compare[i]:\n            return choises[i]\n    \n    return \"wtf\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Run evalution","metadata":{}},{"cell_type":"code","source":"count_proc_testcase = 0\ncount_pass_testcase = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"while count_proc_testcase < len(tokenized_val_dataset_raw):\n    tvdo = tokenized_val_dataset_raw[count_proc_testcase]\n    startTime()\n    answer = autoLLMFormat(tvdo['question'], tvdo['choices'], True)\n    deltaTime = getTime()\n\n    if answer == tvdo['answer']:\n        count_pass_testcase += 1\n    \n    count_proc_testcase += 1\n    print(\"Testcase {0}, time: {1}, answer: {2} | {3}, Passed: {4}, IsPass: {5}\".format(\n        count_proc_testcase,\n        deltaTime,\n        answer,\n        tvdo['answer'],\n        count_pass_testcase,\n        (answer == tvdo['answer'])\n    ))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run public test dataset","metadata":{}},{"cell_type":"code","source":"result_test = []\nif os.path.exists(os.path.join(\"result\", \"result.txt\")):\n    with open(os.path.join(\"result\", \"result.txt\"), \"r\") as f:\n        result_test = f.read().split(\"\\n\")\ncount_id = len(result_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(test_data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"while count_id < len(test_data):\n    one_test_data = test_data[count_id]\n    startTime()\n    answer = autoLLMFormat(one_test_data['question'], one_test_data['choices'], False)\n    deltaTime = getTime()\n    result_test.append(\"{0}\".format(answer))\n    count_id += 1\n    if count_id%10 == 0:\n        with open(os.path.join(\"result\", \"result.txt\"), \"w\", encoding='utf-8') as f:\n            f.write(\"\\n\".join(result_test))\n    print(\"Testcase {0}, time: {1}, answer: {2}\".format(count_id, deltaTime, answer))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(os.path.join(\"result\", \"result.txt\"), \"w\", encoding='utf-8') as f:\n    f.write(\"\\n\".join(result_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n\".join(result_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}