{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install","metadata":{}},{"cell_type":"code","source":"!pip3 install huggingface-hub","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","execution":{"iopub.status.busy":"2023-11-24T12:48:25.770627Z","iopub.execute_input":"2023-11-24T12:48:25.771159Z","iopub.status.idle":"2023-11-24T12:48:42.247058Z","shell.execute_reply.started":"2023-11-24T12:48:25.771120Z","shell.execute_reply":"2023-11-24T12:48:42.245605Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (0.17.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub) (3.12.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub) (2023.10.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub) (4.66.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub) (4.5.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub) (2023.7.22)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade openai\n!pip install --upgrade pydantic","metadata":{"execution":{"iopub.status.busy":"2023-11-24T12:48:42.249699Z","iopub.execute_input":"2023-11-24T12:48:42.250196Z","iopub.status.idle":"2023-11-24T12:49:31.343820Z","shell.execute_reply.started":"2023-11-24T12:48:42.250149Z","shell.execute_reply":"2023-11-24T12:49:31.342616Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting openai\n  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/30/1d/27c3571504fb6fb1e9f7c906d93590ead22f5f34910489e155ee28512eeb/openai-1.3.5-py3-none-any.whl.metadata\n  Downloading openai-1.3.5-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: anyio<4,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (3.7.1)\nCollecting distro<2,>=1.7.0 (from openai)\n  Downloading distro-1.8.0-py3-none-any.whl (20 kB)\nCollecting httpx<1,>=0.23.0 (from openai)\n  Obtaining dependency information for httpx<1,>=0.23.0 from https://files.pythonhosted.org/packages/a2/65/6940eeb21dcb2953778a6895281c179efd9100463ff08cb6232bb6480da7/httpx-0.25.2-py3-none-any.whl.metadata\n  Downloading httpx-0.25.2-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.10.12)\nRequirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai) (4.66.1)\nRequirement already satisfied: typing-extensions<5,>=4.5 in /opt/conda/lib/python3.10/site-packages (from openai) (4.5.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai) (3.4)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\nCollecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/56/ba/78b0a99c4da0ff8b0f59defa2f13ca4668189b134bd9840b6202a93d9a0f/httpcore-1.0.2-py3-none-any.whl.metadata\n  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\nDownloading openai-1.3.5-py3-none-any.whl (220 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.8/220.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading httpx-0.25.2-py3-none-any.whl (74 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: httpcore, distro, httpx, openai\nSuccessfully installed distro-1.8.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.5\nRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (1.10.12)\nCollecting pydantic\n  Obtaining dependency information for pydantic from https://files.pythonhosted.org/packages/0a/2b/64066de1c4cf3d4ed623beeb3bbf3f8d0cc26661f1e7d180ec5eb66b75a5/pydantic-2.5.2-py3-none-any.whl.metadata\n  Downloading pydantic-2.5.2-py3-none-any.whl.metadata (65 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting annotated-types>=0.4.0 (from pydantic)\n  Obtaining dependency information for annotated-types>=0.4.0 from https://files.pythonhosted.org/packages/28/78/d31230046e58c207284c6b2c4e8d96e6d3cb4e52354721b944d3e1ee4aa5/annotated_types-0.6.0-py3-none-any.whl.metadata\n  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\nCollecting pydantic-core==2.14.5 (from pydantic)\n  Obtaining dependency information for pydantic-core==2.14.5 from https://files.pythonhosted.org/packages/7c/f5/3e59681bd53955da311a7f4efbb6315d01006e9d18b8a06b527a22d3d923/pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\nCollecting typing-extensions>=4.6.1 (from pydantic)\n  Obtaining dependency information for typing-extensions>=4.6.1 from https://files.pythonhosted.org/packages/24/21/7d397a4b7934ff4028987914ac1044d3b7d52712f30e2ac7a2ae5bc86dd0/typing_extensions-4.8.0-py3-none-any.whl.metadata\n  Downloading typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)\nDownloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\nDownloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\nInstalling collected packages: typing-extensions, annotated-types, pydantic-core, pydantic\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.5.0\n    Uninstalling typing_extensions-4.5.0:\n      Successfully uninstalled typing_extensions-4.5.0\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 1.10.12\n    Uninstalling pydantic-1.10.12:\n      Successfully uninstalled pydantic-1.10.12\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.1 which is incompatible.\njupyterlab 4.0.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.0 requires jupyterlab<5.0.0a0,>=4.0.6, but you have jupyterlab 4.0.5 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.3 which is incompatible.\ntensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.8.0 which is incompatible.\ntensorflow-probability 0.21.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\ntensorflowjs 4.13.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\nydata-profiling 4.5.1 requires pydantic<2,>=1.8.1, but you have pydantic 2.5.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed annotated-types-0.6.0 pydantic-2.5.2 pydantic-core-2.14.5 typing-extensions-4.7.1\n","output_type":"stream"}]},{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U peft\n%pip install -U accelerate\n%pip install -U trl","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-24T12:49:31.345462Z","iopub.execute_input":"2023-11-24T12:49:31.345839Z","iopub.status.idle":"2023-11-24T12:51:04.351197Z","shell.execute_reply.started":"2023-11-24T12:49:31.345807Z","shell.execute_reply":"2023-11-24T12:51:04.349648Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# !mkdir MetaMath-Mistral-7B\n# !huggingface-cli download meta-math/MetaMath-Mistral-7B --local-dir MetaMath-Mistral-7B --local-dir-use-symlinks False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import library","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nimport os,torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2023-11-24T12:53:37.406317Z","iopub.execute_input":"2023-11-24T12:53:37.406812Z","iopub.status.idle":"2023-11-24T12:54:00.597994Z","shell.execute_reply.started":"2023-11-24T12:53:37.406771Z","shell.execute_reply":"2023-11-24T12:54:00.596883Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n","output_type":"stream"},{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nsecret_wandb = user_secrets.get_secret(\"wandb\")","metadata":{"execution":{"iopub.status.busy":"2023-11-24T12:54:00.599899Z","iopub.execute_input":"2023-11-24T12:54:00.600732Z","iopub.status.idle":"2023-11-24T12:54:01.354103Z","shell.execute_reply.started":"2023-11-24T12:54:00.600695Z","shell.execute_reply":"2023-11-24T12:54:01.352821Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login --token $secret_hf","metadata":{"execution":{"iopub.status.busy":"2023-11-24T12:54:01.355725Z","iopub.execute_input":"2023-11-24T12:54:01.356091Z","iopub.status.idle":"2023-11-24T12:54:03.640440Z","shell.execute_reply.started":"2023-11-24T12:54:01.356059Z","shell.execute_reply":"2023-11-24T12:54:03.638651Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# wandb.login(key = secret_wandb)\n# run = wandb.init(\n# #     project='Fine tuning MetaMath mistral 7B - ZAIC',\n#     project='Fine tuning openchat 7B - ZAIC',\n#     job_type=\"training\", \n#     anonymous=\"allow\"\n# )","metadata":{"execution":{"iopub.status.busy":"2023-11-24T12:54:03.644804Z","iopub.execute_input":"2023-11-24T12:54:03.645335Z","iopub.status.idle":"2023-11-24T12:54:03.654406Z","shell.execute_reply.started":"2023-11-24T12:54:03.645283Z","shell.execute_reply":"2023-11-24T12:54:03.652489Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# base_model = \"meta-math/MetaMath-Mistral-7B\"\nbase_model = \"openchat/openchat_3.5\"\nnew_model = \"BK-BigAI-Math\"\nmodel_hotamath_path = \"/kaggle/working/BK-BigAI-Math\"","metadata":{"execution":{"iopub.status.busy":"2023-11-24T12:54:03.656112Z","iopub.execute_input":"2023-11-24T12:54:03.656636Z","iopub.status.idle":"2023-11-24T12:54:03.725809Z","shell.execute_reply.started":"2023-11-24T12:54:03.656528Z","shell.execute_reply":"2023-11-24T12:54:03.724615Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Download dataset","metadata":{}},{"cell_type":"code","source":"!mkdir dataset","metadata":{"execution":{"iopub.status.busy":"2023-11-24T12:54:03.727932Z","iopub.execute_input":"2023-11-24T12:54:03.728421Z","iopub.status.idle":"2023-11-24T12:54:04.850838Z","shell.execute_reply.started":"2023-11-24T12:54:03.728379Z","shell.execute_reply":"2023-11-24T12:54:04.849473Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\nhf_hub_download(repo_id=\"hotamago/ZAIC-2023\", filename=\"Elementary Maths Solving/test.zip\", revision=\"main\", repo_type=\"dataset\", local_dir=\"dataset\", local_dir_use_symlinks=False)\nhf_hub_download(repo_id=\"hotamago/ZAIC-2023\", filename=\"Elementary Maths Solving/train.zip\", revision=\"main\", repo_type=\"dataset\", local_dir=\"dataset\", local_dir_use_symlinks=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-24T12:54:04.852839Z","iopub.execute_input":"2023-11-24T12:54:04.854271Z","iopub.status.idle":"2023-11-24T12:54:06.620363Z","shell.execute_reply.started":"2023-11-24T12:54:04.854220Z","shell.execute_reply":"2023-11-24T12:54:06.619254Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading test.zip:   0%|          | 0.00/9.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dc1fb1c21794389a95058b10484af38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading train.zip:   0%|          | 0.00/93.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0899278094f4cd3b0bfa7797c9206a6"}},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'dataset/Elementary Maths Solving/train.zip'"},"metadata":{}}]},{"cell_type":"code","source":"!sudo apt-get install unzip","metadata":{"execution":{"iopub.status.busy":"2023-11-24T12:54:06.622160Z","iopub.execute_input":"2023-11-24T12:54:06.622521Z","iopub.status.idle":"2023-11-24T12:54:10.492317Z","shell.execute_reply.started":"2023-11-24T12:54:06.622490Z","shell.execute_reply":"2023-11-24T12:54:10.490787Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nunzip is already the newest version (6.0-25ubuntu1.1).\n0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n","output_type":"stream"}]},{"cell_type":"code","source":"!mkdir datasetRaw\n!unzip -q -o \"dataset/Elementary Maths Solving/test.zip\" -d \"datasetRaw\"\n!unzip -q -o \"dataset/Elementary Maths Solving/train.zip\" -d \"datasetRaw\"","metadata":{"execution":{"iopub.status.busy":"2023-11-24T12:54:10.494205Z","iopub.execute_input":"2023-11-24T12:54:10.494610Z","iopub.status.idle":"2023-11-24T12:54:13.844805Z","shell.execute_reply.started":"2023-11-24T12:54:10.494564Z","shell.execute_reply":"2023-11-24T12:54:13.843357Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Load dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport re\nimport time","metadata":{"execution":{"iopub.status.busy":"2023-11-24T12:54:13.851159Z","iopub.execute_input":"2023-11-24T12:54:13.851620Z","iopub.status.idle":"2023-11-24T12:54:13.858086Z","shell.execute_reply.started":"2023-11-24T12:54:13.851575Z","shell.execute_reply":"2023-11-24T12:54:13.856750Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_data = None\ntest_data = None\nwith open(os.path.join(\"datasetRaw\", \"train\", \"/kaggle/working/datasetRaw/math_train.json\"), \"r\") as f:\n    train_data = json.loads(f.read())['data']\nwith open(os.path.join(\"datasetRaw\", \"test\", \"/kaggle/working/datasetRaw/math_test.json\"), \"r\") as f:\n    test_data = json.loads(f.read())['data']","metadata":{"execution":{"iopub.status.busy":"2023-11-24T12:54:13.859687Z","iopub.execute_input":"2023-11-24T12:54:13.861091Z","iopub.status.idle":"2023-11-24T12:54:13.888317Z","shell.execute_reply.started":"2023-11-24T12:54:13.861048Z","shell.execute_reply":"2023-11-24T12:54:13.887032Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_data[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-24T12:54:13.889726Z","iopub.execute_input":"2023-11-24T12:54:13.890053Z","iopub.status.idle":"2023-11-24T12:54:13.898862Z","shell.execute_reply.started":"2023-11-24T12:54:13.890023Z","shell.execute_reply":"2023-11-24T12:54:13.897599Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'id': '1',\n 'question': 'Một người bán hàng bỏ ra 80,000 đồng tiền vốn và bị lỗ 6%. Để tính số tiền lỗ ta phải tính',\n 'choices': ['A. 80,000 : 6',\n  'B. 80,000 x 6',\n  'C. 80,000 : (6 x 100)',\n  'D. (80,000 x 6) : 100'],\n 'explanation': 'Theo đề bài, số tiền lỗ bằng 6% của 80 000 đồng . Để tìm số tiền lỗ ta có thể lấy 80 000 chia cho 100 rồi nhân với 6 (tức là 80 000 : 100 × 6) hoặc lấy 80000 nhân với 6 rồi chia cho 100 (tức là 80 000 × 6 : 100).',\n 'answer': 'D. (80,000 x 6) : 100'}"},"metadata":{}}]},{"cell_type":"code","source":"test_data[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-24T12:54:13.900478Z","iopub.execute_input":"2023-11-24T12:54:13.900963Z","iopub.status.idle":"2023-11-24T12:54:13.911241Z","shell.execute_reply.started":"2023-11-24T12:54:13.900931Z","shell.execute_reply":"2023-11-24T12:54:13.909837Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"{'id': '01-0203',\n 'question': 'Một cửa hàng đã bán 30% số hàng hiện có và thu được 15 000 000 đồng. Hỏi nếu bán hết hàng thì cửa hàng thu được bao nhiêu tiền?',\n 'choices': ['A. 4 500 000 đồng',\n  'B. 45 000 000 đồng',\n  'C. 50 000 000 đồng',\n  'D. 450 000 000 đồng']}"},"metadata":{}}]},{"cell_type":"code","source":"DEFAULT_PAD_TOKEN = \"[PAD]\"\nDEFAULT_EOS_TOKEN = \"<|end_of_turn|>\" # \"</s>\"\nDEFAULT_BOS_TOKEN = \"<s>\"\nDEFAULT_UNK_TOKEN = \"<unk>\"\nDEFAULT_BOI_TOKEN = \"Human:\" # \"[INST]\"\nDEFAULT_EOI_TOKEN = \"Assistant:\" # \"[/INST]\"\nPROMPT_DICT = {\n    \"prompt_input\": (\n        \"Below is an instruction that describes a task, paired with the choices, one of the choices is the correct answer to the request. \"\n        \"Write a response that appropriately completes the request.\\n\\n\"\n        \"### Instruction:\\n{instruction}\\n\\n### Choices:\\n{choices}\"\n    ),\n    \"prompt_input_run\": (\n        DEFAULT_BOI_TOKEN + \" Below is an instruction that describes a task. \"\n        \"Write a response that appropriately completes the request.\\n\\n\"\n        \"### Instruction:\\n{instruction}\\n\\n### Choices:\\n{choices}\"\n        \"\\n\" + DEFAULT_EOI_TOKEN + \" \\n\\n\"\n        \"### Explanation:\\n Let's think step by step.\\n\"\n    ),\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-24T12:54:13.912650Z","iopub.execute_input":"2023-11-24T12:54:13.913150Z","iopub.status.idle":"2023-11-24T12:54:13.922278Z","shell.execute_reply.started":"2023-11-24T12:54:13.913115Z","shell.execute_reply":"2023-11-24T12:54:13.920698Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"timeGlobal = 0\ndef startTime():\n    global timeGlobal\n    timeGlobal = time.time()\ndef getTime():\n    return (time.time() - timeGlobal)","metadata":{"execution":{"iopub.status.busy":"2023-11-24T12:54:13.924132Z","iopub.execute_input":"2023-11-24T12:54:13.924519Z","iopub.status.idle":"2023-11-24T12:54:13.935193Z","shell.execute_reply.started":"2023-11-24T12:54:13.924483Z","shell.execute_reply":"2023-11-24T12:54:13.933828Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def ApplyPromptTemplate(instruction, choices, typeP = \"prompt_input\"):\n    return PROMPT_DICT[typeP].format(instruction = instruction, choices = \"\\n\".join(choices))","metadata":{"execution":{"iopub.status.busy":"2023-11-24T12:54:13.936859Z","iopub.execute_input":"2023-11-24T12:54:13.937201Z","iopub.status.idle":"2023-11-24T12:54:13.947110Z","shell.execute_reply.started":"2023-11-24T12:54:13.937173Z","shell.execute_reply":"2023-11-24T12:54:13.946038Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Add explantion by GPT 3.5 Tubo","metadata":{}},{"cell_type":"code","source":"from openai import OpenAI\nclient = OpenAI(\n    api_key=user_secrets.get_secret(\"OPENAI_API_KEY\"),\n    organization='org-j48waUrvSOM1n0J1SLXiAr8n',\n)\n\ndef autoGPTAddExplantion(problem, answer):\n    response = client.chat.completions.create(\n      model=\"gpt-3.5-turbo\",\n      messages=[\n        {\"role\": \"system\", \"content\": \"Explan by Vietnamese step-by-step for given answer to given problem.\\nRule:\\n- No markdown format\\n- Given answer always true for given problem\\n- No title\\n- Short explantion\\n\"},\n        {\"role\": \"user\", \"content\": f\"### Problem:\\n{problem}\\n\\n### Answer:\\n{answer}\"},\n        {\"role\": \"system\", \"content\": \"Giải thích: \"},\n      ],\n#       max_tokens=512,\n      temperature=0,\n      top_p=1.0,\n#       top_k=50,\n    )\n    return response.choices[0].message.content","metadata":{"execution":{"iopub.status.busy":"2023-11-24T12:54:13.948759Z","iopub.execute_input":"2023-11-24T12:54:13.949355Z","iopub.status.idle":"2023-11-24T12:54:15.470351Z","shell.execute_reply.started":"2023-11-24T12:54:13.949316Z","shell.execute_reply":"2023-11-24T12:54:15.469355Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import time","metadata":{"execution":{"iopub.status.busy":"2023-11-24T12:54:15.471783Z","iopub.execute_input":"2023-11-24T12:54:15.472426Z","iopub.status.idle":"2023-11-24T12:54:15.477116Z","shell.execute_reply.started":"2023-11-24T12:54:15.472386Z","shell.execute_reply":"2023-11-24T12:54:15.475849Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"startTime()\ncntSt = 0\ni = 0\ntimeWait = 20\nwhile i < len(train_data):\n    singleData = train_data[i]\n    if \"explanation\" in singleData.keys():\n        i += 1\n        continue\n    print(f\"Runing testcase ({i})\")\n    \n    try:\n        res = autoGPTAddExplantion(singleData['question'], singleData['answer'])\n    except Exception as inst:\n        print(type(inst))\n        print(inst)\n        \n        time.sleep(timeWait)\n        timeWait *= 2\n        if timeWait > 80:\n            break\n        print\n        continue\n    \n    timeWait = 20\n        \n    cntSt += 1\n    train_data[i]['explanation'] = res\n    print(f\"Done testcase ({i})\")\n    #     print(train_data[i])\n    if cntSt%3 == 0:\n        deltaTime = getTime()\n        if (60 - deltaTime) > -1:\n            time.sleep(60 - deltaTime + 2)\n        startTime()\n        cntSt = 0\n    \n    i += 1","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-24T12:54:15.479128Z","iopub.execute_input":"2023-11-24T12:54:15.479484Z","iopub.status.idle":"2023-11-24T13:53:54.194754Z","shell.execute_reply.started":"2023-11-24T12:54:15.479449Z","shell.execute_reply":"2023-11-24T13:53:54.193035Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Runing testcase (512)\nDone testcase (512)\nRuning testcase (514)\nDone testcase (514)\nRuning testcase (518)\nDone testcase (518)\nRuning testcase (520)\nDone testcase (520)\nRuning testcase (522)\nDone testcase (522)\nRuning testcase (523)\nDone testcase (523)\nRuning testcase (524)\nDone testcase (524)\nRuning testcase (525)\nDone testcase (525)\nRuning testcase (527)\nDone testcase (527)\nRuning testcase (529)\nDone testcase (529)\nRuning testcase (530)\nDone testcase (530)\nRuning testcase (531)\nDone testcase (531)\nRuning testcase (532)\nDone testcase (532)\nRuning testcase (533)\nDone testcase (533)\nRuning testcase (535)\nDone testcase (535)\nRuning testcase (536)\nDone testcase (536)\nRuning testcase (537)\nDone testcase (537)\nRuning testcase (538)\nDone testcase (538)\nRuning testcase (539)\nDone testcase (539)\nRuning testcase (540)\nDone testcase (540)\nRuning testcase (541)\nDone testcase (541)\nRuning testcase (542)\nDone testcase (542)\nRuning testcase (543)\nDone testcase (543)\nRuning testcase (545)\nDone testcase (545)\nRuning testcase (546)\nDone testcase (546)\nRuning testcase (547)\nDone testcase (547)\nRuning testcase (549)\nDone testcase (549)\nRuning testcase (550)\nDone testcase (550)\nRuning testcase (552)\nDone testcase (552)\nRuning testcase (553)\nDone testcase (553)\nRuning testcase (554)\nDone testcase (554)\nRuning testcase (555)\nDone testcase (555)\nRuning testcase (556)\nDone testcase (556)\nRuning testcase (557)\nDone testcase (557)\nRuning testcase (558)\nDone testcase (558)\nRuning testcase (559)\nDone testcase (559)\nRuning testcase (560)\nDone testcase (560)\nRuning testcase (561)\nDone testcase (561)\nRuning testcase (562)\nDone testcase (562)\nRuning testcase (563)\nDone testcase (563)\nRuning testcase (568)\nDone testcase (568)\nRuning testcase (569)\nDone testcase (569)\nRuning testcase (570)\nDone testcase (570)\nRuning testcase (571)\nDone testcase (571)\nRuning testcase (572)\nDone testcase (572)\nRuning testcase (573)\nDone testcase (573)\nRuning testcase (574)\nDone testcase (574)\nRuning testcase (575)\nDone testcase (575)\nRuning testcase (577)\nDone testcase (577)\nRuning testcase (578)\nDone testcase (578)\nRuning testcase (579)\nDone testcase (579)\nRuning testcase (580)\nDone testcase (580)\nRuning testcase (581)\nDone testcase (581)\nRuning testcase (582)\nDone testcase (582)\nRuning testcase (583)\nDone testcase (583)\nRuning testcase (584)\nDone testcase (584)\nRuning testcase (585)\nDone testcase (585)\nRuning testcase (586)\nDone testcase (586)\nRuning testcase (587)\nDone testcase (587)\nRuning testcase (589)\nDone testcase (589)\nRuning testcase (590)\nDone testcase (590)\nRuning testcase (591)\nDone testcase (591)\nRuning testcase (592)\nDone testcase (592)\nRuning testcase (594)\nDone testcase (594)\nRuning testcase (597)\nDone testcase (597)\nRuning testcase (598)\nDone testcase (598)\nRuning testcase (600)\nDone testcase (600)\nRuning testcase (602)\nDone testcase (602)\nRuning testcase (603)\nDone testcase (603)\nRuning testcase (604)\nDone testcase (604)\nRuning testcase (605)\nDone testcase (605)\nRuning testcase (606)\nDone testcase (606)\nRuning testcase (607)\nDone testcase (607)\nRuning testcase (608)\nDone testcase (608)\nRuning testcase (610)\nDone testcase (610)\nRuning testcase (611)\nDone testcase (611)\nRuning testcase (612)\nDone testcase (612)\nRuning testcase (614)\nDone testcase (614)\nRuning testcase (617)\nDone testcase (617)\nRuning testcase (619)\nDone testcase (619)\nRuning testcase (620)\nDone testcase (620)\nRuning testcase (621)\nDone testcase (621)\nRuning testcase (622)\nDone testcase (622)\nRuning testcase (623)\nDone testcase (623)\nRuning testcase (624)\nDone testcase (624)\nRuning testcase (625)\nDone testcase (625)\nRuning testcase (626)\nDone testcase (626)\nRuning testcase (627)\nDone testcase (627)\nRuning testcase (628)\nDone testcase (628)\nRuning testcase (629)\nDone testcase (629)\nRuning testcase (630)\nDone testcase (630)\nRuning testcase (631)\nDone testcase (631)\nRuning testcase (632)\nDone testcase (632)\nRuning testcase (633)\nDone testcase (633)\nRuning testcase (635)\nDone testcase (635)\nRuning testcase (636)\nDone testcase (636)\nRuning testcase (637)\nDone testcase (637)\nRuning testcase (638)\nDone testcase (638)\nRuning testcase (639)\nDone testcase (639)\nRuning testcase (640)\nDone testcase (640)\nRuning testcase (641)\nDone testcase (641)\nRuning testcase (643)\nDone testcase (643)\nRuning testcase (644)\nDone testcase (644)\nRuning testcase (645)\nDone testcase (645)\nRuning testcase (646)\nDone testcase (646)\nRuning testcase (647)\nDone testcase (647)\nRuning testcase (648)\nDone testcase (648)\nRuning testcase (649)\nDone testcase (649)\nRuning testcase (651)\nDone testcase (651)\nRuning testcase (652)\nDone testcase (652)\nRuning testcase (654)\nDone testcase (654)\nRuning testcase (655)\nDone testcase (655)\nRuning testcase (656)\nDone testcase (656)\nRuning testcase (660)\nDone testcase (660)\nRuning testcase (661)\nDone testcase (661)\nRuning testcase (662)\nDone testcase (662)\nRuning testcase (663)\nDone testcase (663)\nRuning testcase (665)\nDone testcase (665)\nRuning testcase (666)\nDone testcase (666)\nRuning testcase (668)\nDone testcase (668)\nRuning testcase (669)\nDone testcase (669)\nRuning testcase (670)\nDone testcase (670)\nRuning testcase (671)\nDone testcase (671)\nRuning testcase (672)\nDone testcase (672)\nRuning testcase (673)\nDone testcase (673)\nRuning testcase (677)\nDone testcase (677)\nRuning testcase (678)\nDone testcase (678)\nRuning testcase (679)\nDone testcase (679)\nRuning testcase (680)\nDone testcase (680)\nRuning testcase (681)\nDone testcase (681)\nRuning testcase (683)\nDone testcase (683)\nRuning testcase (684)\nDone testcase (684)\nRuning testcase (685)\nDone testcase (685)\nRuning testcase (691)\nDone testcase (691)\nRuning testcase (693)\nDone testcase (693)\nRuning testcase (695)\nDone testcase (695)\nRuning testcase (697)\nDone testcase (697)\nRuning testcase (699)\nDone testcase (699)\nRuning testcase (700)\nDone testcase (700)\nRuning testcase (701)\nDone testcase (701)\nRuning testcase (702)\nDone testcase (702)\nRuning testcase (703)\nDone testcase (703)\nRuning testcase (704)\nDone testcase (704)\nRuning testcase (705)\nDone testcase (705)\nRuning testcase (706)\nDone testcase (706)\nRuning testcase (708)\nDone testcase (708)\nRuning testcase (710)\nDone testcase (710)\nRuning testcase (713)\nDone testcase (713)\nRuning testcase (715)\nDone testcase (715)\nRuning testcase (716)\nDone testcase (716)\nRuning testcase (717)\nDone testcase (717)\nRuning testcase (718)\nDone testcase (718)\nRuning testcase (719)\nDone testcase (719)\nRuning testcase (720)\nDone testcase (720)\nRuning testcase (721)\nDone testcase (721)\nRuning testcase (722)\nDone testcase (722)\nRuning testcase (723)\nDone testcase (723)\nRuning testcase (724)\n<class 'openai.RateLimitError'>\nError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-j48waUrvSOM1n0J1SLXiAr8n on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\nRuning testcase (724)\n<class 'openai.RateLimitError'>\nError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-j48waUrvSOM1n0J1SLXiAr8n on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\nRuning testcase (724)\n<class 'openai.RateLimitError'>\nError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-j48waUrvSOM1n0J1SLXiAr8n on requests per day (RPD): Limit 200, Used 200, Requested 1. Please try again in 7m12s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n","output_type":"stream"}]},{"cell_type":"code","source":"with open(\"/kaggle/working/datasetRaw/math_train.json\", \"w\", encoding='utf-8') as f:\n    json.dump({\n        \"__count__\": len(train_data),\n        \"data\": train_data\n    }, f)","metadata":{"execution":{"iopub.status.busy":"2023-11-24T14:00:16.829060Z","iopub.execute_input":"2023-11-24T14:00:16.829760Z","iopub.status.idle":"2023-11-24T14:00:16.871101Z","shell.execute_reply.started":"2023-11-24T14:00:16.829715Z","shell.execute_reply":"2023-11-24T14:00:16.869880Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"### Add data by education program","metadata":{}},{"cell_type":"markdown","source":"Todo: Create data generation algorithms for each chapter.\n\nBelow is class base, change \\_call function to a function that create radom question for that chapter","metadata":{}},{"cell_type":"code","source":"class BaseGenChapter:\n    def __init__(*args, **kwargs):\n        pass\n    \n    def _call(self, *args, **kwargs) -> str:\n        raise NotImplementedError(\"_call not Implement yet\")\n    \n    def call(self, *args, **kwargs) -> str:\n        return self._call(*args, **kwargs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocess data","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_train_dataset = len(train_data)\nvalition_radio = 0.05\ntokenized_train_dataset_raw = train_data[:-int(num_train_dataset*valition_radio)]\ntokenized_val_dataset_raw = train_data[-int(num_train_dataset*valition_radio):]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datasetStruct = {\"input\":[], \"output\":[]}\ndataset = {\"text\":[]}\nnum_train_dataset = len(tokenized_train_dataset_raw)\nfor i in range(num_train_dataset):\n    ttdro = tokenized_train_dataset_raw[i]\n    \n    if \"explanation\" not in ttdro.keys():\n        continue\n    \n    input_content = \"{0} {1}\".format(\n        DEFAULT_BOI_TOKEN,\n        ApplyPromptTemplate(ttdro['question'], ttdro['choices']),\n    )\n    datasetStruct[\"input\"].append(input_content)\n    \n    if \"explanation\" not in ttdro.keys():\n        output_content = \"\\n{0} \\n\\n{1}\\n\\n{2} {3}\".format(\n            DEFAULT_EOI_TOKEN,\n            \"### Explanation:\\nNo explanation{0}\",\n            \"### Answer:\\n{0}\".format(ttdro['answer']),\n            DEFAULT_EOS_TOKEN,\n        )\n    else:\n        output_content = \"\\n{0} \\n\\n{1}\\n\\n{2} {3}\".format(\n            DEFAULT_EOI_TOKEN,\n            \"### Explanation:\\n{0}\".format(ttdro['explanation']),\n            \"### Answer:\\n{0}\".format(ttdro['answer']),\n            DEFAULT_EOS_TOKEN,\n        )\n    datasetStruct[\"output\"].append(output_content)\n    \n    dataset[\"text\"].append(input_content + output_content)\n    \ndataset = Dataset.from_dict(dataset)\n#     <s>[INST][/INST] </s>","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dataset[\"text\"][32])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(dataset[\"text\"]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train model","metadata":{}},{"cell_type":"markdown","source":"### Load model","metadata":{}},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(  \n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.bfloat16,\n    bnb_4bit_use_double_quant= False,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n#         model_hotamath_path,\n    load_in_4bit=True,\n#     load_in_8bit= True,\n    quantization_config=bnb_config,\n    torch_dtype=torch.bfloat16,\n#         torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    base_model,\n    model_max_length=512,\n    padding_side=\"right\",\n    use_fast=False,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"special_tokens_dict = {\n    'additional_special_tokens': [DEFAULT_BOI_TOKEN, DEFAULT_EOI_TOKEN],\n    'pad_token': DEFAULT_PAD_TOKEN,\n    'bos_token': DEFAULT_BOS_TOKEN,\n    'eos_token': DEFAULT_EOS_TOKEN,\n    'unk_token': DEFAULT_UNK_TOKEN,\n}\nnum_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.encode(\"{0} Hello, how are you? \\n{1} I'm fine, thank you!{2}\".format(\n    DEFAULT_BOI_TOKEN,\n    DEFAULT_EOI_TOKEN,\n    DEFAULT_EOS_TOKEN,\n)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DataCollator","metadata":{}},{"cell_type":"code","source":"from trl import DataCollatorForCompletionOnlyLM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"instruction_template = DEFAULT_BOI_TOKEN\nresponse_template = DEFAULT_EOI_TOKEN\ncollator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer, mlm=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setup model for train","metadata":{}},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=100,\n    logging_steps=10,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    save_total_limit = 3,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    report_to=\"wandb\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model.to(\"cuda:0\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_trainable_parameters(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n#     model = base_model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    max_seq_length=512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n    data_collator=collator,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.config.use_cache = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train and save","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.save_pretrained(new_model)\ntrainer.model.save_pretrained(new_model)\nmodel.config.to_json_file(os.path.join(new_model, \"config.json\"))\ntokenizer.save_pretrained(new_model)\nwandb.finish()\nmodel.config.use_cache = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r BK-BigAI-Math.zip BK-BigAI-Math","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'BK-BigAI-Math.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evalution","metadata":{}},{"cell_type":"markdown","source":"### Download model","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\nhf_hub_download(repo_id=\"hotamago/ZAIC-2023-Model\", filename=\"Hota-Math.zip\", repo_type=\"model\", local_dir=\"/kaggle/working/\", local_dir_use_symlinks=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip -q -o Hota-Math.zip -d ./","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load model","metadata":{}},{"cell_type":"code","source":"model_name_or_path = \"/kaggle/working/BK-BigAI-Math\"\n# bnb_config = BitsAndBytesConfig(  \n#     load_in_4bit= True,\n#     bnb_4bit_quant_type= \"nf4\",\n#     bnb_4bit_compute_dtype= torch.bfloat16,\n#     bnb_4bit_use_double_quant= False,\n# )\n# model = AutoModelForCausalLM.from_pretrained(\n#         model_name_or_path,\n#         load_in_4bit=True,\n#         quantization_config=bnb_config,\n#         torch_dtype=torch.bfloat16,\n#         device_map=\"auto\",\n#         trust_remote_code=True,\n# )\n\n# model = AutoModelForCausalLM.from_pretrained(\n#         model_name_or_path,\n#         torch_dtype=torch.bfloat16,\n#         device_map=\"auto\",\n#         trust_remote_code=True,\n# )\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name_or_path,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    load_in_4bit=True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    model_name_or_path,\n    model_max_length=512,\n    padding_side=\"right\",\n    use_fast=False,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# special_tokens_dict = {'additional_special_tokens': ['[INST]','[/INST]']}\n# num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setup pipeline with auto answer get","metadata":{}},{"cell_type":"code","source":"pipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n    temperature=0.01,\n    top_p=1,\n    top_k=50,\n    repetition_penalty=1.1,\n    pad_token_id=tokenizer.eos_token_id\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n# globalRegxCompire = \"0-9a-zA-Z\\.\\:\\-\\^\\! \"\ndef niceValueToCompire(x):\n#     x = re.sub(\"[^{0}]\".format(globalRegxCompire), \"\", x)\n    x = re.sub(\"[ \\t\\n]\", \"\", x)\n    return x\ndef autoLLMFormat(question, choises = None, debug=False):\n    prompt_template = ApplyPromptTemplate(question, choises, \"prompt_input_run\")\n    res = pipe(prompt_template)[0]['generated_text']\n    if debug:\n        print(res)\n    x = re.findall(\"### Answer:[\\n ](.+)\", res)\n    \n    if choises == None:\n        return x\n    \n    choises_compare = [niceValueToCompire(choise_pred) for choise_pred in choises]\n\n    if len(x) == 0:\n        return choises[random.randrange(0, len(choises))]\n    \n    x = niceValueToCompire(x[0])\n    \n    if (x not in choises_compare):\n        return choises[random.randrange(0, len(choises))]\n    \n    for i in range(len(choises_compare)):\n        if x == choises_compare[i]:\n            return choises[i]\n    \n    return \"wtf\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Run evalution","metadata":{}},{"cell_type":"code","source":"count_proc_testcase = 0\ncount_pass_testcase = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"while count_proc_testcase < len(tokenized_val_dataset_raw):\n    tvdo = tokenized_val_dataset_raw[count_proc_testcase]\n    startTime()\n    answer = autoLLMFormat(tvdo['question'], tvdo['choices'], True)\n    deltaTime = getTime()\n\n    if answer == tvdo['answer']:\n        count_pass_testcase += 1\n    \n    count_proc_testcase += 1\n    print(\"Testcase {0}, time: {1}, answer: {2} | {3}, Passed: {4}, IsPass: {5}\".format(\n        count_proc_testcase,\n        deltaTime,\n        answer,\n        tvdo['answer'],\n        count_pass_testcase,\n        (answer == tvdo['answer'])\n    ))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run public test dataset","metadata":{}},{"cell_type":"code","source":"result_test = []\nif os.path.exists(os.path.join(\"result\", \"result.txt\")):\n    with open(os.path.join(\"result\", \"result.txt\"), \"r\") as f:\n        result_test = f.read().split(\"\\n\")\ncount_id = len(result_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(test_data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"while count_id < len(test_data):\n    one_test_data = test_data[count_id]\n    startTime()\n    answer = autoLLMFormat(one_test_data['question'], one_test_data['choices'], False)\n    deltaTime = getTime()\n    result_test.append(\"{0}\".format(answer))\n    count_id += 1\n    if count_id%10 == 0:\n        with open(os.path.join(\"result\", \"result.txt\"), \"w\", encoding='utf-8') as f:\n            f.write(\"\\n\".join(result_test))\n    print(\"Testcase {0}, time: {1}, answer: {2}\".format(count_id, deltaTime, answer))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(os.path.join(\"result\", \"result.txt\"), \"w\", encoding='utf-8') as f:\n    f.write(\"\\n\".join(result_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n\".join(result_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}