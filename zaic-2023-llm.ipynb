{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install","metadata":{}},{"cell_type":"code","source":"!pip3 install huggingface-hub","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade openai\n!pip install --upgrade pydantic","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U peft\n%pip install -U accelerate\n%pip install -U trl","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !mkdir MetaMath-Mistral-7B\n# !huggingface-cli download meta-math/MetaMath-Mistral-7B --local-dir MetaMath-Mistral-7B --local-dir-use-symlinks False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import library","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nimport os,torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:23:27.825595Z","iopub.execute_input":"2023-11-26T04:23:27.826332Z","iopub.status.idle":"2023-11-26T04:23:38.560789Z","shell.execute_reply.started":"2023-11-26T04:23:27.826297Z","shell.execute_reply":"2023-11-26T04:23:38.559762Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nsecret_wandb = user_secrets.get_secret(\"wandb\")","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:23:38.562830Z","iopub.execute_input":"2023-11-26T04:23:38.564024Z","iopub.status.idle":"2023-11-26T04:23:39.088391Z","shell.execute_reply.started":"2023-11-26T04:23:38.563985Z","shell.execute_reply":"2023-11-26T04:23:39.087589Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login --token $secret_hf","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:23:39.089614Z","iopub.execute_input":"2023-11-26T04:23:39.089935Z","iopub.status.idle":"2023-11-26T04:23:40.592849Z","shell.execute_reply.started":"2023-11-26T04:23:39.089906Z","shell.execute_reply":"2023-11-26T04:23:40.591322Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# wandb.login(key = secret_wandb)\n# run = wandb.init(\n# #     project='Fine tuning MetaMath mistral 7B - ZAIC',\n#     project='Fine tuning openchat 7B - ZAIC',\n#     job_type=\"training\", \n#     anonymous=\"allow\"\n# )","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:23:40.595768Z","iopub.execute_input":"2023-11-26T04:23:40.596149Z","iopub.status.idle":"2023-11-26T04:23:40.601127Z","shell.execute_reply.started":"2023-11-26T04:23:40.596111Z","shell.execute_reply":"2023-11-26T04:23:40.600127Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# base_model = \"meta-math/MetaMath-Mistral-7B\"\nbase_model = \"openchat/openchat_3.5\"\nnew_model = \"BK-BigAI-Math\"\nmodel_hotamath_path = \"/kaggle/working/BK-BigAI-Math\"","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:23:40.602438Z","iopub.execute_input":"2023-11-26T04:23:40.602729Z","iopub.status.idle":"2023-11-26T04:23:40.613883Z","shell.execute_reply.started":"2023-11-26T04:23:40.602703Z","shell.execute_reply":"2023-11-26T04:23:40.613006Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Download dataset","metadata":{}},{"cell_type":"code","source":"!mkdir dataset","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:23:40.615222Z","iopub.execute_input":"2023-11-26T04:23:40.616083Z","iopub.status.idle":"2023-11-26T04:23:41.627296Z","shell.execute_reply.started":"2023-11-26T04:23:40.616056Z","shell.execute_reply":"2023-11-26T04:23:41.626059Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"mkdir: cannot create directory ‘dataset’: File exists\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\nhf_hub_download(repo_id=\"hotamago/ZAIC-2023\", filename=\"Elementary Maths Solving/test.zip\", revision=\"main\", repo_type=\"dataset\", local_dir=\"dataset\", local_dir_use_symlinks=False)\nhf_hub_download(repo_id=\"hotamago/ZAIC-2023\", filename=\"Elementary Maths Solving/train.zip\", revision=\"main\", repo_type=\"dataset\", local_dir=\"dataset\", local_dir_use_symlinks=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:23:41.628901Z","iopub.execute_input":"2023-11-26T04:23:41.629241Z","iopub.status.idle":"2023-11-26T04:23:42.317111Z","shell.execute_reply.started":"2023-11-26T04:23:41.629209Z","shell.execute_reply":"2023-11-26T04:23:42.316076Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading test.zip:   0%|          | 0.00/9.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1c77b405f354fc0b3706ee80ee80286"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading train.zip:   0%|          | 0.00/129k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eaca033cda044709d2dcc858e7eba2b"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'dataset/Elementary Maths Solving/train.zip'"},"metadata":{}}]},{"cell_type":"code","source":"!sudo apt-get install unzip","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:23:42.318348Z","iopub.execute_input":"2023-11-26T04:23:42.318647Z","iopub.status.idle":"2023-11-26T04:23:45.097095Z","shell.execute_reply.started":"2023-11-26T04:23:42.318621Z","shell.execute_reply":"2023-11-26T04:23:45.095787Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nunzip is already the newest version (6.0-26ubuntu3.1).\n0 upgraded, 0 newly installed, 0 to remove and 46 not upgraded.\n","output_type":"stream"}]},{"cell_type":"code","source":"!mkdir datasetRaw\n!unzip -q -o \"dataset/Elementary Maths Solving/test.zip\" -d \"datasetRaw\"\n!unzip -q -o \"dataset/Elementary Maths Solving/train.zip\" -d \"datasetRaw\"","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:23:45.098497Z","iopub.execute_input":"2023-11-26T04:23:45.098789Z","iopub.status.idle":"2023-11-26T04:23:48.034122Z","shell.execute_reply.started":"2023-11-26T04:23:45.098760Z","shell.execute_reply":"2023-11-26T04:23:48.032964Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"mkdir: cannot create directory ‘datasetRaw’: File exists\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport re\nimport time","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:23:48.039015Z","iopub.execute_input":"2023-11-26T04:23:48.039786Z","iopub.status.idle":"2023-11-26T04:23:48.044296Z","shell.execute_reply.started":"2023-11-26T04:23:48.039749Z","shell.execute_reply":"2023-11-26T04:23:48.043320Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_data = None\ntest_data = None\nwith open(os.path.join(\"datasetRaw\", \"train\", \"/kaggle/working/datasetRaw/math_train.json\"), \"r\") as f:\n    train_data = json.loads(f.read())['data']\nwith open(os.path.join(\"datasetRaw\", \"test\", \"/kaggle/working/datasetRaw/math_test.json\"), \"r\") as f:\n    test_data = json.loads(f.read())['data']","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:23:48.045469Z","iopub.execute_input":"2023-11-26T04:23:48.045788Z","iopub.status.idle":"2023-11-26T04:23:48.070194Z","shell.execute_reply.started":"2023-11-26T04:23:48.045756Z","shell.execute_reply":"2023-11-26T04:23:48.069280Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_data[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:23:48.071311Z","iopub.execute_input":"2023-11-26T04:23:48.071552Z","iopub.status.idle":"2023-11-26T04:23:48.078101Z","shell.execute_reply.started":"2023-11-26T04:23:48.071531Z","shell.execute_reply":"2023-11-26T04:23:48.077193Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'id': '1',\n 'question': 'Một người bán hàng bỏ ra 80,000 đồng tiền vốn và bị lỗ 6%. Để tính số tiền lỗ ta phải tính',\n 'choices': ['A. 80,000 : 6',\n  'B. 80,000 x 6',\n  'C. 80,000 : (6 x 100)',\n  'D. (80,000 x 6) : 100'],\n 'explanation': 'Theo đề bài, số tiền lỗ bằng 6% của 80 000 đồng . Để tìm số tiền lỗ ta có thể lấy 80 000 chia cho 100 rồi nhân với 6 (tức là 80 000 : 100 × 6) hoặc lấy 80000 nhân với 6 rồi chia cho 100 (tức là 80 000 × 6 : 100).',\n 'answer': 'D. (80,000 x 6) : 100'}"},"metadata":{}}]},{"cell_type":"code","source":"test_data[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:23:48.079361Z","iopub.execute_input":"2023-11-26T04:23:48.079689Z","iopub.status.idle":"2023-11-26T04:23:48.089437Z","shell.execute_reply.started":"2023-11-26T04:23:48.079654Z","shell.execute_reply":"2023-11-26T04:23:48.088659Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'id': '01-0203',\n 'question': 'Một cửa hàng đã bán 30% số hàng hiện có và thu được 15 000 000 đồng. Hỏi nếu bán hết hàng thì cửa hàng thu được bao nhiêu tiền?',\n 'choices': ['A. 4 500 000 đồng',\n  'B. 45 000 000 đồng',\n  'C. 50 000 000 đồng',\n  'D. 450 000 000 đồng']}"},"metadata":{}}]},{"cell_type":"code","source":"DEFAULT_PAD_TOKEN = \"[PAD]\"\nDEFAULT_EOS_TOKEN = \"<|end_of_turn|>\" # \"</s>\"\nDEFAULT_BOS_TOKEN = \"<s>\"\nDEFAULT_UNK_TOKEN = \"<unk>\"\nDEFAULT_BOI_TOKEN = \"Human:\" # \"[INST]\"\nDEFAULT_EOI_TOKEN = \"Assistant:\" # \"[/INST]\"\nPROMPT_DICT = {\n    \"prompt_input\": (\n        \"Below is an instruction that describes a task, paired with the choices, one of the choices is the correct answer to the request. \"\n        \"Write a response that appropriately completes the request.\\n\\n\"\n        \"### Instruction:\\n{instruction}\\n\\n### Choices:\\n{choices}\"\n    ),\n    \"prompt_input_run\": (\n        DEFAULT_BOI_TOKEN + \" Below is an instruction that describes a task, paired with the choices, one of the choices is the correct answer to the request.  \"\n        \"Write a response that appropriately completes the request.\\n\\n\"\n        \"### Instruction:\\n{instruction}\\n\\n### Choices:\\n{choices}\"\n        \"\\n\" + DEFAULT_EOI_TOKEN + \" \\n\\n\"\n#         \"### Explanation:\\n Let's think step by step.\\n\"\n        \"### Explanation:\\n Hãy suy nghĩ từng bước một.\\n\"\n    ),\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:23:48.090799Z","iopub.execute_input":"2023-11-26T04:23:48.091437Z","iopub.status.idle":"2023-11-26T04:23:48.100178Z","shell.execute_reply.started":"2023-11-26T04:23:48.091410Z","shell.execute_reply":"2023-11-26T04:23:48.099285Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"timeGlobal = 0\ndef startTime():\n    global timeGlobal\n    timeGlobal = time.time()\ndef getTime():\n    return (time.time() - timeGlobal)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:23:48.101202Z","iopub.execute_input":"2023-11-26T04:23:48.102206Z","iopub.status.idle":"2023-11-26T04:23:48.116234Z","shell.execute_reply.started":"2023-11-26T04:23:48.102170Z","shell.execute_reply":"2023-11-26T04:23:48.115254Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def ApplyPromptTemplate(instruction, choices, typeP = \"prompt_input\"):\n    return PROMPT_DICT[typeP].format(instruction = instruction, choices = \"\\n\".join(choices))","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:23:48.117515Z","iopub.execute_input":"2023-11-26T04:23:48.118097Z","iopub.status.idle":"2023-11-26T04:23:48.127290Z","shell.execute_reply.started":"2023-11-26T04:23:48.118062Z","shell.execute_reply":"2023-11-26T04:23:48.126453Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Add explantion by GPT 3.5 Tubo","metadata":{}},{"cell_type":"code","source":"from openai import OpenAI\nclient = OpenAI(\n    api_key=user_secrets.get_secret(\"OPENAI_API_KEY\"),\n#     organization='org-j48waUrvSOM1n0J1SLXiAr8n',\n)\n\ndef autoGPTAddExplantion(problem, answer):\n    response = client.chat.completions.create(\n      model=\"gpt-3.5-turbo\",\n      messages=[\n        {\"role\": \"system\", \"content\": \"Explan by Vietnamese step-by-step for given answer to given problem.\\nRule:\\n- No markdown format\\n- Given answer always true for given problem\\n- No title\\n- Short explantion\\n\\n\"},\n        {\"role\": \"user\", \"content\": f\"### Problem:\\n{problem}\\n\\n### Answer:\\n{answer}\"},\n        {\"role\": \"system\", \"content\": \"Giải thích: \"},\n      ],\n#       max_tokens=512,\n      temperature=0,\n      top_p=1.0,\n#       top_k=50,\n    )\n    return response.choices[0].message.content","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"startTime()\ncntSt = 0\ni = 0\ntimeWait = 20\nwhile i < len(train_data):\n    singleData = train_data[i]\n    if \"explanation\" in singleData.keys():\n        i += 1\n        continue\n    print(f\"Runing testcase ({i})\")\n    \n    try:\n        res = autoGPTAddExplantion(singleData['question'], singleData['answer'])\n    except Exception as inst:\n        print(type(inst))\n        print(inst)\n        \n        time.sleep(timeWait)\n        timeWait *= 2\n        if timeWait > 80:\n            break\n        print\n        continue\n    \n    timeWait = 20\n        \n    cntSt += 1\n    train_data[i]['explanation'] = res\n    print(f\"Done testcase ({i})\")\n    #     print(train_data[i])\n    if cntSt%3 == 0:\n        deltaTime = getTime()\n        if (60 - deltaTime) > -1:\n            time.sleep(60 - deltaTime + 2)\n        startTime()\n        cntSt = 0\n    \n    i += 1\nprint(\"Done all testcase\")","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add explantion by Human","metadata":{}},{"cell_type":"code","source":"def autoGPTAddExplantion(problem, answer):\n    return (\n        \"Explan by Vietnamese step-by-step for given answer to given problem.\\n\"\n        \"Rule:\\n\"\n        \"- No markdown format\\n\"\n        \"- Given answer always true for given problem\\n\"\n        \"- No title\\n\"\n        \"- Short explantion by using math formula\\n\"\n        \"- Eplan few words as possible\"\n        \"\\n\\n\"\n        f\"### Problem:\\n{problem}\\n\\n### Answer:\\n{answer}\\n\\n\"\n        \"### Giải thích: \"\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 0\nwhile i < len(train_data):\n    singleData = train_data[i]\n    if \"explanation\" in singleData.keys():\n        i += 1\n        continue\n    print(f\"Runing testcase ({i})\")\n    \n    auto_prompt = autoGPTAddExplantion(singleData['question'], singleData['answer'])\n    print(auto_prompt)\n    res = input()\n    \n    train_data[i]['explanation'] = res\n    print(train_data[i])\n    print(f\"Done testcase ({i})\")\n    \n    i += 1\nprint(\"Done all testcase\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save json","metadata":{}},{"cell_type":"code","source":"with open(\"/kaggle/working/datasetRaw/math_train.json\", \"w\", encoding='utf-8') as f:\n    json.dump({\n        \"__count__\": len(train_data),\n        \"data\": train_data\n    }, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add data by education program","metadata":{}},{"cell_type":"markdown","source":"Todo: Create data generation algorithms for each chapter.\n\nBelow is class base, change \\_call function to a function that create radom question for that chapter","metadata":{}},{"cell_type":"code","source":"class BaseGenChapter:\n    def __init__(*args, **kwargs):\n        pass\n    \n    def _call(self, *args, **kwargs) -> str:\n        raise NotImplementedError(\"_call not Implement yet\")\n    \n    def call(self, *args, **kwargs) -> str:\n        return self._call(*args, **kwargs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocess data","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\nimport random","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:23:48.128308Z","iopub.execute_input":"2023-11-26T04:23:48.128577Z","iopub.status.idle":"2023-11-26T04:23:48.138094Z","shell.execute_reply.started":"2023-11-26T04:23:48.128553Z","shell.execute_reply":"2023-11-26T04:23:48.137217Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# random.shuffle(train_data)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:23:48.139162Z","iopub.execute_input":"2023-11-26T04:23:48.139409Z","iopub.status.idle":"2023-11-26T04:23:48.149351Z","shell.execute_reply.started":"2023-11-26T04:23:48.139388Z","shell.execute_reply":"2023-11-26T04:23:48.148419Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"num_train_dataset = len(train_data)\n# valition_radio = 0.1\n# tokenized_train_dataset_raw = train_data[:-int(num_train_dataset*valition_radio)]\n# tokenized_val_dataset_raw = train_data[-int(num_train_dataset*valition_radio):]\ntokenized_train_dataset_raw = train_data\ntokenized_val_dataset_raw = random.choices(train_data, k=100)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:23:48.151604Z","iopub.execute_input":"2023-11-26T04:23:48.152374Z","iopub.status.idle":"2023-11-26T04:23:48.159735Z","shell.execute_reply.started":"2023-11-26T04:23:48.152320Z","shell.execute_reply":"2023-11-26T04:23:48.158925Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"datasetStruct = {\"input\":[], \"output\":[]}\ndataset = {\"text\":[]}\nnum_train_dataset = len(tokenized_train_dataset_raw)\nfor i in range(num_train_dataset):\n    ttdro = tokenized_train_dataset_raw[i]\n    \n    if \"explanation\" not in ttdro.keys():\n        continue\n    \n    input_content = \"{0} {1}\".format(\n        DEFAULT_BOI_TOKEN,\n        ApplyPromptTemplate(ttdro['question'], ttdro['choices']),\n    )\n    datasetStruct[\"input\"].append(input_content)\n    \n    if \"explanation\" not in ttdro.keys():\n        output_content = \"\\n{0} \\n\\n{1}\\n\\n{2} {3}\".format(\n            DEFAULT_EOI_TOKEN,\n            \"### Explanation:\\nNo explanation{0}\",\n            \"### Answer:\\n{0}\".format(ttdro['answer']),\n            DEFAULT_EOS_TOKEN,\n        )\n    else:\n        output_content = \"\\n{0} \\n\\n{1}\\n\\n{2} {3}\".format(\n            DEFAULT_EOI_TOKEN,\n            \"### Explanation:\\n{0}\".format(ttdro['explanation']),\n            \"### Answer:\\n{0}\".format(ttdro['answer']),\n            DEFAULT_EOS_TOKEN,\n        )\n    datasetStruct[\"output\"].append(output_content)\n    \n    dataset[\"text\"].append(input_content + output_content)\n    \ndataset = Dataset.from_dict(dataset)\n#     <s>[INST][/INST] </s>","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:23:48.161116Z","iopub.execute_input":"2023-11-26T04:23:48.161461Z","iopub.status.idle":"2023-11-26T04:23:48.196463Z","shell.execute_reply.started":"2023-11-26T04:23:48.161429Z","shell.execute_reply":"2023-11-26T04:23:48.195450Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"print(dataset[\"text\"][32])","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:23:48.197738Z","iopub.execute_input":"2023-11-26T04:23:48.198028Z","iopub.status.idle":"2023-11-26T04:23:48.207571Z","shell.execute_reply.started":"2023-11-26T04:23:48.198003Z","shell.execute_reply":"2023-11-26T04:23:48.206644Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Human: Below is an instruction that describes a task, paired with the choices, one of the choices is the correct answer to the request. Write a response that appropriately completes the request.\n\n### Instruction:\nNgày thứ nhất, bác Thái thu hoạch được 250 kg nhãn. Ngày thứ hai, số ki- lô-gam nhãn bác Thái thu hoạch được đã giảm đi 2 lần so với ngày thứ nhất. Vậy cả hai ngày bác Thái thu hoạch được số ki-lô-gam nhãn là:\n\n### Choices:\nA. 500 kg\nB. 750 kg\nC. 125kg\nD. 375 kg\nAssistant: \n\n### Explanation:\nNgày thứ hai, bác Thái thu hoạch được số ki-lô-ham nhãn là: 250 : 2 = 125 (kg) \n Cả hai ngày bác Thái thu hoạch được số ki-lô-gam nhãn là: $250 + 125 = 375$ (kg)\n Đáp số: 375 kg\n\n### Answer:\nD. 375 kg <|end_of_turn|>\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(dataset[\"text\"]))","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:23:48.208924Z","iopub.execute_input":"2023-11-26T04:23:48.209210Z","iopub.status.idle":"2023-11-26T04:23:48.221649Z","shell.execute_reply.started":"2023-11-26T04:23:48.209185Z","shell.execute_reply":"2023-11-26T04:23:48.220661Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"1200\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Train model","metadata":{}},{"cell_type":"markdown","source":"### Load model","metadata":{}},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(  \n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.bfloat16,\n    bnb_4bit_use_double_quant= False,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n#         model_hotamath_path,\n    load_in_4bit=True,\n#     load_in_8bit= True,\n    quantization_config=bnb_config,\n    torch_dtype=torch.bfloat16,\n#         torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    base_model,\n    model_max_length=2048,\n    padding_side=\"right\",\n    use_fast=False,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"special_tokens_dict = {\n    'additional_special_tokens': [DEFAULT_BOI_TOKEN, DEFAULT_EOI_TOKEN],\n    'pad_token': DEFAULT_PAD_TOKEN,\n    'bos_token': DEFAULT_BOS_TOKEN,\n    'eos_token': DEFAULT_EOS_TOKEN,\n    'unk_token': DEFAULT_UNK_TOKEN,\n}\nnum_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.encode(\"{0} Hello, how are you? \\n{1} I'm fine, thank you!{2}\".format(\n    DEFAULT_BOI_TOKEN,\n    DEFAULT_EOI_TOKEN,\n    DEFAULT_EOS_TOKEN,\n)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DataCollator","metadata":{}},{"cell_type":"code","source":"from trl import DataCollatorForCompletionOnlyLM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"instruction_template = DEFAULT_BOI_TOKEN\nresponse_template = DEFAULT_EOI_TOKEN\ncollator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer, mlm=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setup model for train","metadata":{}},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=100,\n    logging_steps=10,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    save_total_limit = 3,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    report_to=\"wandb\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model.to(\"cuda:0\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_trainable_parameters(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n#     model = base_model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    max_seq_length=512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n    data_collator=collator,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.config.use_cache = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train and save","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.save_pretrained(new_model)\ntrainer.model.save_pretrained(new_model)\nmodel.config.to_json_file(os.path.join(new_model, \"config.json\"))\ntokenizer.save_pretrained(new_model)\nwandb.finish()\nmodel.config.use_cache = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r BK-BigAI-Math.zip BK-BigAI-Math","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'BK-BigAI-Math.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evalution","metadata":{}},{"cell_type":"markdown","source":"### Download model","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\nhf_hub_download(repo_id=\"hotamago/ZAIC-2023-Model\", filename=\"Hota-Math.zip\", repo_type=\"model\", local_dir=\"/kaggle/working/\", local_dir_use_symlinks=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip -q -o Hota-Math.zip -d ./","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load model","metadata":{}},{"cell_type":"code","source":"model_name_or_path = \"/kaggle/working/BK-BigAI-Math\"\n# bnb_config = BitsAndBytesConfig(  \n#     load_in_4bit= True,\n#     bnb_4bit_quant_type= \"nf4\",\n#     bnb_4bit_compute_dtype= torch.bfloat16,\n#     bnb_4bit_use_double_quant= False,\n# )\n# model = AutoModelForCausalLM.from_pretrained(\n#         model_name_or_path,\n#         load_in_4bit=True,\n#         quantization_config=bnb_config,\n#         torch_dtype=torch.bfloat16,\n#         device_map=\"auto\",\n#         trust_remote_code=True,\n# )\n\n# model = AutoModelForCausalLM.from_pretrained(\n#         model_name_or_path,\n#         torch_dtype=torch.bfloat16,\n#         device_map=\"auto\",\n#         trust_remote_code=True,\n# )\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name_or_path,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    load_in_4bit=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:23:50.085228Z","iopub.execute_input":"2023-11-26T04:23:50.085638Z","iopub.status.idle":"2023-11-26T04:25:25.892778Z","shell.execute_reply.started":"2023-11-26T04:23:50.085609Z","shell.execute_reply":"2023-11-26T04:25:25.891648Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd657a1320a94cb6a61d555618bf7cd3"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    model_name_or_path,\n    model_max_length=512,\n    padding_side=\"right\",\n    use_fast=False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:25:25.894650Z","iopub.execute_input":"2023-11-26T04:25:25.895027Z","iopub.status.idle":"2023-11-26T04:25:26.012152Z","shell.execute_reply.started":"2023-11-26T04:25:25.894997Z","shell.execute_reply":"2023-11-26T04:25:26.011189Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"# special_tokens_dict = {'additional_special_tokens': ['[INST]','[/INST]']}\n# num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:25:26.013363Z","iopub.execute_input":"2023-11-26T04:25:26.013660Z","iopub.status.idle":"2023-11-26T04:25:26.146215Z","shell.execute_reply.started":"2023-11-26T04:25:26.013635Z","shell.execute_reply":"2023-11-26T04:25:26.145139Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"Embedding(32005, 4096)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Setup pipeline with auto answer get","metadata":{}},{"cell_type":"code","source":"pipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n    temperature=0.01,\n    top_p=0.3,\n    top_k=5,\n    repetition_penalty=1.1,\n    pad_token_id=tokenizer.eos_token_id\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:25:26.148951Z","iopub.execute_input":"2023-11-26T04:25:26.149545Z","iopub.status.idle":"2023-11-26T04:25:26.154608Z","shell.execute_reply.started":"2023-11-26T04:25:26.149510Z","shell.execute_reply":"2023-11-26T04:25:26.153617Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import random\n# globalRegxCompire = \"0-9a-zA-Z\\.\\:\\-\\^\\! \"\ndef niceValueToCompire(x):\n#     x = re.sub(\"[^{0}]\".format(globalRegxCompire), \"\", x)\n    x = re.sub(\"[ \\t\\n]\", \"\", x)\n    return x\ndef autoLLMFormat(question, choises = None, debug=False):\n    prompt_template = ApplyPromptTemplate(question, choises, \"prompt_input_run\")\n    res = pipe(prompt_template)[0]['generated_text']\n    if debug:\n        print(res)\n    x = re.findall(\"### Answer:[\\n ](.+)\", res)\n    \n    if choises == None:\n        return x\n    \n    choises_compare = [niceValueToCompire(choise_pred) for choise_pred in choises]\n\n    if len(x) == 0:\n        return choises[random.randrange(0, len(choises))]\n    \n    x = niceValueToCompire(x[0])\n    \n    if (x not in choises_compare):\n        return choises[random.randrange(0, len(choises))]\n    \n    for i in range(len(choises_compare)):\n        if x == choises_compare[i]:\n            return choises[i]\n    \n    return \"wtf\"","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:25:26.155732Z","iopub.execute_input":"2023-11-26T04:25:26.156065Z","iopub.status.idle":"2023-11-26T04:25:26.168753Z","shell.execute_reply.started":"2023-11-26T04:25:26.156034Z","shell.execute_reply":"2023-11-26T04:25:26.167996Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"### Run evalution","metadata":{}},{"cell_type":"code","source":"count_proc_testcase = 0\ncount_pass_testcase = 0","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:25:26.169766Z","iopub.execute_input":"2023-11-26T04:25:26.170334Z","iopub.status.idle":"2023-11-26T04:25:26.184270Z","shell.execute_reply.started":"2023-11-26T04:25:26.170291Z","shell.execute_reply":"2023-11-26T04:25:26.183311Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"while count_proc_testcase < len(tokenized_val_dataset_raw):\n    tvdo = tokenized_val_dataset_raw[count_proc_testcase]\n    startTime()\n    answer = autoLLMFormat(tvdo['question'], tvdo['choices'], True)\n    deltaTime = getTime()\n\n    if answer == tvdo['answer']:\n        count_pass_testcase += 1\n    \n    count_proc_testcase += 1\n    print(\"Testcase {0}, time: {1}, answer: {2} | {3}, Passed: {4}, IsPass: {5}\".format(\n        count_proc_testcase,\n        deltaTime,\n        answer,\n        tvdo['answer'],\n        count_pass_testcase,\n        (answer == tvdo['answer'])\n    ))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-26T04:25:26.185297Z","iopub.execute_input":"2023-11-26T04:25:26.185621Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Human: Below is an instruction that describes a task, paired with the choices, one of the choices is the correct answer to the request.  Write a response that appropriately completes the request.\n\n### Instruction:\nSố liền sau của số 75 299 là:\n\n### Choices:\nA.7 5289\nB.75 298\nC.75 300\nD.75 301\nAssistant: \n\n### Explanation:\n Hãy suy nghĩ từng bước một.\n Số 75 299 + 1 = 75 300\n Vậy số liền sau của số 75 299 là: 75 300\n\n### Answer:\nC.75 300 \nTestcase 1, time: 10.804434537887573, answer: C.75 300 | C.75 300, Passed: 1, IsPass: True\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Eval: {0}/{1} = {2}\".format(count_pass_testcase, len(tokenized_val_dataset_raw), count_pass_testcase/len(tokenized_val_dataset_raw)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run public test dataset","metadata":{}},{"cell_type":"code","source":"result_test = []\n\nif os.path.exists(os.path.join(\"result\", \"result.txt\")):\n    with open(os.path.join(\"result\", \"result.txt\"), \"r\") as f:\n        result_test = f.read().split(\"\\n\")\ncount_id = len(result_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(test_data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"while count_id < len(test_data):\n    one_test_data = test_data[count_id]\n    startTime()\n    answer = autoLLMFormat(one_test_data['question'], one_test_data['choices'], False)\n    deltaTime = getTime()\n    result_test.append(\"{0}\".format(answer))\n    count_id += 1\n    if count_id%10 == 0:\n        with open(os.path.join(\"result\", \"result.txt\"), \"w\", encoding='utf-8') as f:\n            f.write(\"\\n\".join(result_test))\n    print(\"Testcase {0}, time: {1}, answer: {2}\".format(count_id, deltaTime, answer))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(os.path.join(\"result\", \"result.txt\"), \"w\", encoding='utf-8') as f:\n    f.write(\"\\n\".join(result_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n\".join(result_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Convert json to csv","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"json_res = []\nfor i in range(test_data):\n    one_test_data = test_data[count_id]\n    json_res.append({\n        \"id\" : one_test_data[\"id\"],\n        \"answer\" : result_test[i]\n    })","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_json(json_res)\ndf.to_csv('result.csv')","metadata":{},"execution_count":null,"outputs":[]}]}